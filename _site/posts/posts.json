[
  {
    "path": "posts/2022-08-05-moneypox-data-analysis-in-r/",
    "title": "Moneypox Data Analysis in R",
    "description": "In this post we will explore a reference to the latest GitHub Monkeybox data and how to visualize in R.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-08-05",
    "categories": [
      "data cleaning",
      "data fusion",
      "geospacial",
      "r",
      "medicine"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nPackages\r\nGetting the data\r\nWorld map data\r\nData cleaning\r\nData fusing\r\nData visualization\r\n\r\nOther data\r\nUSA Visualization\r\n\r\n\r\nReferences\r\n\r\nIntroduction\r\nMoneybox outbreaks have been on the global forefront as cases rise\r\nworld-wide. This original\r\npost describes where to access the data and how to visualize it\r\nusing maps and other useful packages. I will explore how\r\nthis could be extended to a dashboard for real time data analysis, along\r\nwith a few additional visualizations.\r\nPackages\r\n\r\n\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\nlibrary(maps)\r\nlibrary(viridis)\r\nlibrary(lubridate)\r\nlibrary(forcats)\r\n\r\n\r\nGetting the data\r\n\r\n\r\n# Read worldwide case data\r\ncase_series <- read.csv(\"https://raw.githubusercontent.com/globaldothealth/monkeypox/main/timeseries-country-confirmed.csv\")\r\nhead(case_series)\r\n\r\n        Date Cases Cumulative_cases Country\r\n1 2022-07-25     1                1 Andorra\r\n2 2022-07-26     2                3 Andorra\r\n3 2022-07-27     0                3 Andorra\r\n4 2022-07-28     0                3 Andorra\r\n5 2022-07-29     0                3 Andorra\r\n6 2022-07-30     0                3 Andorra\r\n\r\nWorld map data\r\nTo obtain a data frame of locations of different maps, there are\r\nthree options:\r\nworld\r\nusa\r\nstate\r\ncounty\r\nWe can quickly observe that the regions might not match our case data\r\nabove. They also don’t share the same name. We will resolve this when we\r\ndo some joining and data fusion.\r\n\r\n\r\ndf_world <- map_data(\"world\")\r\n# df_usa <- map_data(\"usa\")\r\n# df_state <- map_data(\"state\")\r\n# df_county <- map_data(\"county\")\r\n\r\nhead(df_world)\r\n\r\n       long      lat group order region subregion\r\n1 -69.89912 12.45200     1     1  Aruba      <NA>\r\n2 -69.89571 12.42300     1     2  Aruba      <NA>\r\n3 -69.94219 12.43853     1     3  Aruba      <NA>\r\n4 -70.00415 12.50049     1     4  Aruba      <NA>\r\n5 -70.06612 12.54697     1     5  Aruba      <NA>\r\n6 -70.05088 12.59707     1     6  Aruba      <NA>\r\n\r\n# df_world %>% group_by(region) %>% count()\r\n\r\n\r\nData cleaning\r\n\r\n\r\ncase_map = case_series\r\n\r\ncase_map =\r\n  case_map %>% \r\n  mutate(Country = fct_recode(Country, \r\n                               \"USA\" = \"United States\",\r\n                               \"UK\" = \"United Kingdom\",\r\n                               \"Democratic Republic of the Congo\" = \"Democratic Republic Of The Congo\",\r\n                               \"Bosnia and Herzegovina\" = \"Bosnia And Herzegovina\")) %>% \r\n  filter(Country != \"Gibraltar\") %>% \r\n  rename(region = Country) %>% \r\n  mutate(Cumulative_cases = as.numeric(Cumulative_cases))\r\n\r\ncase_map %>% head\r\n\r\n        Date Cases Cumulative_cases  region\r\n1 2022-07-25     1                1 Andorra\r\n2 2022-07-26     2                3 Andorra\r\n3 2022-07-27     0                3 Andorra\r\n4 2022-07-28     0                3 Andorra\r\n5 2022-07-29     0                3 Andorra\r\n6 2022-07-30     0                3 Andorra\r\n\r\nData fusing\r\n\r\n\r\ncases_joined = case_map %>% inner_join(df_world, by = \"region\")\r\ncases_joined %>% tail\r\n\r\n              Date Cases Cumulative_cases    region      long\r\n3587832 2022-08-05     0                1 Venezuela -60.80098\r\n3587833 2022-08-05     0                1 Venezuela -60.48149\r\n3587834 2022-08-05     0                1 Venezuela -60.40449\r\n3587835 2022-08-05     0                1 Venezuela -60.34023\r\n3587836 2022-08-05     0                1 Venezuela -60.16748\r\n3587837 2022-08-05     0                1 Venezuela -60.01753\r\n             lat group order subregion\r\n3587832 8.592139  1582 98756      <NA>\r\n3587833 8.547265  1582 98757      <NA>\r\n3587834 8.610254  1582 98758      <NA>\r\n3587835 8.628759  1582 98759      <NA>\r\n3587836 8.616992  1582 98760      <NA>\r\n3587837 8.549316  1582 98761      <NA>\r\n\r\nData visualization\r\n\r\n\r\nplot_cases = function(date, xlim, ylim, title){\r\n  cases_joined %>% filter(Date == date) %>% \r\n  ggplot(., aes(long, lat, group = group)) + \r\n  geom_polygon(aes(fill = Cumulative_cases), color = \"white\", size = 0.2) +\r\n  scale_fill_viridis() +\r\n  theme_linedraw() +\r\n  theme(legend.position = \"right\",\r\n        legend.direction = \"vertical\") +\r\n  labs(fill = \"Cumulative Cases\") +\r\n  ggtitle(label = title, subtitle = paste(\"As of \", date)) +\r\n  xlab(\"Longitude\") +\r\n  ylab(\"Latitude\") +\r\n  coord_map(xlim = xlim, ylim = ylim) \r\n}\r\n\r\nplot_cases(\"2022-08-05\", c(-180, 180), c(-55, 90), \"Global Monkeypox Cases\")\r\n\r\n\r\n\r\n\r\n\r\nplot_cases(\"2022-08-05\", c(-22, 38), c(35, 64), \"Europe Monkeypox Cases\")\r\n\r\n\r\n\r\nOther data\r\nThe CDC\r\nalso has a CSV file you can download to get the latest just in the US. I\r\ndownloaded that and load it in here.\r\n\r\n\r\ncases_usa = \r\n  read.csv(\"C:\\\\Users\\\\blake\\\\Desktop\\\\blog_data\\\\2022 U.S. Map & Case Count.csv\") %>% \r\n  rename(region = State) %>% \r\n  mutate(region = tolower(region))\r\n\r\n\r\nmap_usa = map_data(\"state\")\r\n\r\nusa_joined = cases_usa %>% inner_join(map_usa, by = \"region\")\r\nusa_joined %>% head\r\n\r\n   region Cases Case.Range      long      lat group order subregion\r\n1 alabama    19   11 to 50 -87.46201 30.38968     1     1      <NA>\r\n2 alabama    19   11 to 50 -87.48493 30.37249     1     2      <NA>\r\n3 alabama    19   11 to 50 -87.52503 30.37249     1     3      <NA>\r\n4 alabama    19   11 to 50 -87.53076 30.33239     1     4      <NA>\r\n5 alabama    19   11 to 50 -87.57087 30.32665     1     5      <NA>\r\n6 alabama    19   11 to 50 -87.58806 30.32665     1     6      <NA>\r\n\r\nUSA Visualization\r\n\r\n\r\nggplot(usa_joined, aes(long, lat, group = group)) +\r\n  geom_polygon(aes(fill = Cases)) +\r\n  scale_fill_viridis() +\r\n  theme_linedraw() +\r\n  theme(legend.position = \"right\",\r\n        legend.direction = \"vertical\") +\r\n  labs(fill = \"Cases\") +\r\n  ggtitle(label = \"USA Monkeybox Cases\") +\r\n  xlab(\"Longitude\") +\r\n  ylab(\"Latitude\")\r\n\r\n\r\n\r\n\r\n\r\ncase_series =\r\n  case_series %>% \r\n  mutate(Date = as_date(Date),\r\n         Country = as.factor(Country))\r\n\r\nplot_stats = function(country){\r\n  case_series %>% filter(Country == country) %>% \r\n  ggplot(., aes(x = Date)) +\r\n  geom_line(aes(y = Cumulative_cases, color = \"Cumulative_cases\")) +\r\n  geom_line(aes(y = Cases, color = \"Cases\")) +\r\n  scale_x_date(breaks = scales::breaks_pretty(10)) +\r\n  labs(color = \"Category\") +\r\n  ggtitle(paste(country, \"Monkeypox Cases\")) +\r\n  xlab(\"Date\") +\r\n  ylab(\"Number of Cases\") +\r\n  theme_bw() +\r\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\r\n}\r\n\r\nplot_stats(\"United States\")\r\n\r\n\r\nplot_stats(\"Canada\")\r\n\r\n\r\n\r\n\r\n\r\ntop10 =\r\n  case_series %>% \r\n  arrange(-desc(Date)) %>%\r\n  group_by(Country) %>% \r\n  summarise(Last_Total_Cases = last(Cumulative_cases)) %>% \r\n  arrange(desc(Last_Total_Cases)) %>% \r\n  filter(row_number() <= 12)\r\n\r\ncase_series %>% filter(Country %in% top10$Country) %>% \r\n  ggplot(., aes(x = Date)) +\r\n  geom_line(aes(y = Cumulative_cases, color = \"Cumulative_cases\")) +\r\n  geom_line(aes(y = Cases, color = \"Cases\")) +\r\n  facet_wrap(~Country) +\r\n  scale_x_date(breaks = scales::breaks_pretty(10)) +\r\n  labs(color = \"Category\") +\r\n  ggtitle(paste(\"Top 12 Countries Monkeypox Cases\")) +\r\n  xlab(\"Date\") +\r\n  ylab(\"Number of Cases\") +\r\n  theme_bw() +\r\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\r\n\r\n\r\n\r\nReferences\r\nhttps://www.r-bloggers.com/2022/07/access-and-map-the-latest-monkeypox-case-data-in-r/\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-05-moneypox-data-analysis-in-r/moneypox-data-analysis-in-r_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2022-08-05T22:27:53-04:00",
    "input_file": "moneypox-data-analysis-in-r.knit.md"
  },
  {
    "path": "posts/2022-08-03-statistical-tests-and-modeling-in-r/",
    "title": "Statistical Tests and Modeling in R",
    "description": "In this post we will explore a few statistical tests and plots that are built into R.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-08-03",
    "categories": [
      "statistics"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nGetting the data\r\nFrequency Tables\r\nCrosstabs\r\n\r\nChi-squared test\r\nT-test\r\nPaired T-test\r\nLinear Regression\r\nResidual Diagnostics\r\n\r\nANOVA\r\n\r\nReferences\r\n\r\nIntroduction\r\nStatistical tests are common-place for detecting significant\r\ndifferences among data fields. In the\r\noriginal article there are several of these, among other statistical\r\ntechniques introduced. I want to emphesize a few of these and some\r\nsimple modeling plots in this article. These include:\r\nChi-squared test\r\nT-test\r\nPaired T-test\r\nLinear regression\r\nANOVA\r\nGetting the data\r\n\r\n\r\nbloodtest <- data.frame(id = 1:10,\r\n                        gender = c(\"female\", \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"female\", \"male\", \"female\"),\r\n                        hospital = c(\"CLH\", \"MH\", \"MH\", \"MH\", \"CLH\", \"MH\", \"MDH\", \"MDH\", \"CLH\", \"MH\"),\r\n                        doc_id = c(1, 1, 1, 2, 2, 2, 3, 3, 3, 3),\r\n                        insured = c(0, 1, 1, 1, 0, 1, 1, 0, 1, 1),\r\n                        age = c(23, 45, 37, 49, 51, 55, 56, 37, 26, 40),\r\n                        test1  = c(47, 67, 41, 65, 60, 52, 68, 37, 44, 44),\r\n                        test2 = c(46, 57, 47, 65, 62, 51 ,62 ,44 ,46, 61),\r\n                        test3 = c(49, 73, 50, 64, 77, 57, 75, 55, 62, 55),\r\n                        test4 = c(61, 61, 51, 71, 56, 57, 61, 46, 46, 46))\r\nbloodtest\r\n\r\n   id gender hospital doc_id insured age test1 test2 test3 test4\r\n1   1 female      CLH      1       0  23    47    46    49    61\r\n2   2   male       MH      1       1  45    67    57    73    61\r\n3   3 female       MH      1       1  37    41    47    50    51\r\n4   4 female       MH      2       1  49    65    65    64    71\r\n5   5 female      CLH      2       0  51    60    62    77    56\r\n6   6   male       MH      2       1  55    52    51    57    57\r\n7   7   male      MDH      3       1  56    68    62    75    61\r\n8   8 female      MDH      3       0  37    37    44    55    46\r\n9   9   male      CLH      3       1  26    44    46    62    46\r\n10 10 female       MH      3       1  40    44    61    55    46\r\n\r\n\r\n\r\nmean(bloodtest$age)\r\n\r\n[1] 41.9\r\n\r\nmedian(bloodtest$age)\r\n\r\n[1] 42.5\r\n\r\nvar(bloodtest$age)\r\n\r\n[1] 130.5444\r\n\r\nsummary(bloodtest$test1)\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n  37.00   44.00   49.50   52.50   63.75   68.00 \r\n\r\nFrequency Tables\r\n\r\n\r\n# Counts of gender by category\r\ntable(bloodtest$gender)\r\n\r\n\r\nfemale   male \r\n     6      4 \r\n\r\n# Counts of hoppital by category\r\ntable(bloodtest$hospital)\r\n\r\n\r\nCLH MDH  MH \r\n  3   2   5 \r\n\r\n# Proportions of hospital by category - should always sum to 1\r\nprop.table(table(bloodtest$hospital))\r\n\r\n\r\nCLH MDH  MH \r\n0.3 0.2 0.5 \r\n\r\nCrosstabs\r\nYou can look at proportions over two or more variables by extending\r\nthe table function to include multiple variables. We include three\r\nhere,\r\ngender\r\nhospital\r\ninsured\r\n\r\n\r\nmy2way <- table(bloodtest$gender, bloodtest$hospital, bloodtest$insured)\r\nmy2way\r\n\r\n, ,  = 0\r\n\r\n        \r\n         CLH MDH MH\r\n  female   2   1  0\r\n  male     0   0  0\r\n\r\n, ,  = 1\r\n\r\n        \r\n         CLH MDH MH\r\n  female   0   0  3\r\n  male     1   1  2\r\n\r\n# row proportions - prop of gender that fall into hospitals\r\nprop.table(my2way, margin=1)\r\n\r\n, ,  = 0\r\n\r\n        \r\n               CLH       MDH        MH\r\n  female 0.3333333 0.1666667 0.0000000\r\n  male   0.0000000 0.0000000 0.0000000\r\n\r\n, ,  = 1\r\n\r\n        \r\n               CLH       MDH        MH\r\n  female 0.0000000 0.0000000 0.5000000\r\n  male   0.2500000 0.2500000 0.5000000\r\n\r\n# column proportions - prop of hospital that fall into genders\r\nprop.table(my2way, margin=2)\r\n\r\n, ,  = 0\r\n\r\n        \r\n               CLH       MDH        MH\r\n  female 0.6666667 0.5000000 0.0000000\r\n  male   0.0000000 0.0000000 0.0000000\r\n\r\n, ,  = 1\r\n\r\n        \r\n               CLH       MDH        MH\r\n  female 0.0000000 0.0000000 0.6000000\r\n  male   0.3333333 0.5000000 0.4000000\r\n\r\nIt looks like the for the non-insurred visitors accross the hospitals\r\nthe majority proportion (2/3) goes to CLH, and they happen\r\nto be mostly females. Could this be do to some other cause,\r\nsuch:\r\nBirths?\r\nAbortions?\r\nSpecial female treatments/operations?\r\nThat said, there are no females who were insurred at this hospital.\r\nBut close behind is MDH with the same issue, but on less a\r\nseverse scale only with a 1/2 split of insurred females to insurred\r\nmales (as oppose to our 2/3 to 1/3 previous situation at\r\nCLH). For MH we have a nice porportional split\r\nof insurred male and female patients, with slightly more females (6/10\r\nto 4/10).\r\nChi-squared test\r\nThe chi-squared test is an interesting one I needed to brush up on.\r\nIt tests two categorical variables. The test will identify if they are\r\ndifferent. Pretty nifty. How does it do this? It will examine the ratio\r\nof proportions. So point by point, the proportion of observations that\r\nfall into the same classes will provide a low chi-square distribution\r\nvalue. That is, if the observations time and time again share the same\r\ncategories, low chi-square values will yield. If they, however, do not\r\nshare the same values over all observations, but show variance, the\r\nchi-squared distribution value will be higher, resulting in a lower\r\np-value (or in the tail), whih will identify statistical significant,\r\nand a difference detected.\r\n\r\n\r\n# Pretty independent categories\r\nchisq.test(bloodtest$hospital, bloodtest$insured)\r\n\r\n\r\n    Pearson's Chi-squared test\r\n\r\ndata:  bloodtest$hospital and bloodtest$insured\r\nX-squared = 4.4444, df = 2, p-value = 0.1084\r\n\r\nhospital and insured look pretty\r\nindependent.\r\nWhat about between insurance and gender?\r\n\r\n\r\n# Pretty independent categories\r\nchisq.test(bloodtest$gender, bloodtest$insured)\r\n\r\n\r\n    Pearson's Chi-squared test with Yates' continuity correction\r\n\r\ndata:  bloodtest$gender and bloodtest$insured\r\nX-squared = 0.97222, df = 1, p-value = 0.3241\r\n\r\nThese do not look as independent, but 0.3 to me is still\r\npretty low. And based on our frequency analysis, might be something\r\nworth looking into later!\r\nT-test\r\nThe t-test is not paired, so we do not suspect a\r\ncorrelation between the variables. This will test whether the difference\r\nbetween the test1 and gender is significantly different.\r\n\r\n\r\nt.test(test1 ~ gender, data=bloodtest)\r\n\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  test1 by gender\r\nt = -1.1813, df = 6.2951, p-value = 0.2802\r\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\r\n95 percent confidence interval:\r\n -26.670132   9.170132\r\nsample estimates:\r\nmean in group female   mean in group male \r\n               49.00                57.75 \r\n\r\nWith such a high p-value, we do not suspect the first test to differ\r\nmuch across genders.\r\nAre the results for test1 significantly different accross hospitals?\r\nThe t-test will only check between categorical variables with two levels\r\nor two numeric variables. The ANOVA will expand this to factors of many\r\nlevels. But the essence is still the same. The f-distribution will\r\ndetect a strong different of total mean squared to sum of square ratios,\r\nand if it is high it may indicate key differences across the categorical\r\nvariable.\r\n\r\n\r\nsummary(aov(test1 ~ hospital, data=bloodtest))\r\n\r\n            Df Sum Sq Mean Sq F value Pr(>F)\r\nhospital     2   22.5   11.27   0.066  0.936\r\nResiduals    7 1188.0  169.71               \r\n\r\nWith such a high p-value, we do not suspect this category to be very\r\ndifferent accross the first test results.\r\nWhat about\r\n\r\n\r\nt.test(test1 ~ insured, data=bloodtest)\r\n\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  test1 by insured\r\nt = -0.79887, df = 3.9851, p-value = 0.4693\r\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\r\n95 percent confidence interval:\r\n -28.80380  15.94666\r\nsample estimates:\r\nmean in group 0 mean in group 1 \r\n       48.00000        54.42857 \r\n\r\nInsurance indication does not appear to be very different for the\r\nfirst blood test.\r\nPaired T-test\r\nThe paired t-test is when we suspect correlation, but the question\r\nposed is always the same: Are these two things different? Low p-values\r\nprovide evidence that their mean difference is non-zero (they’re pretty\r\ndifferent, in statistical speak).\r\nWe test here the difference between the blood tests themselves. Were\r\ntheir values pretty different (after centered, scaled, etc..)?\r\n\r\n\r\nt.test(bloodtest$test1, bloodtest$test3, paired=TRUE)\r\n\r\n\r\n    Paired t-test\r\n\r\ndata:  bloodtest$test1 and bloodtest$test3\r\nt = -4.3231, df = 9, p-value = 0.001925\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -14.014139  -4.385861\r\nsample estimates:\r\nmean of the differences \r\n                   -9.2 \r\n\r\nThey were significantly different. It is important to remember the\r\nassumptions of such a test. This assumes normality of your response\r\nvariables under consideration.\r\nLinear Regression\r\nThis has largely been updated by the tidymodels package,\r\nusing the engine(\"lm\") method, but the engine is still the\r\nsame, so it is important to know how to use it for statistical testing\r\nand more sophisticated data optimization if required.\r\n\r\n\r\nm1 <- lm(test1 ~ age + gender, data=bloodtest)\r\nm1\r\n\r\n\r\nCall:\r\nlm(formula = test1 ~ age + gender, data = bloodtest)\r\n\r\nCoefficients:\r\n(Intercept)          age   gendermale  \r\n    24.4871       0.6206       5.0265  \r\n\r\nResidual Diagnostics\r\nThere are for plots produced for the lm object. These\r\nproduce:\r\nresidual vs fitted\r\nnormal q-q-plot of residuals\r\nscale-location\r\nresiduals vs leverage\r\n\r\n\r\n# plots all 4 plots at once (otherwise one at a time)\r\nlayout(matrix(c(1,2,3,4),2,2))\r\n\r\n# 4 diagnostic plots\r\nplot(m1)\r\n\r\n\r\n\r\nSome important functions to look at your regression model:\r\nsummary()\r\ncoef()\r\nresiduals()\r\npredict()\r\nconfint()\r\n\r\n\r\ncoef(m1)\r\n\r\n(Intercept)         age  gendermale \r\n 24.4871383   0.6205788   5.0265273 \r\n\r\nconfint(m1)\r\n\r\n                  2.5 %    97.5 %\r\n(Intercept) -3.37869862 52.352975\r\nage         -0.04713382  1.288291\r\ngendermale  -9.74700686 19.800062\r\n\r\ndf = as.data.frame(cbind(bloodtest$test1, predict(m1), residuals(m1)))\r\nnames(df) = c(\"test1\", \"prediction\", \"residuals\")\r\ndf\r\n\r\n   test1 prediction  residuals\r\n1     47   38.76045   8.239550\r\n2     67   57.43971   9.560289\r\n3     41   47.44855  -6.448553\r\n4     65   54.89550  10.104502\r\n5     60   56.13666   3.863344\r\n6     52   63.64550 -11.645498\r\n7     68   64.26608   3.733923\r\n8     37   47.44855 -10.448553\r\n9     44   45.64871  -1.648714\r\n10    44   49.31029  -5.310289\r\n\r\nANOVA\r\nWe can now supply the lm model into the\r\nanova() function to produce an anova table for our linear\r\nmodel and the formula provided.\r\n\r\n\r\nanova(m1)\r\n\r\nAnalysis of Variance Table\r\n\r\nResponse: test1\r\n          Df Sum Sq Mean Sq F value Pr(>F)  \r\nage        1 546.77  546.77  6.2997 0.0404 *\r\ngender     1  56.18   56.18  0.6473 0.4475  \r\nResiduals  7 607.55   86.79                 \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nFrom the author,\r\n\r\n“The anova() function is often used to conduct a likelihood ratio\r\ntest, that compares the fit of nested models to the data. This test\r\nallows one to assess whether the addition of several predictors improves\r\nthe fit of the model.\r\nSimply specify two nested models to anova() to conduct the likelihood\r\nratio test:”\r\n\r\n\r\n\r\nm2 <- lm(test1 ~ age + gender + hospital, data = bloodtest)\r\n\r\nanova.results = anova(m2, m1)\r\n\r\nanova.results\r\n\r\nAnalysis of Variance Table\r\n\r\nModel 1: test1 ~ age + gender + hospital\r\nModel 2: test1 ~ age + gender\r\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\r\n1      5 525.14                           \r\n2      7 607.55 -2   -82.414 0.3923 0.6946\r\n\r\nanova.results$`Pr(>F)`[[2]]\r\n\r\n[1] 0.694584\r\n\r\nThis will see if the second model offers any improvement by adding in\r\na new variable. This will test the ratio of residual sum of squares\r\n(total error accross all observations) against one another, then map it\r\nonto the f-distribution to discern if it is in the tail. A low p-value\r\nwill indicate that the second model (first argument) is low\r\nOR the first model (second argument) is high\r\n(or both in concurrence). If this is the case, the f-distribution will\r\nyield a very high value, and the p-value will be very low (i.e., low\r\nprobability of this value being observed). This will issue to us that\r\nthere is a statistical difference in models, and your added variable is\r\nimportant information!\r\nIn our case, this is not so. So we will not add hospital to the mix.\r\nWe could experiment and see if adding any other variables might\r\nhelp.\r\n\r\n\r\n# Let's start with age and see if we can incrementally add valuable information in, only if it improves based on the anova test.\r\n\r\nbase_model = lm(test1 ~ age, data=bloodtest)\r\nselected_variables = c(\"age\")\r\n\r\nfor(name in names(bloodtest)){\r\n  if(!(name %in% selected_variables) && name != \"test1\"){\r\n    # alternative_model = lm()\r\n    hyp_updated_variables = c(selected_variables, name)\r\n    formulaString = paste(\"test1 ~ \", paste(hyp_updated_variables, collapse = \"+\"))\r\n    alternate_model = lm(as.formula(formulaString), data=bloodtest)\r\n    alternate_model\r\n    \r\n    anova.results = anova(alternate_model, base_model)\r\n    p = anova.results$`Pr(>F)`[[2]]\r\n    \r\n    # print(\"Testing base against alternate\")\r\n    # print(formulaString)\r\n    # print(p)\r\n    if(p <= 0.05){\r\n      print(paste(\"*** UPDATING MODEL TO INCLUDE \", name, \"***\", sep=\"\") )\r\n      print(paste(\"p.value = \", p))\r\n      selected_variables <- c(selected_variables, name)\r\n      newFormula = paste(\"test1 ~\", paste(selected_variables, collapse=\"+\"))\r\n      base_model = lm(newFormula, data=bloodtest)\r\n    }\r\n  }\r\n}\r\n\r\n[1] \"*** UPDATING MODEL TO INCLUDE test3***\"\r\n[1] \"p.value =  0.0394582134317015\"\r\n[1] \"*** UPDATING MODEL TO INCLUDE test4***\"\r\n[1] \"p.value =  0.00081872553526069\"\r\n\r\nsummary(base_model)\r\n\r\n\r\nCall:\r\nlm(formula = newFormula, data = bloodtest)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3.3964 -1.1543  0.1323  1.2704  3.3295 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -33.31021    7.36089  -4.525 0.003995 ** \r\nage           0.09134    0.10801   0.846 0.430168    \r\ntest3         0.63222    0.11645   5.429 0.001619 ** \r\ntest4         0.77293    0.12487   6.190 0.000819 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2.799 on 6 degrees of freedom\r\nMultiple R-squared:  0.9612,    Adjusted R-squared:  0.9417 \r\nF-statistic:  49.5 on 3 and 6 DF,  p-value: 0.0001263\r\n\r\nIt looks like no other single variable added any important enough\r\ninformation to decrease residual sum of squares, but the other test\r\nresults were helpful to know with age. An exercise would be to do this\r\nforward, and backward removing some variables also. One could also add\r\npairs, and interactions to see if they are significant. One could even\r\ndo random sampling of higher order information to see. This anova test\r\nto compare if new model variables are important is pretty significant in\r\nthe statistical toolkit. Hopefully you enjoyed learning a little bit\r\nmore about it today and how optimization can here be used to learn more\r\nabout your data as related to statistical distributions and testing.\r\nReferences\r\nhttps://stats.oarc.ucla.edu/stat/data/intro_r/intro_r_interactive_flat.html#chi-square-test-of-indepedence\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-03-statistical-tests-and-modeling-in-r/statistical-tests-and-modeling-in-r_files/figure-html5/unnamed-chunk-12-1.png",
    "last_modified": "2022-08-03T09:13:30-04:00",
    "input_file": "statistical-tests-and-modeling-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-31-gni-per-capita-and-plastic-pollution-analyss-in-r/",
    "title": "GNI Per Capita and Plastic Pollution Analyss in R",
    "description": "In this post we will reference analysis done on 20 countries that produce and most mismanaged waste based on their GNI income status in R.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-31",
    "categories": [
      "web scraping",
      "economics",
      "data cleaning",
      "linear regression"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nGetting the data\r\nVisualizing the data\r\nModeling with\r\ntidymodels\r\nVariable importance\r\n\r\n\r\nIntroduction\r\nIn this post we will be working through some analysis posted here\r\nanalyzing 20 different countries that produce mismanaged waste based on\r\ntheir GNI income status.\r\nGetting the data\r\nFirst we want to get a list of dataframes from the wikipedia website\r\nfor Gross National Income. After some recon, one can\r\nreadily identify that each table has the class table and\r\nsubclass wikitable, which is ad-joined to become\r\ntable.wikitable in CSS language.\r\nThe data is available here.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidymodels)\r\nlibrary(janitor)\r\nlibrary(rvest)\r\nlibrary(scales)\r\n# library(bbplot)\r\nlibrary(vip)\r\n\r\n\r\n#Scraping the GNI data set\r\nurl <- \"https://en.wikipedia.org/wiki/List_of_countries_by_GNI_(nominal)_per_capita\"\r\nlist_html <-\r\n  read_html(url) %>% #scraping the interested web page\r\n  rvest::html_elements(\"table.wikitable\") %>% # takes the tables we want\r\n  rvest::html_table() \r\n\r\n\r\nlist_html\r\n\r\n[[1]]\r\n# A tibble: 36 x 4\r\n   Rank  Country              `GNI per capita (US$)[1]`  Year\r\n   <chr> <chr>                <chr>                     <int>\r\n 1 —     Bermuda (UK)         116,540                    2021\r\n 2 1     Liechtenstein        116,440                    2009\r\n 3 2     Switzerland          90,360                     2021\r\n 4 3     Norway               84,090                     2021\r\n 5 —     Isle of Man (UK)     83,920                     2019\r\n 6 4     Luxembourg           81,110                     2020\r\n 7 5     Ireland              74,520                     2021\r\n 8 6     United States        70,430                     2021\r\n 9 7     Denmark              68,110                     2021\r\n10 —     Channel Islands (UK) 66,220                     2007\r\n# ... with 26 more rows\r\n# i Use `print(n = ...)` to see more rows\r\n\r\n[[2]]\r\n# A tibble: 36 x 4\r\n   Rank  Country                       `GNI per capita (US$)[1]`  Year\r\n   <chr> <chr>                         <chr>                     <int>\r\n 1 30    Malta                         30,560                     2021\r\n 2 31    Spain                         29,740                     2021\r\n 3 32    Slovenia                      28,240                     2021\r\n 4 33    Cyprus                        28,130                     2021\r\n 5 —     Sint Maarten (Netherlands)    27,510                     2018\r\n 6 34    Bahamas                       27,220                     2021\r\n 7 35    Estonia                       25,920                     2021\r\n 8 36    Czech Republic                24,070                     2021\r\n 9 37    Portugal                      23,730                     2021\r\n10 —     Turks and Caicos Islands (UK) 23,600                     2021\r\n# ... with 26 more rows\r\n# i Use `print(n = ...)` to see more rows\r\n\r\n[[3]]\r\n# A tibble: 26 x 4\r\n   Rank  Country     `GNI per capita (US$)[1]`  Year\r\n   <chr> <chr>       <chr>                     <int>\r\n 1 59    Costa Rica  12,310                     2021\r\n 2 —     World       12,070                     2021\r\n 3 60    China       11,890                     2021\r\n 4 61    Russia      11,600                     2021\r\n 5 62    Malaysia    10,930                     2021\r\n 6 63    Mauritius   10,860                     2021\r\n 7 64    Bulgaria    10,720                     2021\r\n 8 65    Argentina   10,050                     2021\r\n 9 66    Turkey      9,830                      2021\r\n10 67    Saint Lucia 9,680                      2021\r\n# ... with 16 more rows\r\n# i Use `print(n = ...)` to see more rows\r\n\r\n[[4]]\r\n# A tibble: 27 x 4\r\n    Rank Country                `GNI per capita (US$)[1]`  Year\r\n   <int> <chr>                  <chr>                     <int>\r\n 1    84 Belarus                6,950                      2021\r\n 2    85 Botswana               6,940                      2021\r\n 3    86 Bosnia and Herzegovina 6,770                      2021\r\n 4    87 Tuvalu                 6,760                      2021\r\n 5    88 Peru                   6,520                      2021\r\n 6    89 South Africa           6,440                      2021\r\n 7    90 Colombia               6,160                      2021\r\n 8    91 North Macedonia        6,130                      2021\r\n 9    92 Albania                6,110                      2021\r\n10    93 Ecuador                5,930                      2021\r\n# ... with 17 more rows\r\n# i Use `print(n = ...)` to see more rows\r\n\r\n[[5]]\r\n# A tibble: 26 x 4\r\n    Rank Country                        GNI per capita (US$)[1~1  Year\r\n   <int> <chr>                          <chr>                    <int>\r\n 1   111 Palestine                      4,220                     2021\r\n 2   112 El Salvador                    4,140                     2021\r\n 3   112 Indonesia                      4,140                     2021\r\n 4   114 Ukraine                        4,120                     2021\r\n 5   115 Federated States of Micronesia 3,880                     2021\r\n 6   116 Samoa                          3,860                     2021\r\n 7   117 Sri Lanka                      3,820                     2021\r\n 8   118 Mongolia                       3,760                     2021\r\n 9   119 Eswatini                       3,680                     2021\r\n10   120 Algeria                        3,660                     2021\r\n# ... with 16 more rows, and abbreviated variable name\r\n#   1: `GNI per capita (US$)[1]`\r\n# i Use `print(n = ...)` to see more rows\r\n\r\n[[6]]\r\n# A tibble: 28 x 4\r\n    Rank Country               `GNI per capita (US$)[1]`  Year\r\n   <int> <chr>                 <chr>                     <int>\r\n 1   137 Laos                  2,520                      2021\r\n 2   138 Ivory Coast           2,450                      2021\r\n 3   139 Ghana                 2,360                      2021\r\n 4   140 Solomon Islands       2,300                      2021\r\n 5   141 Sao Tome and Principe 2,280                      2021\r\n 6   142 India                 2,170                      2021\r\n 7   143 Nigeria               2,100                      2021\r\n 8   144 Nicaragua             2,010                      2021\r\n 9   144 Kenya                 2,010                      2021\r\n10   146 Uzbekistan            1,960                      2021\r\n# ... with 18 more rows\r\n# i Use `print(n = ...)` to see more rows\r\n\r\n[[7]]\r\n# A tibble: 13 x 4\r\n    Rank Country       `GNI per capita (US$)[1]`  Year\r\n   <int> <chr>         <chr>                     <int>\r\n 1   165 South Sudan   1,090                      2015\r\n 2   166 Zambia        1,040                      2021\r\n 3   167 Guinea        1,010                      2021\r\n 4   168 Togo          980                        2021\r\n 5   169 Ethiopia      960                        2021\r\n 6   170 Syria         930                        2018\r\n 7   171 Mali          870                        2021\r\n 8   172 Burkina Faso  860                        2021\r\n 9   173 Rwanda        850                        2021\r\n10   174 Uganda        840                        2021\r\n11   175 The Gambia    800                        2021\r\n12   176 Guinea-Bissau 780                        2021\r\n13   177 Yemen         670                        2020\r\n\r\n[[8]]\r\n# A tibble: 14 x 4\r\n    Rank Country                          GNI per capita (US$)~1  Year\r\n   <int> <chr>                                             <int> <int>\r\n 1   177 Sudan                                               670  2021\r\n 2   179 Chad                                                650  2021\r\n 3   180 Malawi                                              630  2021\r\n 4   181 Liberia                                             620  2021\r\n 5   182 Eritrea                                             600  2011\r\n 6   183 Niger                                               590  2021\r\n 7   184 Democratic Republic of the Congo                    580  2021\r\n 8   185 Central African Republic                            530  2021\r\n 9   186 Sierra Leone                                        510  2021\r\n10   187 Madagascar                                          500  2021\r\n11   187 Afghanistan                                         500  2020\r\n12   189 Mozambique                                          480  2021\r\n13   190 Somalia                                             450  2021\r\n14   191 Burundi                                             240  2021\r\n# ... with abbreviated variable name 1: `GNI per capita (US$)[1]`\r\n\r\n[[9]]\r\n# A tibble: 16 x 5\r\n   `High-income group`           High-income~1 High-~2 High-~3 High-~4\r\n   <chr>                         <chr>         <chr>   <chr>   <chr>  \r\n 1 British Virgin Islands (UK)   <NA>          <NA>    <NA>    <NA>   \r\n 2 Cook Islands (New Zealand)    <NA>          <NA>    <NA>    <NA>   \r\n 3 Faroe Islands (Denmark)       <NA>          <NA>    <NA>    <NA>   \r\n 4 Gibraltar (UK)                <NA>          <NA>    <NA>    <NA>   \r\n 5 Guam (US)                     <NA>          <NA>    <NA>    <NA>   \r\n 6 Monaco                        <NA>          <NA>    <NA>    <NA>   \r\n 7 Northern Mariana Islands (US) <NA>          <NA>    <NA>    <NA>   \r\n 8 Saint Martin (France)         <NA>          <NA>    <NA>    <NA>   \r\n 9 San Marino                    <NA>          <NA>    <NA>    <NA>   \r\n10 Taiwan                        <NA>          <NA>    <NA>    <NA>   \r\n11 U.S. Virgin Islands (US)      <NA>          <NA>    <NA>    <NA>   \r\n12 Upper-middle-income group     Upper-middle~ Upper-~ Upper-~ Upper-~\r\n13 American Samoa (US)           <NA>          <NA>    <NA>    <NA>   \r\n14 Venezuela                     <NA>          <NA>    <NA>    <NA>   \r\n15 Low-income group              Low-income g~ Low-in~ Low-in~ Low-in~\r\n16 North Korea                   <NA>          <NA>    <NA>    <NA>   \r\n# ... with abbreviated variable names 1: `High-income group`,\r\n#   2: `High-income group`, 3: `High-income group`,\r\n#   4: `High-income group`\r\n\r\nNext we will use the magic of the purrr package to flow\r\nour categories through each dataframe using the map\r\nfunction. Then we will flow the list of updated datafarmes into the\r\nrbind function with a do.call. Finally we\r\n“consistentize” the columns using janitor’s\r\nclean_names function. Last we will select, rename, and\r\nmutate columns as needed to get a nice clean wiki scraped data\r\nframe.\r\n\r\n\r\n#gni income levels vector\r\nstatus <- c(\"high\",\"high\",\"upper_middle\",\"upper_middle\",\r\n            \"lower_middle\",\"lower_middle\",\"low\",\"low\")\r\n\r\n#Building and tidying GNI dataframe\r\ndf_gni = \r\n  #assigning the gni income levels to the corresponding countries\r\n  purrr::map(1:8, function(x){\r\n    list_html[[x]] %>% \r\n      dplyr::mutate(gni_status=status[x])\r\n  }) %>% \r\n  do.call(what=rbind) %>% \r\n  janitor::clean_names() %>%\r\n  dplyr::mutate(gni_status = forcats::as_factor(gni_status)) %>% \r\n  dplyr::select(country, \r\n                gni_metric=gni_per_capita_us_1, # renames\r\n                gni_status) %>% \r\n  dplyr::mutate(gni_metric = stringr::str_remove(gni_metric,\",\") %>% as.numeric()) \r\ndf_gni\r\n\r\n# A tibble: 206 x 3\r\n   country              gni_metric gni_status\r\n   <chr>                     <dbl> <fct>     \r\n 1 Bermuda (UK)             116540 high      \r\n 2 Liechtenstein            116440 high      \r\n 3 Switzerland               90360 high      \r\n 4 Norway                    84090 high      \r\n 5 Isle of Man (UK)          83920 high      \r\n 6 Luxembourg                81110 high      \r\n 7 Ireland                   74520 high      \r\n 8 United States             70430 high      \r\n 9 Denmark                   68110 high      \r\n10 Channel Islands (UK)      66220 high      \r\n# ... with 196 more rows\r\n# i Use `print(n = ...)` to see more rows\r\n\r\nHow many countries do we know GNI on?\r\n\r\n\r\ndf_gni %>% \r\n  select(country) %>% \r\n  unique %>% count()\r\n\r\n# A tibble: 1 x 1\r\n      n\r\n  <int>\r\n1   206\r\n\r\nAt this point we know information about about 206 countries, their\r\ngross national income, and their gni_status of high, mid, or low. Now we\r\nwant to collect data on waste by country. This is available here.\r\nFirst we get the raw dataframe using the consistentcy of\r\nreadr::read_csv. Next we “consistentize” the\r\ncolumn names, drop missing rows, and rename to match our GNI\r\ndataframe.\r\n\r\n\r\n#Building and tidying waste dataframe\r\ndf_waste <- readr::read_csv(\"https://raw.githubusercontent.com/mesdi/plastic-pollution/main/plastic-waste-mismanaged.csv\")\r\n\r\ndf_waste =\r\n  df_waste %>%  \r\n  janitor::clean_names() %>% \r\n  tidyr::drop_na() %>% \r\n  select(country=entity,\r\n         waste_metric=mismanaged_plastic_waste_metric_tons_year_1)\r\n\r\ndf_waste\r\n\r\n# A tibble: 160 x 2\r\n   country             waste_metric\r\n   <chr>                      <dbl>\r\n 1 Albania                    69833\r\n 2 Algeria                   764578\r\n 3 Angola                    236946\r\n 4 Antigua and Barbuda          627\r\n 5 Argentina                 465808\r\n 6 Australia                   5266\r\n 7 Bahamas                     2212\r\n 8 Bahrain                     1043\r\n 9 Bangladesh               1021990\r\n10 Barbados                     872\r\n# ... with 150 more rows\r\n# i Use `print(n = ...)` to see more rows\r\n\r\nHow many countries do we have waste info on?\r\n\r\n\r\ndf_waste %>% \r\n  select(country) %>% \r\n  count\r\n\r\n# A tibble: 1 x 1\r\n      n\r\n  <int>\r\n1   160\r\n\r\nNow we want to unify the data to speak about GNI and waste.\r\nFirst we will left_join the df_waste with the\r\ndf_gni. Then only keep unique countries (equivalent to\r\nremove_duplicates in dplyr), filter out the world country, and drop\r\nmissing rows again that the df_gni had as country names,\r\nbut df_waste did not. This implicitly gives precendence to\r\nthe df_waste data. This is no different than doing an\r\ninner_join where it will just drop those non-matches\r\nanyway. The left_join provides the benefit of axamining\r\nwhich, and how many, rows were not matched in the join. Not a bad idea\r\nto just do, then filter.\r\n\r\n\r\n#Binding waste and gni dataframes by country\r\ndf_tidy =\r\n  df_waste %>% \r\n  dplyr::left_join(df_gni) %>% \r\n  dplyr::distinct(country, .keep_all = TRUE) %>% # remove duplicate countries\r\n  dplyr::filter(!country == \"World\") %>% \r\n  tidyr::drop_na()\r\n\r\ndf_tidy\r\n\r\n# A tibble: 142 x 4\r\n   country             waste_metric gni_metric gni_status  \r\n   <chr>                      <dbl>      <dbl> <fct>       \r\n 1 Albania                    69833       6110 upper_middle\r\n 2 Algeria                   764578       3660 lower_middle\r\n 3 Angola                    236946       1770 lower_middle\r\n 4 Antigua and Barbuda          627      14900 high        \r\n 5 Argentina                 465808      10050 upper_middle\r\n 6 Australia                   5266      56760 high        \r\n 7 Bahamas                     2212      27220 high        \r\n 8 Bahrain                     1043      19930 high        \r\n 9 Bangladesh               1021990       2620 lower_middle\r\n10 Barbados                     872      16720 high        \r\n# ... with 132 more rows\r\n# i Use `print(n = ...)` to see more rows\r\n\r\nNext identify the 6 worst counties with\r\nmismanaged_plastic_waste_metric_tons_year_1, which we renamed\r\nwaste_metric. The slice_max function is\r\nincredible here. It will take your dataframe, let you select a column to\r\norder by, then do a max operation and cut it off by some n,\r\neffectively leaving you with the top_k or the\r\nslice_max of the column ordered by something you care\r\nabout. This is great for quick assessments like:\r\nWhat are the top 6 countries by GNI?\r\nWhat are the top 6 countries w.r.t income to waste?\r\nWhat are the top 6 countries by mismanaged plastic waste metric tons\r\nper year?\r\n\r\n\r\ndf_tidy %>% \r\n  dplyr::slice_max(order_by = gni_metric, n = 6) %>% \r\n  dplyr::select(country, gni_metric) %>% \r\n  dplyr::mutate(gni_metric_label = paste0(round(gni_metric/1000, 2), \" thousand\"))\r\n\r\n# A tibble: 6 x 3\r\n  country       gni_metric gni_metric_label\r\n  <chr>              <dbl> <chr>           \r\n1 Norway             84090 84.09 thousand  \r\n2 Ireland            74520 74.52 thousand  \r\n3 United States      70430 70.43 thousand  \r\n4 Denmark            68110 68.11 thousand  \r\n5 Iceland            64410 64.41 thousand  \r\n6 Singapore          64010 64.01 thousand  \r\n\r\n\r\n\r\ndf_tidy %>% \r\n  dplyr::mutate(gni_to_waste = gni_metric / waste_metric) %>% \r\n  dplyr::slice_max(order_by = gni_to_waste, n=20) %>% \r\n    dplyr::select(country, gni_to_waste)\r\n\r\n# A tibble: 20 x 2\r\n   country               gni_to_waste\r\n   <chr>                        <dbl>\r\n 1 Montenegro                   581. \r\n 2 Iceland                      427. \r\n 3 Seychelles                   402. \r\n 4 Marshall Islands             316. \r\n 5 Saint Kitts and Nevis        191. \r\n 6 Denmark                      175. \r\n 7 Maldives                     140  \r\n 8 Palau                        124. \r\n 9 Malta                        118. \r\n10 Norway                        56.3\r\n11 Brunei                        45.5\r\n12 Estonia                       43.2\r\n13 Kiribati                      39.3\r\n14 Qatar                         37.3\r\n15 Mauritius                     36.3\r\n16 Cyprus                        33.6\r\n17 Slovenia                      33.5\r\n18 Ireland                       27.9\r\n19 New Zealand                   26.5\r\n20 Singapore                     25.9\r\n\r\n\r\n\r\n#Top 6 countries in terms of the amounts of mismanaged plastic waste \r\ndf_6 =\r\n  df_tidy %>% \r\n  dplyr::slice_max(order_by = waste_metric, n=6) %>% \r\n  dplyr::mutate(waste = paste0(round(waste_metric/10^6, 2), \" million t\"))\r\n\r\ndf_6\r\n\r\n# A tibble: 6 x 5\r\n  country     waste_metric gni_metric gni_status   waste          \r\n  <chr>              <dbl>      <dbl> <fct>        <chr>          \r\n1 India           12994100       2170 lower_middle 12.99 million t\r\n2 China           12272200      11890 upper_middle 12.27 million t\r\n3 Philippines      4025300       3640 lower_middle 4.03 million t \r\n4 Brazil           3296700       7720 upper_middle 3.3 million t  \r\n5 Nigeria          1948950       2100 lower_middle 1.95 million t \r\n6 Tanzania         1716400       1140 lower_middle 1.72 million t \r\n\r\nVisualizing the data\r\n\r\n\r\ndf_tidy %>% \r\n  \r\n  # Top 20 waste countries\r\n  dplyr::slice_max(order_by = waste_metric, n=20) %>% \r\n  \r\n  # X,Y, and color variables\r\n  ggplot(aes(x=gni_metric, y=waste_metric, color=gni_status)) +\r\n  \r\n  # Country names\r\n  geom_text(aes(label = country),\r\n            hjust= 0, vjust= -0.5, size=4, key_glyph= \"rect\") +\r\n  \r\n  # Top 6 waste labels\r\n  geom_text(data = df_6, aes(x = gni_metric, y = waste_metric, label = waste),\r\n            hjust = 0, vjust=1.2) +\r\n  \r\n  scale_x_log10(labels = scales::label_dollar(accuracy = 2)) +\r\n  scale_y_log10(labels = scales::label_number(scale_cut = cut_si(\"t\"))) +\r\n  scale_color_discrete(labels = c(\"Upper-Middle\", \"Lower-Middle\", \"Low\")) +\r\n  labs(title = \"Mismanaged Plastic Waste (2019) vs. GNI Income.\",\r\n       y = \"Plastic Waste (Metric Tons/year)\",\r\n       x = \"Gross National Income ($)\") +\r\n  coord_fixed(ratio = 0.5, clip = \"off\") +\r\n  theme_minimal()\r\n\r\n\r\n\r\nModeling with tidymodels\r\n\r\n\r\n# Remember what we're modeling\r\ndf_tidy %>% head\r\n\r\n# A tibble: 6 x 4\r\n  country             waste_metric gni_metric gni_status  \r\n  <chr>                      <dbl>      <dbl> <fct>       \r\n1 Albania                    69833       6110 upper_middle\r\n2 Algeria                   764578       3660 lower_middle\r\n3 Angola                    236946       1770 lower_middle\r\n4 Antigua and Barbuda          627      14900 high        \r\n5 Argentina                 465808      10050 upper_middle\r\n6 Australia                   5266      56760 high        \r\n\r\n# Build a receipe\r\n#   1. log(y)\r\n#   2. dummies(x)\r\n#\r\ndf_rec =\r\n  df_tidy %>% \r\n  recipes::recipe(waste_metric ~ gni_status) %>% \r\n  recipes::step_log(waste_metric, base = 10) %>% \r\n  recipes::step_dummy(gni_status)\r\n\r\ndf_rec\r\n\r\nRecipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n   outcome          1\r\n predictor          1\r\n\r\nOperations:\r\n\r\nLog transformation on waste_metric\r\nDummy variables from gni_status\r\n\r\n# Build a model\r\n#   1. Linear regression\r\n#\r\nlm_model =\r\n  linear_reg() %>% \r\n  set_engine(\"lm\")\r\n\r\nlm_model\r\n\r\nLinear Regression Model Specification (regression)\r\n\r\nComputational engine: lm \r\n\r\n# Build workflow\r\n# 1. Add model\r\n# 2. Add recipe\r\n#\r\nlm_wflow =\r\n  workflow() %>% \r\n  add_model(lm_model) %>% \r\n  add_recipe(df_rec)\r\n\r\nlm_wflow\r\n\r\n== Workflow ==========================================================\r\nPreprocessor: Recipe\r\nModel: linear_reg()\r\n\r\n-- Preprocessor ------------------------------------------------------\r\n2 Recipe Steps\r\n\r\n* step_log()\r\n* step_dummy()\r\n\r\n-- Model -------------------------------------------------------------\r\nLinear Regression Model Specification (regression)\r\n\r\nComputational engine: lm \r\n\r\n# Fit the workflow with some data\r\nlm_fit = fit(lm_wflow, df_tidy)\r\n\r\n# Descriptive and inferential statistics\r\nlm_fit %>% \r\n  extract_fit_engine() %>% \r\n  summary()\r\n\r\n\r\nCall:\r\nstats::lm(formula = ..y ~ ., data = data)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3.2939 -0.6246  0.1494  0.8228  2.6911 \r\n\r\nCoefficients:\r\n                        Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)               3.5666     0.1595  22.359  < 2e-16 ***\r\ngni_status_upper_middle   0.8313     0.2393   3.474 0.000685 ***\r\ngni_status_lower_middle   1.5965     0.2410   6.625 7.15e-10 ***\r\ngni_status_low            1.0701     0.3512   3.047 0.002768 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.128 on 138 degrees of freedom\r\nMultiple R-squared:  0.2471,    Adjusted R-squared:  0.2307 \r\nF-statistic: 15.09 on 3 and 138 DF,  p-value: 1.501e-08\r\n\r\n# ... OR ...\r\n\r\ntidy(lm_fit)\r\n\r\n# A tibble: 4 x 5\r\n  term                    estimate std.error statistic  p.value\r\n  <chr>                      <dbl>     <dbl>     <dbl>    <dbl>\r\n1 (Intercept)                3.57      0.160     22.4  1.01e-47\r\n2 gni_status_upper_middle    0.831     0.239      3.47 6.85e- 4\r\n3 gni_status_lower_middle    1.60      0.241      6.63 7.15e-10\r\n4 gni_status_low             1.07      0.351      3.05 2.77e- 3\r\n\r\nVariable importance\r\n\r\n\r\n#variable importance  \r\nlm_fit %>% \r\n  extract_fit_parsnip() %>%\r\n  vip(aesthetics = list(color = \"lightblue\", fill = \"lightblue\")) + \r\n  theme_minimal() +\r\n  ggtitle(\"Variable Importance\", subtitle = \"Lower Middle Status Indicates Higher Waste\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-31-gni-per-capita-and-plastic-pollution-analyss-in-r/gni-per-capita-and-plastic-pollution-analyss-in-r_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2022-07-31T14:32:49-04:00",
    "input_file": "gni-per-capita-and-plastic-pollution-analyss-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-30-two-way-anova-in-r/",
    "title": "Two-way ANOVA in R",
    "description": "In this post we will reference an interaction plot post that uses two-way anova in R.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-30",
    "categories": [
      "statistics",
      "anova",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nBottom Line Up Front\r\nOverview\r\nData\r\n\r\nFit ANOVA\r\nANOVA Assumptions\r\n\r\nInteraction Plot\r\nReferneces\r\n\r\nBottom Line Up Front\r\nIt’s called a Two-way ANOVA, because we are exploring\r\ntwo independent factor variables difference in means\r\n(variance) against a single response variable.\r\nIf an interaction.plot shows crossing lines for a\r\ntwo-way analysis, it is likely to have an effect on the\r\nresponse in concert.\r\nOverview\r\nIn the original\r\npost on this the author walks through how to create an interaction\r\nplot using ANOVA in R. I wanted to re-post this as it falls under\r\nstatistical and optimization concerns in the case of\r\nn variables and to curate this information accordingly.\r\n\r\n“To find out if the means of three or more independent groups that\r\nhave been divided based on two factors differ, a two-way ANOVA is\r\nperformed.\r\nWhen we want to determine whether two distinct factors have an impact on\r\na certain response variable, we employ a two-way ANOVA.”\r\n\r\nData\r\n\r\n\r\nset.seed(123)\r\n\r\n# Two independent variables: Factors\r\n# One response variable: Numeric\r\n#\r\n# 60 observations, 30 for gender duplication, 20 for exercise duplication. \r\n#\r\ndata <- data.frame(gender = rep(c(\"Male\", \"Female\"), \r\n                                each = 30, times = 1),\r\n                   exercise = rep(c(\"None\", \"Light\", \"Intense\"),\r\n                                  each = 10, times = 2),\r\n                   weight_loss = c(runif(10, -3, 3),\r\n                                   runif(10, 0, 5),\r\n                                   runif(10, 5, 9),\r\n                                   runif(10, -4, 2),\r\n                                   runif(10, 0, 3), \r\n                                   runif(10, 3, 8)))\r\n\r\n# Sample from our data\r\nhead(data)\r\n\r\n  gender exercise weight_loss\r\n1   Male     None  -1.2745349\r\n2   Male     None   1.7298308\r\n3   Male     None  -0.5461385\r\n4   Male     None   2.2981044\r\n5   Male     None   2.6428037\r\n6   Male     None  -2.7266610\r\n\r\ne.grid = expand.grid(gender = c(\"Male\", \"Female\"),\r\n            exercise = c(\"None\", \"Light\", \"Intense\"))\r\n\r\n# All independent factor combinations\r\ne.grid\r\n\r\n  gender exercise\r\n1   Male     None\r\n2 Female     None\r\n3   Male    Light\r\n4 Female    Light\r\n5   Male  Intense\r\n6 Female  Intense\r\n\r\n# Number of groups\r\nnrow(e.grid)\r\n\r\n[1] 6\r\n\r\nFit ANOVA\r\nFor our experiment, we are interested in how gender and\r\nexercise impact weight loss.\r\n\r\n“Let’s say researchers want to know if gender and activity volume\r\naffect weight loss.\r\nThey enlist 30 men and 30 women to take part in an experiment where\r\n10 of each gender are randomly assigned to follow a program of\r\neither no activity, light exercise, or severe exercise for one month in\r\norder to test this.\r\nTo see the interaction impact between exercise and gender, use the\r\nfollowing procedures to generate a data frame in R, run a two-way ANOVA,\r\nand create an interactive graphic.”\r\n\r\n\r\n\r\nmodel <- aov(weight_loss ~ gender*exercise, data = data)\r\nsummary(model)\r\n\r\n                Df Sum Sq Mean Sq F value   Pr(>F)    \r\ngender           1   43.7   43.72  19.032 5.83e-05 ***\r\nexercise         2  438.9  219.43  95.515  < 2e-16 ***\r\ngender:exercise  2    2.9    1.46   0.634    0.535    \r\nResiduals       54  124.1    2.30                     \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nAt a glance, we can see that independently, gender or\r\nexercise are significant, that is one or the other.\r\nHowever, together there is no significant variance in weight loss. So we\r\ncan see that gender as an individual factor is significant toward weight\r\nloss (up or down, will require regression or interaction plots). Same\r\ngoes for exericse. However together, the influence is not strong, that\r\nis they don’t together influence weight loss\r\nsignificantly. Another way to think about it is that their product\r\ndoesn’t offer any additional information. Let’s take a look at the\r\ninteraction plot to see if this can validate the things the model\r\ndiscovered.\r\nANOVA Assumptions\r\nThis is just a little gotcha when using ANOVA.\r\n*Independence of variables**: The two variables for testing should\r\nbe independent of each other. One should not affect the other, or else\r\nit could result in skewness . This means that one cannot use the two-way\r\nANOVA test in settings with categorical variables.\r\nHomoscedasticity: In a two-way ANOVA test, the\r\nvariance should be homogeneous. The variation around the mean for each\r\nset of data should not vary significantly for all the groups.\r\nNormal distribution of variables: The two variables\r\nin a two-way ANOVA test should have a normal distribution . When plotted\r\nindividually, each should have a bell curve . If the data does not meet\r\nthis criterion, one could attempt statistical data transformation to\r\nachieve the desired result.\r\n\r\n\r\nlibrary(ggplot2)\r\nggplot(data = data) +\r\n  geom_histogram(aes(x = weight_loss, fill=gender)) +\r\n  facet_grid(gender ~ exercise, scales=\"free\") + \r\n  xlab(\"Weight Loss (Centered and Scaled)\") + \r\n  ylab(\"Count\") + ggtitle(\"Glance for Weight Loss Normality\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nInteraction Plot\r\nNow let’s take a look and see\r\n\r\n“An interaction plot is the most effective tool for spotting\r\nand comprehending the effects of interactions between two\r\nvariables.\r\nThis particular plot style shows the values of the first factor on\r\nthe x-axis and the fitted values of the response variable on the\r\ny-axis.\r\nThe values of the second component of interest are depicted by the\r\nlines in the plot.”\r\n\r\n\r\n\r\ninteraction.plot(x.factor = data$exercise, #x-axis variable \r\n                 trace.factor = data$gender, #variable for lines\r\n                 response = data$weight_loss, #y-axis variable\r\n                 fun = median, #metric to plot\r\n                 main = \"Weight Loss Two-way ANOVA Interaction\",\r\n                 ylab = \"Weight Loss\",\r\n                 xlab = \"Exercise Intensity\",\r\n                 col = c(\"pink\", \"blue\"),\r\n                 lty = 1, #line type\r\n                 lwd = 2, #line width\r\n                 trace.label = \"Gender\"\r\n)\r\n\r\n\r\n\r\n\r\n“In general, there is no interaction effect if the two lines on the\r\ninteraction plot are parallel. However, there will probably be\r\nan interaction effect if the lines cross.\r\nAs we can see in this plot, there is no intersection between the\r\nlines for men and women, suggesting that there is no interaction between\r\nthe variables of exercise intensity and gender.\r\nThis is consistent with the p-value in the ANOVA table’s output,\r\nwhich indicated that the interaction term in the ANOVA model was\r\nnot statistically significant.”\r\n\r\nReferneces\r\nhttps://www.r-bloggers.com/2022/07/how-to-create-an-interaction-plot-in-r/\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-30-two-way-anova-in-r/two-way-anova-in-r_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-07-30T11:39:44-04:00",
    "input_file": "two-way-anova-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-26-minimax-chess-engine-in-r/",
    "title": "Minimax Chess Engine in R",
    "description": "In this post we will walk through a reference to a minimax chess game engine written entirely in R.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-26",
    "categories": [
      "artificial intelligence",
      "game theory",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nOverview\r\nCreate a new chess game\r\nSimulate 30 moves\r\nSimulate a random game\r\nImproving the chess engine\r\nPosition &\r\nMaterial Evaluation Function\r\n\r\nMinimax Search function\r\n\r\nReferences\r\n\r\nOverview\r\nThis week I found a wonderful\r\npost developing a mini-max algorithm in R which I thought was\r\nperfect for a “retweet” and discussion here in this optimization forum.\r\nThis uses the rchess\r\npackage.\r\nLet’s dig right in and start discussing what’s up.\r\nCreate a new chess game\r\n\r\n\r\nlibrary(rchess)\r\n\r\nchess_game <- Chess$new()\r\nchess_game$moves()\r\n\r\n [1] \"a3\"  \"a4\"  \"b3\"  \"b4\"  \"c3\"  \"c4\"  \"d3\"  \"d4\"  \"e3\"  \"e4\"  \"f3\" \r\n[12] \"f4\"  \"g3\"  \"g4\"  \"h3\"  \"h4\"  \"Na3\" \"Nc3\" \"Nf3\" \"Nh3\"\r\n\r\nchess_game$move(\"e4\")\r\nchess_game$turn()\r\n\r\n[1] \"b\"\r\n\r\nchess_game$moves()\r\n\r\n [1] \"Nc6\" \"Na6\" \"Nh6\" \"Nf6\" \"a6\"  \"a5\"  \"b6\"  \"b5\"  \"c6\"  \"c5\"  \"d6\" \r\n[12] \"d5\"  \"e6\"  \"e5\"  \"f6\"  \"f5\"  \"g6\"  \"g5\"  \"h6\"  \"h5\" \r\n\r\nchess_game$move(\"a5\")\r\nplot(chess_game)\r\n\r\n\r\n\r\nSimulate 30 moves\r\n\r\n\r\nget_random_move <- function(chess_game) {\r\n  valid_moves = chess_game$moves()\r\n  choice = sample(valid_moves, size = 1)\r\n  return(choice)\r\n}\r\n\r\n\r\n\r\n\r\n# Set up a new game\r\nchess_game <- Chess$new()\r\n\r\n# Simulate just 50 turns (back and forth, white then black)\r\nfor(i in 1:30){\r\n  chess_game$move(get_random_move(chess_game))\r\n}\r\nplot(chess_game)\r\n\r\n\r\n\r\n\r\n\r\n\r\nSome other helpful functions that could be used are below also.\r\n\r\n\r\nchess_game$game_over()\r\n\r\n[1] FALSE\r\n\r\n## [1] TRUE\r\nchess_game$in_checkmate()\r\n\r\n[1] FALSE\r\n\r\n## [1] TRUE\r\nchess_game$in_check()\r\n\r\n[1] FALSE\r\n\r\n## [1] TRUE\r\n\r\n\r\nSimulate a random game\r\n\r\n\r\n# Set up a new game\r\nchess_game <- Chess$new()\r\n\r\n# Perform random legal moves until the game ends in mate or draw\r\nwhile (!chess_game$game_over()) {\r\n  chess_game$move(get_random_move(chess_game))\r\n  # chess_game$turn\r\n}\r\n\r\n# Plot the final position\r\nplot(chess_game)\r\n\r\n\r\n\r\nImproving the chess engine\r\nAs the article referenced suggests, there are a couple things we’d\r\nlike to do now,\r\n\r\nEvaluation function: implement a simple heuristic to\r\nevaluate individual positions based on the pieces on the board and the\r\nsafety of the king Search function: implement an\r\nalgorithm that allows our engine to search through positions multiple\r\nmoves ahead\r\n\r\nPosition & Material\r\nEvaluation Function\r\n\r\n\r\n# Position evaluation function\r\nevaluate_position <- function(position) {\r\n  # Test if black won\r\n  if (position$in_checkmate() & (position$turn() == \"w\")) {\r\n    return(-1000)\r\n  }\r\n  # Test if white won\r\n  if (position$in_checkmate() & (position$turn() == \"b\")) {\r\n    return(1000)\r\n  }\r\n  # Test if game ended in a draw\r\n  if (position$game_over()) {\r\n    return(0)\r\n  }\r\n  # Compute material advantage\r\n  position_fen <- strsplit(strsplit(position$fen(), split = \" \")[[1]][1], split = \"\")[[1]]\r\n  white_score <- length(which(position_fen == \"Q\")) * 9 + \r\n                  length(which(position_fen == \"R\")) * 5 + \r\n                  length(which(position_fen == \"B\")) * 3 + \r\n                  length(which(position_fen == \"N\")) * 3 + \r\n                  length(which(position_fen == \"P\"))\r\n  black_score <- length(which(position_fen == \"q\")) * 9 + \r\n                  length(which(position_fen == \"r\")) * 5 + \r\n                  length(which(position_fen == \"b\")) * 3 +\r\n                  length(which(position_fen == \"n\")) * 3 +\r\n                  length(which(position_fen == \"p\"))\r\n  \r\n  # Some grand masters say that board centrality is an advantage\r\n  # Board centrality points here here ...\r\n  \r\n  # Evaluate king safety\r\n  check_score <- 0\r\n  if (position$in_check() & (position$turn() == \"w\")) \r\n    check_score <- -1\r\n  if (position$in_check() & (position$turn() == \"b\"))\r\n    check_score <- 1\r\n  \r\n  # Return final position score\r\n  return(white_score - black_score + check_score)\r\n}\r\n# \r\n# chess_game <- Chess$new()\r\n# chess_game$moves()\r\n# chess_game$move(\"e4\")\r\n# chess_game$turn()\r\n# chess_game$moves()\r\n# chess_game$move(\"a5\")\r\n# chess_game$fen()\r\n# plot(chess_game)\r\n\r\nevaluate_position(chess_game)\r\n\r\n[1] 1000\r\n\r\nMinimax Search function\r\nThis code was happily stolen from the tutorial referenced. But in\r\nessence, this recursive function will take the current board and some\r\ndepth. For the current players move, it will assess the array of\r\npossible moves. Decreasing depth it will pass in again. This continues\r\nuntil the depth is zero and returns the points from\r\nevaluate_position. Notice the exit condition is always the\r\nfirst section in a recursive function like this. Each move is gentle\r\nundone after stepping into it.\r\nOnce the collection of possible next_move_scores has\r\nbeen populated. For each possible player white or\r\nblack, the max is returned for the white turn\r\nand min for the black.\r\n\r\n\r\n# Score position via minimax strategy\r\nminimax_scoring <- function(chess_game, depth) {\r\n  # If the game is already over or the depth limit is reached\r\n  # then return the heuristic evaluation of the position\r\n  if (depth == 0 | chess_game$game_over()) {\r\n    return(evaluate_position(chess_game))\r\n  }\r\n  # Run the minimax scoring recursively on every legal next move, making sure the search depth is not exceeded\r\n  next_moves <- chess_game$moves()\r\n  next_move_scores <- vector(length = length(next_moves))\r\n  for (i in 1:length(next_moves)) {\r\n    chess_game$move(next_moves[i])\r\n    next_move_scores[i] <- minimax_scoring(chess_game, depth - 1)\r\n    chess_game$undo()\r\n  }\r\n  # White will select the move that maximizes the position score\r\n  # Black will select the move that minimizes the position score\r\n  if (chess_game$turn() == \"w\") {\r\n    return(max(next_move_scores))\r\n  } else {\r\n    return(min(next_move_scores))\r\n  }\r\n}\r\n\r\n\r\n# Select the next move based on the minimax scoring\r\nget_minimax_move <- function(chess_game) {\r\n  # Score all next moves via minimax\r\n  next_moves <- chess_game$moves()\r\n  next_move_scores <- vector(length = length(next_moves))\r\n  for (i in 1:length(next_moves)) {\r\n    chess_game$move(next_moves[i])\r\n    # To ensure fast execution of the minimax function we select a depth of 1\r\n    # This depth can be increased to enable stronger play at the expense of longer runtime\r\n    next_move_scores[i] <- minimax_scoring(chess_game, 1)\r\n    chess_game$undo()\r\n  }\r\n  # For white return the move with maximum score\r\n  # For black return the move with minimum score\r\n  # If the optimal score is achieved by multiple moves, select one at random\r\n  # This random selection from the optimal moves adds some variability to the play\r\n  if (chess_game$turn() == \"w\") {\r\n    return(sample(next_moves[which(next_move_scores == max(next_move_scores))], size = 1))\r\n  } else {\r\n    return(sample(next_moves[which(next_move_scores == min(next_move_scores))], size = 1))\r\n  }\r\n}\r\n\r\n\r\n\r\n\r\n# Function that takes a side as input (\"w\" or \"b\") and plays 10 games\r\n# The selected side will choose moves based on the minimax algorithm\r\n# The opponent will use the random move generator\r\nplay_5_games <- function(minimax_player) {\r\n  game_results <- vector(length = 5)\r\n  for (i in 1:10) {\r\n    chess_game <- Chess$new()\r\n    while (!chess_game$game_over()) {\r\n      if (chess_game$turn() == minimax_player) {\r\n        # Selected player uses the minimax strategy\r\n        chess_game$move(get_minimax_move(chess_game))\r\n      } else {\r\n        # Opponent uses the random move generator\r\n        chess_game$move(get_random_move(chess_game))\r\n      }\r\n    }\r\n    # Record the result of the current finished game\r\n    # If mate: the losing player is recorded\r\n    # If draw: record a 0\r\n    if (chess_game$in_checkmate()) {\r\n      game_results[i] <- chess_game$turn()\r\n    } else {\r\n      game_results[i] <- \"0\"\r\n    }\r\n  }\r\n  # Print the outcome of the 10 games\r\n  print(table(game_results))\r\n}\r\n\r\n\r\n\r\n\r\n# play_10_games(\"w\")\r\n## game_results\r\n##  b \r\n## 10\r\n\r\n\r\n\r\n\r\n# play_10_games(\"b\")\r\n## game_results\r\n##  w \r\n## 10\r\n\r\n\r\nReferences\r\n https://www.r-bloggers.com/2022/07/programming-a-simple-minimax-chess-engine-in-r/\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-26-minimax-chess-engine-in-r/chess.png",
    "last_modified": "2022-07-28T15:54:04-04:00",
    "input_file": "minimax-chess-engine-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-24-recipes-in-r/",
    "title": "Recipes in R",
    "description": "In this post we will discuss how to use recipes to build effective machine learning pipelines in R.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-24",
    "categories": [
      "machine learning",
      "data pipeline",
      "data cleaning",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nGentle overview\r\nGeting data\r\nSimple recipe\r\n\r\nAn Example\r\nGetting data\r\nAn initial recipe\r\nProcessing Steps\r\nChecks\r\n\r\nFull example\r\nGetting creative\r\nMake a model\r\nCross-validation\r\nand Model performance\r\nFit the whole model\r\nRecipe visualization\r\n\r\n\r\nReferences\r\n\r\nIntroduction\r\nrecipes is a built in package I found within\r\ntidymodels. We actually wrote a section on this in our last\r\npost showing how to 1) add roles, 2) create features, and 3) add\r\nrecipes to model workflows. In another post we will go\r\nthrough the rsample package which I discovered through the\r\nsame means. However for recipes, it offers a very clean\r\ndplyr-like syntax to pass data through before modeling\r\ntakes place. This helps for a variety of reasons:\r\ndata pre-processing,\r\ndata cleaning,\r\nfeature engineering,\r\nreproducibility, and\r\nstreamlining\r\nEnough chit chat, let’s dive in and discuss what’s going on.\r\nGentle overview\r\nGeting data\r\n\r\n\r\nlibrary(recipes)\r\n\r\ndata(ad_data, package=\"modeldata\")\r\n\r\nad_data %>% glimpse\r\n\r\nRows: 333\r\nColumns: 131\r\n$ ACE_CD143_Angiotensin_Converti   <dbl> 2.0031003, 1.5618560, 1.520~\r\n$ ACTH_Adrenocorticotropic_Hormon  <dbl> -1.3862944, -1.3862944, -1.~\r\n$ AXL                              <dbl> 1.09838668, 0.68328157, -0.~\r\n$ Adiponectin                      <dbl> -5.360193, -5.020686, -5.80~\r\n$ Alpha_1_Antichymotrypsin         <dbl> 1.7404662, 1.4586150, 1.193~\r\n$ Alpha_1_Antitrypsin              <dbl> -12.631361, -11.909882, -13~\r\n$ Alpha_1_Microglobulin            <dbl> -2.577022, -3.244194, -2.88~\r\n$ Alpha_2_Macroglobulin            <dbl> -72.65029, -154.61228, -136~\r\n$ Angiopoietin_2_ANG_2             <dbl> 1.06471074, 0.74193734, 0.8~\r\n$ Angiotensinogen                  <dbl> 2.510547, 2.457283, 1.97636~\r\n$ Apolipoprotein_A_IV              <dbl> -1.427116, -1.660731, -1.66~\r\n$ Apolipoprotein_A1                <dbl> -7.402052, -7.047017, -7.68~\r\n$ Apolipoprotein_A2                <dbl> -0.26136476, -0.86750057, -~\r\n$ Apolipoprotein_B                 <dbl> -4.624044, -6.747507, -3.97~\r\n$ Apolipoprotein_CI                <dbl> -1.2729657, -1.2729657, -1.~\r\n$ Apolipoprotein_CIII              <dbl> -2.312635, -2.343407, -2.74~\r\n$ Apolipoprotein_D                 <dbl> 2.0794415, 1.3350011, 1.335~\r\n$ Apolipoprotein_E                 <dbl> 3.7545215, 3.0971187, 2.753~\r\n$ Apolipoprotein_H                 <dbl> -0.15734908, -0.57539617, -~\r\n$ B_Lymphocyte_Chemoattractant_BL  <dbl> 2.2969819, 1.6731213, 1.673~\r\n$ BMP_6                            <dbl> -2.200744, -1.728053, -2.06~\r\n$ Beta_2_Microglobulin             <dbl> 0.69314718, 0.47000363, 0.3~\r\n$ Betacellulin                     <int> 34, 53, 49, 52, 67, 51, 41,~\r\n$ C_Reactive_Protein               <dbl> -4.074542, -6.645391, -8.04~\r\n$ CD40                             <dbl> -0.7964147, -1.2733760, -1.~\r\n$ CD5L                             <dbl> 0.09531018, -0.67334455, 0.~\r\n$ Calbindin                        <dbl> 33.21363, 25.27636, 22.1660~\r\n$ Calcitonin                       <dbl> 1.3862944, 3.6109179, 2.116~\r\n$ CgA                              <dbl> 397.6536, 465.6759, 347.863~\r\n$ Clusterin_Apo_J                  <dbl> 3.555348, 3.044522, 2.77258~\r\n$ Complement_3                     <dbl> -10.36305, -16.10824, -16.1~\r\n$ Complement_Factor_H              <dbl> 3.5737252, 3.6000471, 4.474~\r\n$ Connective_Tissue_Growth_Factor  <dbl> 0.5306283, 0.5877867, 0.641~\r\n$ Cortisol                         <dbl> 10.0, 12.0, 10.0, 14.0, 11.~\r\n$ Creatine_Kinase_MB               <dbl> -1.710172, -1.751002, -1.38~\r\n$ Cystatin_C                       <dbl> 9.041922, 9.067624, 8.95415~\r\n$ EGF_R                            <dbl> -0.1354543, -0.3700474, -0.~\r\n$ EN_RAGE                          <dbl> -3.688879, -3.816713, -4.75~\r\n$ ENA_78                           <dbl> -1.349543, -1.356595, -1.39~\r\n$ Eotaxin_3                        <int> 53, 62, 62, 44, 64, 57, 64,~\r\n$ FAS                              <dbl> -0.08338161, -0.52763274, -~\r\n$ FSH_Follicle_Stimulation_Hormon  <dbl> -0.6516715, -1.6272839, -1.~\r\n$ Fas_Ligand                       <dbl> 3.1014922, 2.9788133, 1.360~\r\n$ Fatty_Acid_Binding_Protein       <dbl> 2.5208712, 2.2477966, 0.906~\r\n$ Ferritin                         <dbl> 3.329165, 3.932959, 3.17687~\r\n$ Fetuin_A                         <dbl> 1.2809338, 1.1939225, 1.410~\r\n$ Fibrinogen                       <dbl> -7.035589, -8.047190, -7.19~\r\n$ GRO_alpha                        <dbl> 1.381830, 1.372438, 1.41267~\r\n$ Gamma_Interferon_induced_Monokin <dbl> 2.949822, 2.721793, 2.76223~\r\n$ Glutathione_S_Transferase_alpha  <dbl> 1.0641271, 0.8670202, 0.889~\r\n$ HB_EGF                           <dbl> 6.559746, 8.754531, 7.74546~\r\n$ HCC_4                            <dbl> -3.036554, -4.074542, -3.64~\r\n$ Hepatocyte_Growth_Factor_HGF     <dbl> 0.58778666, 0.53062825, 0.0~\r\n$ I_309                            <dbl> 3.433987, 3.135494, 2.39789~\r\n$ ICAM_1                           <dbl> -0.1907787, -0.4620172, -0.~\r\n$ IGF_BP_2                         <dbl> 5.609472, 5.347108, 5.18178~\r\n$ IL_11                            <dbl> 5.121987, 4.936704, 4.66591~\r\n$ IL_13                            <dbl> 1.282549, 1.269463, 1.27413~\r\n$ IL_16                            <dbl> 4.192081, 2.876338, 2.61610~\r\n$ IL_17E                           <dbl> 5.731246, 6.705891, 4.14932~\r\n$ IL_1alpha                        <dbl> -6.571283, -8.047190, -8.18~\r\n$ IL_3                             <dbl> -3.244194, -3.912023, -4.64~\r\n$ IL_4                             <dbl> 2.484907, 2.397895, 1.82454~\r\n$ IL_5                             <dbl> 1.09861229, 0.69314718, -0.~\r\n$ IL_6                             <dbl> 0.26936976, 0.09622438, 0.1~\r\n$ IL_6_Receptor                    <dbl> 0.64279595, 0.43115645, 0.0~\r\n$ IL_7                             <dbl> 4.8050453, 3.7055056, 1.005~\r\n$ IL_8                             <dbl> 1.711325, 1.675557, 1.69139~\r\n$ IP_10_Inducible_Protein_10       <dbl> 6.242223, 5.686975, 5.04985~\r\n$ IgA                              <dbl> -6.812445, -6.377127, -6.31~\r\n$ Insulin                          <dbl> -0.6258253, -0.9431406, -1.~\r\n$ Kidney_Injury_Molecule_1_KIM_1   <dbl> -1.204295, -1.197703, -1.19~\r\n$ LOX_1                            <dbl> 1.7047481, 1.5260563, 1.163~\r\n$ Leptin                           <dbl> -1.5290628, -1.4660558, -1.~\r\n$ Lipoprotein_a                    <dbl> -4.268698, -4.933674, -5.84~\r\n$ MCP_1                            <dbl> 6.740519, 6.849066, 6.76734~\r\n$ MCP_2                            <dbl> 1.9805094, 1.8088944, 0.400~\r\n$ MIF                              <dbl> -1.237874, -1.897120, -2.30~\r\n$ MIP_1alpha                       <dbl> 4.968453, 3.690160, 4.04950~\r\n$ MIP_1beta                        <dbl> 3.258097, 3.135494, 2.39789~\r\n$ MMP_2                            <dbl> 4.478566, 3.781473, 2.86663~\r\n$ MMP_3                            <dbl> -2.207275, -2.465104, -2.30~\r\n$ MMP10                            <dbl> -3.270169, -3.649659, -2.73~\r\n$ MMP7                             <dbl> -3.7735027, -5.9681907, -4.~\r\n$ Myoglobin                        <dbl> -1.89711998, -0.75502258, -~\r\n$ NT_proBNP                        <dbl> 4.553877, 4.219508, 4.24849~\r\n$ NrCAM                            <dbl> 5.003946, 5.209486, 4.74493~\r\n$ Osteopontin                      <dbl> 5.356586, 6.003887, 5.01728~\r\n$ PAI_1                            <dbl> 1.00350156, -0.03059880, 0.~\r\n$ PAPP_A                           <dbl> -2.902226, -2.813276, -2.93~\r\n$ PLGF                             <dbl> 4.442651, 4.025352, 4.51086~\r\n$ PYY                              <dbl> 3.218876, 3.135494, 2.89037~\r\n$ Pancreatic_polypeptide           <dbl> 0.5787808, 0.3364722, -0.89~\r\n$ Prolactin                        <dbl> 0.00000000, -0.51082562, -0~\r\n$ Prostatic_Acid_Phosphatase       <dbl> -1.620527, -1.739232, -1.63~\r\n$ Protein_S                        <dbl> -1.784998, -2.463991, -2.25~\r\n$ Pulmonary_and_Activation_Regulat <dbl> -0.8439701, -2.3025851, -1.~\r\n$ RANTES                           <dbl> -6.214608, -6.938214, -6.64~\r\n$ Resistin                         <dbl> -16.475315, -16.025283, -16~\r\n$ S100b                            <dbl> 1.5618560, 1.7566212, 1.435~\r\n$ SGOT                             <dbl> -0.94160854, -0.65392647, 0~\r\n$ SHBG                             <dbl> -1.897120, -1.560648, -2.20~\r\n$ SOD                              <dbl> 5.609472, 5.814131, 5.72358~\r\n$ Serum_Amyloid_P                  <dbl> -5.599422, -6.119298, -5.38~\r\n$ Sortilin                         <dbl> 4.908629, 5.478731, 3.81018~\r\n$ Stem_Cell_Factor                 <dbl> 4.174387, 3.713572, 3.43398~\r\n$ TGF_alpha                        <dbl> 8.649098, 11.331619, 10.858~\r\n$ TIMP_1                           <dbl> 15.204651, 11.266499, 12.28~\r\n$ TNF_RII                          <dbl> -0.06187540, -0.32850407, -~\r\n$ TRAIL_R3                         <dbl> -0.1829004, -0.5007471, -0.~\r\n$ TTR_prealbumin                   <dbl> 2.944439, 2.833213, 2.94443~\r\n$ Tamm_Horsfall_Protein_THP        <dbl> -3.095810, -3.111190, -3.16~\r\n$ Thrombomodulin                   <dbl> -1.340566, -1.675252, -1.53~\r\n$ Thrombopoietin                   <dbl> -0.1026334, -0.6733501, -0.~\r\n$ Thymus_Expressed_Chemokine_TECK  <dbl> 4.149327, 3.810182, 2.79199~\r\n$ Thyroid_Stimulating_Hormone      <dbl> -3.863233, -4.828314, -4.99~\r\n$ Thyroxine_Binding_Globulin       <dbl> -1.4271164, -1.6094379, -1.~\r\n$ Tissue_Factor                    <dbl> 2.04122033, 2.02814825, 1.4~\r\n$ Transferrin                      <dbl> 3.332205, 2.890372, 2.89037~\r\n$ Trefoil_Factor_3_TFF3            <dbl> -3.381395, -3.912023, -3.72~\r\n$ VCAM_1                           <dbl> 3.258097, 2.708050, 2.63905~\r\n$ VEGF                             <dbl> 22.03456, 18.60184, 17.4761~\r\n$ Vitronectin                      <dbl> -0.04082199, -0.38566248, -~\r\n$ von_Willebrand_Factor            <dbl> -3.146555, -3.863233, -3.54~\r\n$ age                              <dbl> 0.9876238, 0.9861496, 0.986~\r\n$ tau                              <dbl> 6.297754, 6.659294, 6.27098~\r\n$ p_tau                            <dbl> 4.348108, 4.859967, 4.40024~\r\n$ Ab_42                            <dbl> 12.019678, 11.015759, 12.30~\r\n$ male                             <dbl> 0, 0, 1, 0, 0, 1, 1, 1, 0, ~\r\n$ Genotype                         <fct> E3E3, E3E4, E3E4, E3E4, E3E~\r\n$ Class                            <fct> Control, Control, Control, ~\r\n\r\nThis data is entirely numeric with a response variable\r\nthat is binary. We can also see it is a factor.\r\n\r\n\r\nad_data %>% count(Class) %>% mutate(prop = n/sum(n))\r\n\r\n# A tibble: 2 x 3\r\n  Class        n  prop\r\n  <fct>    <int> <dbl>\r\n1 Impaired    91 0.273\r\n2 Control    242 0.727\r\n\r\nSimple recipe\r\n\r\n\r\nad_recipe = \r\n  recipe(Class ~ tau + VEGF, data = ad_data) %>% \r\n  step_normalize(all_numeric_predictors())\r\n\r\nad_recipe\r\n\r\nRecipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n   outcome          1\r\n predictor          2\r\n\r\nOperations:\r\n\r\nCentering and scaling for all_numeric_predictors()\r\n\r\nWith the recipes package, you create a standard recipe\r\nfor the model itself. This will contain outcomes and\r\npredictors. These are synonymous with dependent and\r\nindependent variables.\r\nThen once you create the recipe formula, each step will\r\ndo some type of work against it. For example, this step\r\nnormalized all predictors. Pretty slick! And easy..\r\nAn Example\r\nGetting data\r\nI really like this modeldata package. The developers often use it and\r\nit has a rich spread of unique data. For this example it is credit card\r\ninformation, which is very interesting, and contains enough nominal\r\ninformation to be as practical as it gets out there in the real\r\nworld.\r\n\r\n\r\nlibrary(recipes)\r\nlibrary(rsample)\r\nlibrary(modeldata)\r\n\r\ndata(\"credit_data\")\r\nset.seed(55)\r\n\r\ncredit_data %>% glimpse\r\n\r\nRows: 4,454\r\nColumns: 14\r\n$ Status    <fct> good, good, bad, good, good, good, good, good, goo~\r\n$ Seniority <int> 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0~\r\n$ Home      <fct> rent, rent, owner, rent, rent, owner, owner, paren~\r\n$ Time      <int> 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60~\r\n$ Age       <int> 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30~\r\n$ Marital   <fct> married, widow, married, single, single, married, ~\r\n$ Records   <fct> no, no, yes, no, no, no, no, no, no, no, no, no, n~\r\n$ Job       <fct> freelance, fixed, freelance, fixed, fixed, fixed, ~\r\n$ Expenses  <int> 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75~\r\n$ Income    <int> 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 12~\r\n$ Assets    <int> 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 400~\r\n$ Debt      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, ~\r\n$ Amount    <int> 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1~\r\n$ Price     <int> 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957~\r\n\r\nIt looks like our outcome is the Status\r\nwith several other numeric and nominal\r\npredictors.\r\n\r\n\r\ncredit_data %>% count(Status) %>% mutate(prop = n/sum(n))\r\n\r\n  Status    n      prop\r\n1    bad 1254 0.2815447\r\n2   good 3200 0.7184553\r\n\r\n\r\n\r\ntrain_test_split = credit_data %>% initial_split(prop = 3/4, strata = Status)\r\ncredit_train = training(train_test_split)\r\ncredit_test = testing(train_test_split)\r\n\r\ncredit_train %>% glimpse\r\n\r\nRows: 3,340\r\nColumns: 14\r\n$ Status    <fct> bad, bad, bad, bad, bad, bad, bad, bad, bad, bad, ~\r\n$ Seniority <int> 10, 0, 0, 0, 2, 3, 0, 0, 1, 5, 2, 1, 4, 1, 0, 25, ~\r\n$ Home      <fct> owner, parents, other, ignore, rent, owner, NA, ow~\r\n$ Time      <int> 36, 48, 18, 48, 60, 24, 48, 36, 54, 48, 36, 48, 42~\r\n$ Age       <int> 46, 41, 21, 36, 25, 23, 37, 23, 36, 31, 27, 29, 27~\r\n$ Marital   <fct> married, married, single, married, single, married~\r\n$ Records   <fct> yes, no, yes, no, no, no, no, no, no, no, no, yes,~\r\n$ Job       <fct> freelance, partime, partime, partime, fixed, fixed~\r\n$ Expenses  <int> 90, 90, 35, 45, 46, 75, 35, 45, 70, 44, 48, 85, 35~\r\n$ Income    <int> 200, 80, 50, 130, 107, 85, NA, 122, 99, 90, 128, 1~\r\n$ Assets    <int> 3000, 0, 0, 750, 0, 5000, NA, 2500, 0, 0, 0, 0, 0,~\r\n$ Debt      <int> 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\r\n$ Amount    <int> 2000, 1200, 400, 1100, 1500, 600, 1500, 400, 950, ~\r\n$ Price     <int> 2985, 1468, 500, 1511, 2189, 1600, 1850, 400, 950,~\r\n\r\ncredit_test %>% glimpse\r\n\r\nRows: 1,114\r\nColumns: 14\r\n$ Status    <fct> good, good, good, good, bad, good, good, good, bad~\r\n$ Seniority <int> 17, 6, 8, 19, 1, 12, 14, 10, 0, 2, 3, 14, 8, 10, 4~\r\n$ Home      <fct> rent, owner, owner, priv, owner, owner, priv, owne~\r\n$ Time      <int> 60, 48, 60, 36, 60, 36, 24, 24, 60, 60, 12, 24, 48~\r\n$ Age       <int> 58, 34, 30, 37, 45, 54, 51, 56, 32, 43, 37, 55, 31~\r\n$ Marital   <fct> widow, married, married, married, married, married~\r\n$ Records   <fct> no, no, no, no, no, no, no, no, yes, no, no, no, n~\r\n$ Job       <fct> fixed, freelance, fixed, fixed, partime, fixed, pa~\r\n$ Expenses  <int> 48, 60, 75, 75, 105, 60, 75, 45, 45, 75, 60, 90, 9~\r\n$ Income    <int> 131, 125, 199, 170, 112, 130, 198, 208, 142, 71, 1~\r\n$ Assets    <int> 0, 4000, 5000, 3500, 2000, 4000, 1000, 13500, 7000~\r\n$ Debt      <int> 0, 0, 2500, 260, 500, 0, 0, 0, 3000, 0, 0, 0, 0, 0~\r\n$ Amount    <int> 1000, 1150, 1500, 600, 600, 700, 450, 500, 1250, 1~\r\n$ Price     <int> 1658, 1577, 1650, 940, 1332, 2100, 650, 1560, 1566~\r\n\r\nNotice that we have a nice 3/4 split. Also that there are some nasty\r\nNAs in our data.\r\n\r\n\r\nvapply(credit_train, FUN = function(x){ mean(!is.na(x))}, FUN.VALUE = numeric(1))\r\n\r\n   Status Seniority      Home      Time       Age   Marital   Records \r\n1.0000000 1.0000000 0.9982036 1.0000000 1.0000000 0.9997006 1.0000000 \r\n      Job  Expenses    Income    Assets      Debt    Amount     Price \r\n0.9997006 1.0000000 0.9116766 0.9886228 0.9955090 1.0000000 1.0000000 \r\n\r\nWe can use recipes to impute this missing data instead of dropping\r\nit.\r\nAn initial recipe\r\nThis does no justice to the examples to follow, but gives a feel for\r\nhow this will start to look as we create recipes, add steps, then prep,\r\nthen bake it it all together for a model (or analysis).\r\n\r\n\r\nrec_obj = recipe(Status ~ ., data = credit_train)\r\nrec_obj\r\n\r\nRecipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n   outcome          1\r\n predictor         13\r\n\r\nProcessing Steps\r\nThere are so many options for pre-processing steps. The manual page\r\n?selections will provide them all. I headed over that way\r\nand collected a few I found:\r\nstep_pca\r\nstep_center\r\nstep_dummy (allows for step_interact to be\r\neffective)\r\nstep_interact\r\nstep_log\r\nstep_rm\r\nA way to explore even more steps is below,\r\n\r\n\r\ngrep(\"step_\", ls(\"package:recipes\"), value = TRUE)\r\n\r\n [1] \"step_arrange\"            \"step_bagimpute\"         \r\n [3] \"step_bin2factor\"         \"step_BoxCox\"            \r\n [5] \"step_bs\"                 \"step_center\"            \r\n [7] \"step_classdist\"          \"step_corr\"              \r\n [9] \"step_count\"              \"step_cut\"               \r\n[11] \"step_date\"               \"step_depth\"             \r\n[13] \"step_discretize\"         \"step_dummy\"             \r\n[15] \"step_dummy_extract\"      \"step_dummy_multi_choice\"\r\n[17] \"step_factor2string\"      \"step_filter\"            \r\n[19] \"step_filter_missing\"     \"step_geodist\"           \r\n[21] \"step_harmonic\"           \"step_holiday\"           \r\n[23] \"step_hyperbolic\"         \"step_ica\"               \r\n[25] \"step_impute_bag\"         \"step_impute_knn\"        \r\n[27] \"step_impute_linear\"      \"step_impute_lower\"      \r\n[29] \"step_impute_mean\"        \"step_impute_median\"     \r\n[31] \"step_impute_mode\"        \"step_impute_roll\"       \r\n[33] \"step_indicate_na\"        \"step_integer\"           \r\n[35] \"step_interact\"           \"step_intercept\"         \r\n[37] \"step_inverse\"            \"step_invlogit\"          \r\n[39] \"step_isomap\"             \"step_knnimpute\"         \r\n[41] \"step_kpca\"               \"step_kpca_poly\"         \r\n[43] \"step_kpca_rbf\"           \"step_lag\"               \r\n[45] \"step_lincomb\"            \"step_log\"               \r\n[47] \"step_logit\"              \"step_lowerimpute\"       \r\n[49] \"step_meanimpute\"         \"step_medianimpute\"      \r\n[51] \"step_modeimpute\"         \"step_mutate\"            \r\n[53] \"step_mutate_at\"          \"step_naomit\"            \r\n[55] \"step_nnmf\"               \"step_nnmf_sparse\"       \r\n[57] \"step_normalize\"          \"step_novel\"             \r\n[59] \"step_ns\"                 \"step_num2factor\"        \r\n[61] \"step_nzv\"                \"step_ordinalscore\"      \r\n[63] \"step_other\"              \"step_pca\"               \r\n[65] \"step_percentile\"         \"step_pls\"               \r\n[67] \"step_poly\"               \"step_profile\"           \r\n[69] \"step_range\"              \"step_ratio\"             \r\n[71] \"step_regex\"              \"step_relevel\"           \r\n[73] \"step_relu\"               \"step_rename\"            \r\n[75] \"step_rename_at\"          \"step_rm\"                \r\n[77] \"step_rollimpute\"         \"step_sample\"            \r\n[79] \"step_scale\"              \"step_select\"            \r\n[81] \"step_shuffle\"            \"step_slice\"             \r\n[83] \"step_spatialsign\"        \"step_sqrt\"              \r\n[85] \"step_string2factor\"      \"step_time\"              \r\n[87] \"step_unknown\"            \"step_unorder\"           \r\n[89] \"step_window\"             \"step_YeoJohnson\"        \r\n[91] \"step_zv\"                \r\n\r\nAnd then finally, you call the prep() function at the\r\nend of the chain and this will cook it up and return your final\r\ndataset.\r\nYou can also select variables using some build in functions and the\r\ntypical dplyr and tidyselect syntax.\r\ncontains()\r\nends_with()\r\neverything()\r\nmatches()\r\nnum_range()\r\nstarts_with()\r\none_of()\r\nall_of()\r\nall_outcomes()\r\nany_of()\r\nall_predictions()\r\nhas_role() - useful if you set the role IDs in a setup\r\nstep\r\nall_nominal()\r\nall_numeric()\r\nhas_type()\r\nall_nominal_predictors() - useful with\r\nstep_dummies\r\nall_numeric_predictors() - useful with\r\nstep_center and step_pca\r\n\r\n\r\nimputed = rec_obj %>% \r\n  step_impute_knn(all_predictors())\r\nimputed\r\n\r\nRecipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n   outcome          1\r\n predictor         13\r\n\r\nOperations:\r\n\r\nK-nearest neighbor imputation for all_predictors()\r\n\r\n\r\n\r\nind_vars = imputed %>% \r\n  step_dummy(all_nominal_predictors())\r\nind_vars\r\n\r\nRecipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n   outcome          1\r\n predictor         13\r\n\r\nOperations:\r\n\r\nK-nearest neighbor imputation for all_predictors()\r\nDummy variables from all_nominal_predictors()\r\n\r\n\r\n\r\nstandardized = ind_vars %>% \r\n  step_center(all_numeric_predictors()) %>% \r\n  step_scale(all_numeric_predictors())\r\nstandardized\r\n\r\nRecipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n   outcome          1\r\n predictor         13\r\n\r\nOperations:\r\n\r\nK-nearest neighbor imputation for all_predictors()\r\nDummy variables from all_nominal_predictors()\r\nCentering for all_numeric_predictors()\r\nScaling for all_numeric_predictors()\r\n\r\n\r\n\r\ntrained_rec = prep(standardized, training = credit_train)\r\ntrained_rec\r\n\r\nRecipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n   outcome          1\r\n predictor         13\r\n\r\nTraining data contained 3340 data points and 321 incomplete rows. \r\n\r\nOperations:\r\n\r\nK-nearest neighbor imputation for Seniority, Home, Time, Age, Marital,... [trained]\r\nDummy variables from Home, Marital, Records, Job [trained]\r\nCentering for Seniority, Time, Age, Expenses, Incom... [trained]\r\nScaling for Seniority, Time, Age, Expenses, Incom... [trained]\r\n\r\n\r\n\r\ntrain_data = bake(trained_rec, new_data = credit_train)\r\ntest_data = bake(trained_rec, new_data = credit_test)\r\n\r\ntrain_data %>% glimpse\r\n\r\nRows: 3,340\r\nColumns: 23\r\n$ Seniority         <dbl> 0.2326022, -0.9756267, -0.9756267, -0.9756~\r\n$ Time              <dbl> -0.7046334, 0.1143941, -1.9331747, 0.11439~\r\n$ Age               <dbl> 0.819168853, 0.363377561, -1.459787609, -0~\r\n$ Expenses          <dbl> 1.7475123, 1.7475123, -1.0452600, -0.53748~\r\n$ Income            <dbl> 0.75001838, -0.78930019, -1.17412983, -0.1~\r\n$ Assets            <dbl> -0.21062879, -0.48150940, -0.48150940, -0.~\r\n$ Debt              <dbl> -0.2615791, -0.2615791, -0.2615791, -0.261~\r\n$ Amount            <dbl> 2.06539627, 0.35400235, -1.35739158, 0.140~\r\n$ Price             <dbl> 2.44185759, 0.01922555, -1.52665963, 0.087~\r\n$ Status            <fct> bad, bad, bad, bad, bad, bad, bad, bad, ba~\r\n$ Home_other        <dbl> -0.2750668, -0.2750668, 3.6343927, -0.2750~\r\n$ Home_owner        <dbl> 1.0616266, -0.9416688, -0.9416688, -0.9416~\r\n$ Home_parents      <dbl> -0.4693061, 2.1301673, -0.4693061, -0.4693~\r\n$ Home_priv         <dbl> -0.2358159, -0.2358159, -0.2358159, -0.235~\r\n$ Home_rent         <dbl> -0.5338767, -0.5338767, -0.5338767, -0.533~\r\n$ Marital_married   <dbl> 0.6146338, 0.6146338, -1.6264981, 0.614633~\r\n$ Marital_separated <dbl> -0.1774584, -0.1774584, -0.1774584, -0.177~\r\n$ Marital_single    <dbl> -0.5324876, -0.5324876, 1.8774157, -0.5324~\r\n$ Marital_widow     <dbl> -0.1194505, -0.1194505, -0.1194505, -0.119~\r\n$ Records_yes       <dbl> 2.215860, -0.451157, 2.215860, -0.451157, ~\r\n$ Job_freelance     <dbl> 1.8235759, -0.5482089, -0.5482089, -0.5482~\r\n$ Job_others        <dbl> -0.198784, -0.198784, -0.198784, -0.198784~\r\n$ Job_partime       <dbl> -0.3332834, 2.9995509, 2.9995509, 2.999550~\r\n\r\ntest_data %>% glimpse\r\n\r\nRows: 1,114\r\nColumns: 23\r\n$ Seniority         <dbl> 1.078362381, -0.250689410, -0.009043629, 1~\r\n$ Time              <dbl> 0.9334216, 0.1143941, 0.9334216, -0.704633~\r\n$ Age               <dbl> 1.913067955, -0.274730248, -0.639363282, -~\r\n$ Expenses          <dbl> -0.3851502, 0.2241819, 0.9858471, 0.985847~\r\n$ Income            <dbl> -0.135089799, -0.212055727, 0.737190723, 0~\r\n$ Assets            <dbl> -0.48150940, -0.12033525, -0.03004171, -0.~\r\n$ Debt              <dbl> -0.26157915, -0.26157915, 1.64153404, -0.0~\r\n$ Amount            <dbl> -0.07384614, 0.24704023, 0.99577507, -0.92~\r\n$ Price             <dbl> 0.32265342, 0.19329733, 0.30987751, -0.823~\r\n$ Status            <fct> good, good, good, good, bad, good, good, g~\r\n$ Home_other        <dbl> -0.2750668, -0.2750668, -0.2750668, -0.275~\r\n$ Home_owner        <dbl> -0.9416688, 1.0616266, 1.0616266, -0.94166~\r\n$ Home_parents      <dbl> -0.4693061, -0.4693061, -0.4693061, -0.469~\r\n$ Home_priv         <dbl> -0.2358159, -0.2358159, -0.2358159, 4.2393~\r\n$ Home_rent         <dbl> 1.8725310, -0.5338767, -0.5338767, -0.5338~\r\n$ Marital_married   <dbl> -1.6264981, 0.6146338, 0.6146338, 0.614633~\r\n$ Marital_separated <dbl> -0.1774584, -0.1774584, -0.1774584, -0.177~\r\n$ Marital_single    <dbl> -0.5324876, -0.5324876, -0.5324876, -0.532~\r\n$ Marital_widow     <dbl> 8.3691608, -0.1194505, -0.1194505, -0.1194~\r\n$ Records_yes       <dbl> -0.451157, -0.451157, -0.451157, -0.451157~\r\n$ Job_freelance     <dbl> -0.5482089, 1.8235759, -0.5482089, -0.5482~\r\n$ Job_others        <dbl> -0.198784, -0.198784, -0.198784, -0.198784~\r\n$ Job_partime       <dbl> -0.3332834, -0.3332834, -0.3332834, -0.333~\r\n\r\n\r\n\r\nvapply(train_data, function(x) mean(!is.na(x)), numeric(1))\r\n\r\n        Seniority              Time               Age \r\n                1                 1                 1 \r\n         Expenses            Income            Assets \r\n                1                 1                 1 \r\n             Debt            Amount             Price \r\n                1                 1                 1 \r\n           Status        Home_other        Home_owner \r\n                1                 1                 1 \r\n     Home_parents         Home_priv         Home_rent \r\n                1                 1                 1 \r\n  Marital_married Marital_separated    Marital_single \r\n                1                 1                 1 \r\n    Marital_widow       Records_yes     Job_freelance \r\n                1                 1                 1 \r\n       Job_others       Job_partime \r\n                1                 1 \r\n\r\nvapply(test_data, function(x) mean(!is.na(x)), numeric(1))\r\n\r\n        Seniority              Time               Age \r\n                1                 1                 1 \r\n         Expenses            Income            Assets \r\n                1                 1                 1 \r\n             Debt            Amount             Price \r\n                1                 1                 1 \r\n           Status        Home_other        Home_owner \r\n                1                 1                 1 \r\n     Home_parents         Home_priv         Home_rent \r\n                1                 1                 1 \r\n  Marital_married Marital_separated    Marital_single \r\n                1                 1                 1 \r\n    Marital_widow       Records_yes     Job_freelance \r\n                1                 1                 1 \r\n       Job_others       Job_partime \r\n                1                 1 \r\n\r\nChecks\r\n\r\n\r\ntrained_rec <- trained_rec %>%\r\n  check_missing(contains(\"Marital\"))\r\ntrained_rec\r\n\r\nRecipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n   outcome          1\r\n predictor         13\r\n\r\nTraining data contained 3340 data points and 321 incomplete rows. \r\n\r\nOperations:\r\n\r\nK-nearest neighbor imputation for Seniority, Home, Time, Age, Marital,... [trained]\r\nDummy variables from Home, Marital, Records, Job [trained]\r\nCentering for Seniority, Time, Age, Expenses, Incom... [trained]\r\nScaling for Seniority, Time, Age, Expenses, Incom... [trained]\r\nCheck missing values for contains(\"Marital\")\r\n\r\nFull example\r\nIt’s always easier for me to view everything all at once. So here is\r\nthe full example chained together in the dplyr syntax.\r\n\r\n\r\n# Get the data\r\ndata(\"credit_data\")\r\ncredit_data %>% glimpse\r\n\r\nRows: 4,454\r\nColumns: 14\r\n$ Status    <fct> good, good, bad, good, good, good, good, good, goo~\r\n$ Seniority <int> 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0~\r\n$ Home      <fct> rent, rent, owner, rent, rent, owner, owner, paren~\r\n$ Time      <int> 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60~\r\n$ Age       <int> 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30~\r\n$ Marital   <fct> married, widow, married, single, single, married, ~\r\n$ Records   <fct> no, no, yes, no, no, no, no, no, no, no, no, no, n~\r\n$ Job       <fct> freelance, fixed, freelance, fixed, fixed, fixed, ~\r\n$ Expenses  <int> 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75~\r\n$ Income    <int> 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 12~\r\n$ Assets    <int> 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 400~\r\n$ Debt      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, ~\r\n$ Amount    <int> 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1~\r\n$ Price     <int> 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957~\r\n\r\n# Create train-test split\r\ntrain_test_split = credit_data %>% initial_split(prop = 3/4, strata = Status)\r\ncredit_train = training(train_test_split)\r\ncredit_test = testing(train_test_split)\r\n\r\n# Create a recipe w/ formula\r\ncredit_recipe = \r\n  recipe(Status ~ ., data = credit_train) %>% \r\n  step_impute_knn(all_predictors()) %>% \r\n  step_dummy(all_nominal_predictors()) %>% \r\n  step_center(all_numeric_predictors()) %>% \r\n  step_scale(all_numeric_predictors()) %>% \r\n  prep(training = credit_train)\r\n\r\n\r\ntrain_data = \r\n  credit_recipe %>% \r\n  bake(new_data = credit_train)\r\n\r\ntest_data =\r\n  credit_recipe %>% \r\n  bake(new_data = credit_test)\r\n\r\ntrain_data %>% head\r\n\r\n# A tibble: 6 x 23\r\n  Seniority    Time     Age Expen~1 Income Assets   Debt Amount  Price\r\n      <dbl>   <dbl>   <dbl>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\r\n1     0.240 -0.733   0.805    1.74   0.792 -0.207 -0.291  2.07   2.51 \r\n2    -0.987 -1.97   -1.47    -1.04  -1.21  -0.462 -0.291 -1.37  -1.58 \r\n3    -0.741  0.915  -1.11    -0.485 -0.446 -0.462 -0.291  0.992  1.20 \r\n4    -0.864  0.915   0.714    2.50  -0.380 -0.292  0.145 -0.941 -0.208\r\n5    -0.987  0.0908 -0.0143  -1.04  -0.193 -0.394 -0.291  0.992  0.644\r\n6    -0.864  0.503  -0.105    0.729 -0.553 -0.462 -0.291 -0.189 -0.836\r\n# ... with 14 more variables: Status <fct>, Home_other <dbl>,\r\n#   Home_owner <dbl>, Home_parents <dbl>, Home_priv <dbl>,\r\n#   Home_rent <dbl>, Marital_married <dbl>, Marital_separated <dbl>,\r\n#   Marital_single <dbl>, Marital_widow <dbl>, Records_yes <dbl>,\r\n#   Job_freelance <dbl>, Job_others <dbl>, Job_partime <dbl>, and\r\n#   abbreviated variable name 1: Expenses\r\n# i Use `colnames()` to see all variable names\r\n\r\ntest_data %>% head\r\n\r\n# A tibble: 6 x 23\r\n  Senio~1    Time    Age Expen~2   Income Assets   Debt  Amount  Price\r\n    <dbl>   <dbl>  <dbl>   <dbl>    <dbl>  <dbl>  <dbl>   <dbl>  <dbl>\r\n1   1.10   0.915   1.90   -0.384 -0.127   -0.462 -0.291 -0.0820 0.328 \r\n2  -0.987  0.0908  0.350   1.74  -0.806   -0.462 -0.291  0.348  0.0159\r\n3  -0.251  0.0908 -0.288   0.223 -0.207   -0.122 -0.291  0.240  0.195 \r\n4   0.853 -1.56    1.35   -1.04   2.52     0.939 -0.291  0.992  1.22  \r\n5  -0.987  0.0908 -0.105  -0.535 -0.140   -0.398 -0.291  0.133  0.0866\r\n6   2.32   0.915   0.350   0.931 -0.00713 -0.462 -0.291 -0.189  0.0636\r\n# ... with 14 more variables: Status <fct>, Home_other <dbl>,\r\n#   Home_owner <dbl>, Home_parents <dbl>, Home_priv <dbl>,\r\n#   Home_rent <dbl>, Marital_married <dbl>, Marital_separated <dbl>,\r\n#   Marital_single <dbl>, Marital_widow <dbl>, Records_yes <dbl>,\r\n#   Job_freelance <dbl>, Job_others <dbl>, Job_partime <dbl>, and\r\n#   abbreviated variable names 1: Seniority, 2: Expenses\r\n# i Use `colnames()` to see all variable names\r\n\r\nGetting creative\r\nWhat if we wanted to do PCA on just our categorical columns after\r\nimputation? Then get those as numeric, united with the rest of the\r\nnumeric variables, center, scale, prep, and bake as usual?\r\n\r\n\r\n# Create a recipe w/ formula\r\ncredit_pca_recipe = \r\n  \r\n  # Formula\r\n  recipe(Status ~ ., data = credit_train) %>% \r\n  \r\n  # Add ID roles, if rows have unique identifiers\r\n  # update_role(flight, time_hour, new_role = \"ID\") %>%\r\n  \r\n  # Impute missing data\r\n  step_impute_knn(all_predictors()) %>% \r\n  \r\n  # Make dummies for factors\r\n  step_dummy(all_nominal_predictors()) %>% \r\n  \r\n  # Drop those dummies using PCA\r\n  step_pca(contains(\"_\"), threshold = .90) %>%\r\n  \r\n  # Center&Scale\r\n  step_center(all_numeric_predictors()) %>% \r\n  step_scale(all_numeric_predictors()) %>% \r\n  \r\n  # Drop zero variance columns\r\n  step_zv(all_numeric_predictors()) %>% \r\n  \r\n  # Prep from the training data\r\n  prep(training = credit_train)\r\n\r\ntrain_data_pca = \r\n  credit_pca_recipe %>% \r\n  bake(new_data = credit_train)\r\n\r\ntest_data_pca =\r\n  credit_pca_recipe %>% \r\n  bake(new_data = credit_test)\r\n\r\ntrain_data_pca %>% head\r\n\r\n# A tibble: 6 x 17\r\n  Seniority    Time     Age Expen~1 Income Assets   Debt Amount  Price\r\n      <dbl>   <dbl>   <dbl>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\r\n1     0.240 -0.733   0.805    1.74   0.792 -0.207 -0.291  2.07   2.51 \r\n2    -0.987 -1.97   -1.47    -1.04  -1.21  -0.462 -0.291 -1.37  -1.58 \r\n3    -0.741  0.915  -1.11    -0.485 -0.446 -0.462 -0.291  0.992  1.20 \r\n4    -0.864  0.915   0.714    2.50  -0.380 -0.292  0.145 -0.941 -0.208\r\n5    -0.987  0.0908 -0.0143  -1.04  -0.193 -0.394 -0.291  0.992  0.644\r\n6    -0.864  0.503  -0.105    0.729 -0.553 -0.462 -0.291 -0.189 -0.836\r\n# ... with 8 more variables: Status <fct>, PC1 <dbl>, PC2 <dbl>,\r\n#   PC3 <dbl>, PC4 <dbl>, PC5 <dbl>, PC6 <dbl>, PC7 <dbl>, and\r\n#   abbreviated variable name 1: Expenses\r\n# i Use `colnames()` to see all variable names\r\n\r\ntest_data_pca %>% head\r\n\r\n# A tibble: 6 x 17\r\n  Senio~1    Time    Age Expen~2   Income Assets   Debt  Amount  Price\r\n    <dbl>   <dbl>  <dbl>   <dbl>    <dbl>  <dbl>  <dbl>   <dbl>  <dbl>\r\n1   1.10   0.915   1.90   -0.384 -0.127   -0.462 -0.291 -0.0820 0.328 \r\n2  -0.987  0.0908  0.350   1.74  -0.806   -0.462 -0.291  0.348  0.0159\r\n3  -0.251  0.0908 -0.288   0.223 -0.207   -0.122 -0.291  0.240  0.195 \r\n4   0.853 -1.56    1.35   -1.04   2.52     0.939 -0.291  0.992  1.22  \r\n5  -0.987  0.0908 -0.105  -0.535 -0.140   -0.398 -0.291  0.133  0.0866\r\n6   2.32   0.915   0.350   0.931 -0.00713 -0.462 -0.291 -0.189  0.0636\r\n# ... with 8 more variables: Status <fct>, PC1 <dbl>, PC2 <dbl>,\r\n#   PC3 <dbl>, PC4 <dbl>, PC5 <dbl>, PC6 <dbl>, PC7 <dbl>, and\r\n#   abbreviated variable names 1: Seniority, 2: Expenses\r\n# i Use `colnames()` to see all variable names\r\n\r\nAbsolutely ridiculous. You can just specify what columns you want,\r\nthen it will drop the dimensions on them for you and keep moving. Let’s\r\ntable this credit_recipe_pca for now.\r\nMake a model\r\nCross-validation and\r\nModel performance\r\nIt’s only natural now to build a classification model. We will be\r\nreferencing our example from the tidymodels\r\npost.\r\n\r\n\r\nlibrary(tidymodels)\r\n\r\nlr_mod <- logistic_reg() %>% \r\n  set_engine(\"glm\")\r\n\r\n# Cross validation set from train\r\nfolds = vfold_cv(credit_train, v = 10)\r\n\r\n# Workflow to incorporate folds\r\ncredit_workflow_rs = \r\n  workflows::workflow() %>% \r\n  workflows::add_model(lr_mod) %>% \r\n  workflows::add_recipe(credit_recipe) %>% \r\n  tune::fit_resamples(folds)\r\n\r\n# See performance\r\ncollect_metrics(credit_workflow_rs)\r\n\r\n# A tibble: 2 x 6\r\n  .metric  .estimator  mean     n std_err .config             \r\n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \r\n1 accuracy binary     0.795    10 0.00743 Preprocessor1_Model1\r\n2 roc_auc  binary     0.829    10 0.00590 Preprocessor1_Model1\r\n\r\nThis could be done kind of manually by building the model, predicting\r\nout values, then seeing how probabilities stack against the\r\nroc_auc and labels against accuracy. We will\r\nleave this for another dya.\r\nFit the whole model\r\n\r\n\r\n# Workflow for whole train set\r\ncredit_workflow = \r\n  workflows::workflow() %>% \r\n  workflows::add_model(lr_mod) %>% \r\n  workflows::add_recipe(credit_recipe)\r\n\r\n# Train on the whole now\r\ncredit_fit =\r\n  credit_workflow %>%\r\n  fit(data = credit_train)\r\n\r\ntidy(credit_fit)\r\n\r\n# A tibble: 23 x 5\r\n   term        estimate std.error statistic   p.value\r\n   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\r\n 1 (Intercept)  1.37       0.0549   25.0    1.12e-137\r\n 2 Seniority    0.678      0.0700    9.69   3.19e- 22\r\n 3 Time         0.00568    0.0588    0.0967 9.23e-  1\r\n 4 Age         -0.122      0.0631   -1.94   5.28e-  2\r\n 5 Expenses    -0.273      0.0580   -4.72   2.41e-  6\r\n 6 Income       0.529      0.0614    8.61   7.11e- 18\r\n 7 Assets       0.356      0.0927    3.85   1.20e-  4\r\n 8 Debt        -0.121      0.0501   -2.42   1.57e-  2\r\n 9 Amount      -0.985      0.0948  -10.4    2.54e- 25\r\n10 Price        0.677      0.0932    7.26   3.80e- 13\r\n# ... with 13 more rows\r\n# i Use `print(n = ...)` to see more rows\r\n\r\nWhat was our response?\r\n\r\n\r\ncredit_data %>% glimpse\r\n\r\nRows: 4,454\r\nColumns: 14\r\n$ Status    <fct> good, good, bad, good, good, good, good, good, goo~\r\n$ Seniority <int> 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0~\r\n$ Home      <fct> rent, rent, owner, rent, rent, owner, owner, paren~\r\n$ Time      <int> 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60~\r\n$ Age       <int> 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30~\r\n$ Marital   <fct> married, widow, married, single, single, married, ~\r\n$ Records   <fct> no, no, yes, no, no, no, no, no, no, no, no, no, n~\r\n$ Job       <fct> freelance, fixed, freelance, fixed, fixed, fixed, ~\r\n$ Expenses  <int> 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75~\r\n$ Income    <int> 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 12~\r\n$ Assets    <int> 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 400~\r\n$ Debt      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, ~\r\n$ Amount    <int> 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1~\r\n$ Price     <int> 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957~\r\n\r\ncredit_data %>% select(Status) %>% str\r\n\r\n'data.frame':   4454 obs. of  1 variable:\r\n $ Status: Factor w/ 2 levels \"bad\",\"good\": 2 2 1 2 2 2 2 2 2 1 ...\r\n\r\nRight, good and bad credit. The levels start with bad, then go to\r\ngood.\r\nRecipe visualization\r\n\r\n\r\ntidy(credit_fit) %>% filter(p.value <= 0.25) %>% \r\n  ggplot(., aes(fill=p.value)) +\r\n  geom_col(aes(x=reorder(term, -estimate), y=estimate)) +\r\n  geom_hline(yintercept=0, linetype=\"longdash\", color = \"black\") +\r\n  coord_flip() +\r\n  scale_fill_gradient(low = \"green\", high = \"red\") +\r\n  theme_bw() +\r\n  labs(fill = \"Statistical p-value\", x = \"Term\", y = \"Estimate\") +\r\n  ggtitle(\"Credit Signifiant Coeffients\")\r\n\r\n\r\n\r\nReferences\r\n https://recipes.tidymodels.org/ \r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-24-recipes-in-r/recipes-in-r_files/figure-html5/unnamed-chunk-22-1.png",
    "last_modified": "2022-07-24T10:38:02-04:00",
    "input_file": "recipes-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-21-tidymodels-in-r/",
    "title": "Tidymodels in R",
    "description": "This post is an introduction to using Tidymodels in R.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-23",
    "categories": [
      "machine learning",
      "data pipeline",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nBuilding a model\r\nGetting data\r\nBuild and fit a model\r\nUsing a model to\r\npredict\r\n\r\nPreprocess your data\r\nwith recipes\r\nGetting data\r\nData splitting\r\nCreate recipe and roles\r\nCreate features\r\nFit a model with recipe\r\nUse a trained workflow\r\nto predict\r\n\r\nEvaluate your model\r\nwith resampling\r\nGetting data\r\nData splitting\r\nModeling\r\nEstimating performance\r\nResampling\r\nFit a model with\r\nresampling\r\n\r\nTune model parameters\r\nGetting data\r\nData splitting\r\nTuning hyperparameters\r\nModel tuning with a\r\ngrid\r\nFinalizing our model\r\n\r\n\r\nSummary\r\nReferences\r\n\r\nIntroduction\r\nTidymodels is a relatively new R package that integrates data\r\nprocessing, cleaning, and modeling into a single workflow. Tidymodels\r\nalso solves the notorious problem of multiple R interfaces to different\r\nstatistical models that one might want to develop (e.g.,\r\nrpart for decision tree, glm for logistic\r\nregression, or randomForest for a random forest). Each with\r\nredudanct and duplicative parameters with multiple meanings and uses,\r\ntidymodels unifies the modeling process.\r\nThe tidy models site\r\ncontains many of the code snippets found here. Also the tidy models book provides a great more\r\ndeal of information using the package.\r\nLet’s get started!\r\nBuilding a model\r\n\r\n\r\nlibrary(tidymodels)  # for the parsnip package, along with the rest of tidymodels\r\n\r\n# Helper packages\r\nlibrary(readr)       # for importing data\r\nlibrary(broom.mixed) # for converting bayesian models to tidy tibbles\r\nlibrary(dotwhisker)  # for visualizing regression results\r\n\r\n\r\nGetting data\r\nThe urchins dataset has three columns:\r\nFood regime [independent]\r\nInitial Volume [indepdenent]\r\nWidth [dependent]\r\nThis dataset is used just to illustrate the capability that\r\ntidymodels offers for model building and using\r\nrecipes to construct a data pipeline.\r\n\r\n\r\nurchins <- read_csv(\"https://tidymodels.org/start/models/urchins.csv\") %>% \r\n  setNames(c(\"food_regime\", \"initial_volume\", \"width\")) %>% \r\n  dplyr::mutate(food_regime = factor(food_regime, levels = c(\"Initial\", \"Low\", \"High\")))\r\nurchins\r\n\r\n# A tibble: 72 x 3\r\n   food_regime initial_volume width\r\n   <fct>                <dbl> <dbl>\r\n 1 Initial                3.5 0.01 \r\n 2 Initial                5   0.02 \r\n 3 Initial                8   0.061\r\n 4 Initial               10   0.051\r\n 5 Initial               13   0.041\r\n 6 Initial               13   0.061\r\n 7 Initial               15   0.041\r\n 8 Initial               15   0.071\r\n 9 Initial               16   0.092\r\n10 Initial               17   0.051\r\n# ... with 62 more rows\r\n# i Use `print(n = ...)` to see more rows\r\n\r\nBelow is a quick visual of the dataset with some trends within the\r\ngroup=food_regime.\r\n\r\n\r\nggplot(urchins,\r\n       aes(x= initial_volume,\r\n           y= width,\r\n           group = food_regime,\r\n           col = food_regime)) +\r\n  geom_point() + \r\n  geom_smooth(method = lm, formula = \"y~x\", se = FALSE) +\r\n  scale_color_viridis_d(option = \"plasma\", end = 0.7) +\r\n  theme_bw() +\r\n  ggtitle(\"Urchins Food Regime Width\")\r\n\r\n\r\n\r\nBuild and fit a model\r\nthe parsnip package comes along with\r\ntidymodels and this contains a bunch of unified interfaces\r\nto models we know and love. Linear regression being\r\nlinear_reg(). As usual, create the model, fit with a\r\nformula, then predict on new data. The tidy function is a\r\ndominant feature to this package. It trumps the historic\r\nsummary and coef from the fit model object by\r\noutputting a clean table with estimates, errors, and p-values.\r\n\r\n\r\nlm_mod = parsnip::linear_reg()\r\nlm_fit = lm_mod %>% \r\n  fit(width ~ initial_volume * food_regime, data = urchins)\r\nbroom.mixed::tidy(lm_fit)\r\n\r\n# A tibble: 6 x 5\r\n  term                            estimate std.error statistic p.value\r\n  <chr>                              <dbl>     <dbl>     <dbl>   <dbl>\r\n1 (Intercept)                     0.0331    0.00962      3.44  1.00e-3\r\n2 initial_volume                  0.00155   0.000398     3.91  2.22e-4\r\n3 food_regimeLow                  0.0198    0.0130       1.52  1.33e-1\r\n4 food_regimeHigh                 0.0214    0.0145       1.47  1.45e-1\r\n5 initial_volume:food_regimeLow  -0.00126   0.000510    -2.47  1.62e-2\r\n6 initial_volume:food_regimeHigh  0.000525  0.000702     0.748 4.57e-1\r\n\r\nOne can easily construct upper and lower confidence limits by\r\n+- std.error and view the ranges on the population (if the\r\nmodel assumptions are validated, i.e., errors normally distributed).\r\n\r\n\r\ntidy(lm_fit) %>% \r\n  mutate(estimate_lower = estimate - 2*std.error,\r\n         estimate_upper = estimate + 2*std.error) %>% \r\n  ggplot(., aes(x = estimate, y = term, color = p.value)) +\r\n  scale_colour_gradient(low = \"red\", high = \"green\") + \r\n  geom_point() + \r\n  geom_linerange(aes(xmin = estimate_lower, xmax=estimate_upper)) +\r\n  geom_vline(xintercept = 0, colour = \"grey50\", linetype = \"dashed\") +\r\n  labs(color = \"Statistical p-value\", x = \"Estimate\", y = \"Term\") +\r\n  ggtitle(\"Urchin Coefficient Estimates\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nUsing a model to predict\r\nPredicting new points is super easy too. Simply construct a dataframe\r\nwith the same columns as your trained data. For us it was\r\ninitial_volume which was numeric and\r\nfood_regime which was a factor with three levels. So we\r\nwant to ultimately predict on volume = 20, but we will\r\nexpand.grid that for each level supplied. The output of\r\nexpand.grid is always the product of each list supplied\r\n(e.g.,\r\nA = c(a1,a2,a3), B=c(b1,b2,b3,b4) will be |A|*|B|=3*4= 12 rows,\r\ncontaining every possible tupple combination). In general this is,\r\n\\[\r\nrows =\\prod_{j=1}^{k}{|S_{j}|}\r\n\\]\r\n\r\n\r\nnew_points = expand.grid(initial_volume= c(20),\r\n                         food_regime = c(\"Initial\", \"Low\", \"High\"))\r\n\r\nnew_points\r\n\r\n  initial_volume food_regime\r\n1             20     Initial\r\n2             20         Low\r\n3             20        High\r\n\r\nPredictions are made using a standard interface.\r\n\r\n\r\nmean_pred = predict(lm_fit, new_data = new_points)\r\nmean_pred\r\n\r\n# A tibble: 3 x 1\r\n   .pred\r\n   <dbl>\r\n1 0.0642\r\n2 0.0588\r\n3 0.0961\r\n\r\nConfidence intervals are a type option, which is nice\r\nfor getting the above values without the computation.\r\n\r\n\r\nconf_int_pred = predict(lm_fit,\r\n                        new_data = new_points,\r\n                        type = \"conf_int\")\r\nconf_int_pred\r\n\r\n# A tibble: 3 x 2\r\n  .pred_lower .pred_upper\r\n        <dbl>       <dbl>\r\n1      0.0555      0.0729\r\n2      0.0499      0.0678\r\n3      0.0870      0.105 \r\n\r\nError plots on prediction intervals are now made pretty easy.\r\n\r\n\r\n# Combine\r\nplot_data = new_points %>% \r\n  dplyr::bind_cols(mean_pred) %>% \r\n  dplyr::bind_cols(conf_int_pred)\r\n\r\n# and plot:\r\nggplot(plot_data, aes(x=food_regime, col = food_regime)) +\r\n  scale_color_viridis_d(option = \"plasma\", end = 0.7) + # for discrete color option\r\n  geom_point(aes(y = .pred)) +\r\n  geom_errorbar(aes(ymin = .pred_lower,\r\n                    ymax = .pred_upper),\r\n                width = 0.2) +\r\n  labs(y=\"Urchin size\", x = \"Food Regime\", col = \"Food Regime\") +\r\n  theme_bw() +\r\n  ggtitle(\"Urchin Width Prediction Interval\")\r\n\r\n\r\n\r\nNow we can fit a bayesian model to this and see how the differences\r\ncompare.\r\n\r\n\r\nprior_dist <- rstanarm::student_t(df = 1)\r\nset.seed(123)\r\n\r\n# make the parsnip model\r\nbayes_mod <-   \r\n  linear_reg() %>% \r\n  set_engine(\"stan\", \r\n             prior_intercept = prior_dist, \r\n             prior = prior_dist) \r\n\r\n# train the model\r\nbayes_fit <- bayes_mod %>% \r\n  fit(width ~ initial_volume * food_regime, data = urchins)\r\n\r\nprint(bayes_fit, digits = 5)\r\n\r\nparsnip model object\r\n\r\nstan_glm\r\n family:       gaussian [identity]\r\n formula:      width ~ initial_volume * food_regime\r\n observations: 72\r\n predictors:   6\r\n------\r\n                               Median   MAD_SD  \r\n(Intercept)                     0.03310  0.00943\r\ninitial_volume                  0.00155  0.00040\r\nfood_regimeLow                  0.01989  0.01326\r\nfood_regimeHigh                 0.02159  0.01406\r\ninitial_volume:food_regimeLow  -0.00126  0.00050\r\ninitial_volume:food_regimeHigh  0.00053  0.00070\r\n\r\nAuxiliary parameter(s):\r\n      Median  MAD_SD \r\nsigma 0.02120 0.00184\r\n\r\n------\r\n* For help interpreting the printed output see ?print.stanreg\r\n* For info on the priors used see ?prior_summary.stanreg\r\n\r\ntidy(bayes_fit, conf.int = TRUE)\r\n\r\n# A tibble: 6 x 5\r\n  term                            estimate std.error conf.low conf.h~1\r\n  <chr>                              <dbl>     <dbl>    <dbl>    <dbl>\r\n1 (Intercept)                     0.0331    0.00943   1.70e-2  4.93e-2\r\n2 initial_volume                  0.00155   0.000405  8.91e-4  2.21e-3\r\n3 food_regimeLow                  0.0199    0.0133   -2.62e-4  4.15e-2\r\n4 food_regimeHigh                 0.0216    0.0141   -2.59e-3  4.61e-2\r\n5 initial_volume:food_regimeLow  -0.00126   0.000503 -2.11e-3 -4.41e-4\r\n6 initial_volume:food_regimeHigh  0.000533  0.000700 -6.85e-4  1.67e-3\r\n# ... with abbreviated variable name 1: conf.high\r\n\r\n\r\n\r\nbayes_plot_data <-\r\n  new_points %>% \r\n  bind_cols(predict(bayes_fit, new_data = new_points)) %>% \r\n  bind_cols(predict(bayes_fit, new_data = new_points, type = \"conf_int\")) %>% \r\n  mutate(model = \"bayes\")\r\n\r\nplot_data <- plot_data %>% \r\n  mutate(model = \"linear_reg\")\r\n\r\nnew_plot_data <- bind_rows(plot_data, bayes_plot_data) %>% \r\n  mutate(model = factor(model, levels = c(\"linear_reg\", \"bayes\")))\r\n\r\nggplot(data = new_plot_data, aes(x=food_regime, group=model, col=food_regime)) +\r\n  geom_point(aes( y = .pred)) +\r\n  geom_errorbar(aes(ymin = .pred_lower,\r\n                    ymax = .pred_upper),\r\n                width = 0.2) +\r\n  facet_grid(cols = vars(model)) +\r\n  scale_colour_viridis_d(option = \"plasma\", end = 0.7) +\r\n  labs(col = \"Food Regime\", x = \"Food Regime\", y = \"Prediction\") +\r\n  ggtitle(\"Urchin Model Prediction Interval Comparison\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nSo far we’ve learned some pretty handy tricks from the tidymodels\r\npackage. There is a unified interface for models and we explored the\r\nlinear_reg() one today. This was as simple as fitting and\r\npredicting. We even explored putting in new data and getting predictions\r\nusing the expand.grid function. A simple linar model\r\ncomparison was done also to see how different models can be brought\r\ntogether using the dplyr package. The\r\ntype = \"conf_int\" option from the tidymodel interface\r\nprovided a .pred_lower and .pred_upper output,\r\nwhich was helpful to surround the estimate with error bars. Lastly, the\r\ntidy function was new and can be used in place of the\r\nexisting summary option that most go to with the unweildy\r\ninterface of getting information (i.e., using the coef\r\nfunction).\r\nPreprocess your data with\r\nrecipes\r\nGetting data\r\nThe NYC Flights data contains two data tables.\r\nFlights\r\nWeather\r\nThe flights table contain information about\r\nsource, destination, time,\r\ncarrier and other useful aircraft related information. The\r\nimportant thing to note about this dataset is that it is not\r\ntime series even though there is date time attributes\r\nattributes. This threw me off at first. Each row is a flight. The goal\r\nis to see if we can predict if a flight is late. We will define late as\r\nfollows,\r\n\\[\r\nlate=delay\\ge30\\:mins\r\n\\] On time as,\r\n\\[\r\non \\: time =delay < 30 \\: mins\r\n\\] The weather table contains a key which can be\r\njoined against to augment the flight information. This time stamp column\r\nwill allow the flight to be observed with other, possibly useful,\r\nattributes such as temperature, windspeed, and others. One very\r\nimpressive augmentation for dates (YYYY-MM-DD specifically) is the\r\npackage timeDate::listHolidays(\"US\"), which will create\r\nindicators for over 19 holidays and light them up with a 1\r\nor 0 if it is found in the record. Pretty nifty to have in\r\nthe tool kit.\r\nLet’s get moving!\r\n\r\n\r\nlibrary(tidymodels)\r\nlibrary(nycflights13)\r\nlibrary(skimr)\r\n\r\n\r\n\r\n\r\nflights %>% glimpse\r\n\r\nRows: 336,776\r\nColumns: 19\r\n$ year           <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 201~\r\n$ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\r\n$ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\r\n$ dep_time       <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, ~\r\n$ sched_dep_time <int> 515, 529, 540, 545, 600, 558, 600, 600, 600, ~\r\n$ dep_delay      <dbl> 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, ~\r\n$ arr_time       <int> 830, 850, 923, 1004, 812, 740, 913, 709, 838,~\r\n$ sched_arr_time <int> 819, 830, 850, 1022, 837, 728, 854, 723, 846,~\r\n$ arr_delay      <dbl> 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2,~\r\n$ carrier        <chr> \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV~\r\n$ flight         <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, ~\r\n$ tailnum        <chr> \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668~\r\n$ origin         <chr> \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EW~\r\n$ dest           <chr> \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FL~\r\n$ air_time       <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 1~\r\n$ distance       <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, ~\r\n$ hour           <dbl> 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, ~\r\n$ minute         <dbl> 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0~\r\n$ time_hour      <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 20~\r\n\r\nRemember, every row is just a flight :) The goal is to perform\r\nclassification. Do we think a new flight will be late or\r\nnot? Let’s decide on features to make this prediction.\r\n\r\n\r\nweather %>% glimpse\r\n\r\nRows: 26,115\r\nColumns: 15\r\n$ origin     <chr> \"EWR\", \"EWR\", \"EWR\", \"EWR\", \"EWR\", \"EWR\", \"EWR\", ~\r\n$ year       <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2~\r\n$ month      <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~\r\n$ day        <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~\r\n$ hour       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16~\r\n$ temp       <dbl> 39.02, 39.02, 39.02, 39.92, 39.02, 37.94, 39.02, ~\r\n$ dewp       <dbl> 26.06, 26.96, 28.04, 28.04, 28.04, 28.04, 28.04, ~\r\n$ humid      <dbl> 59.37, 61.63, 64.43, 62.21, 64.43, 67.21, 64.43, ~\r\n$ wind_dir   <dbl> 270, 250, 240, 250, 260, 240, 240, 250, 260, 260,~\r\n$ wind_speed <dbl> 10.35702, 8.05546, 11.50780, 12.65858, 12.65858, ~\r\n$ wind_gust  <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~\r\n$ precip     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\r\n$ pressure   <dbl> 1012.0, 1012.3, 1012.5, 1012.2, 1011.9, 1012.4, 1~\r\n$ visib      <dbl> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1~\r\n$ time_hour  <dttm> 2013-01-01 01:00:00, 2013-01-01 02:00:00, 2013-0~\r\n\r\nNotice the commonality, time_hour. We will also use this\r\nwith lubridate to construct a convenient date\r\ncolumn that can be used for our above-said feature engineering on\r\nholidays.\r\n\r\n\r\nset.seed(123)\r\n\r\nflight_data <- \r\n  flights %>% \r\n  mutate(\r\n    # Resposne variable\r\n    arr_delay = ifelse(arr_delay >= 30, \"late\", \"on_time\"),\r\n    arr_delay = factor(arr_delay),\r\n    \r\n    # Handy date column\r\n    date = lubridate::as_date(time_hour)\r\n  ) %>% \r\n  \r\n  # Weather info\r\n  inner_join(weather, by = c(\"origin\", \"time_hour\")) %>% \r\n  \r\n  # Everything we want\r\n  select( # IDs\r\n          time_hour, flight,\r\n          \r\n          # To Be Engineered..\r\n          date, \r\n          \r\n          # Independent Variables\r\n          # Numeric/Nominal Flight Info.\r\n          dep_time, origin, dest, air_time, distance, carrier,\r\n          \r\n          # Numeric/Nominal Weather Info.\r\n          temp, dewp, humid, starts_with(\"wind_\"), precip, pressure, visib,\r\n          \r\n          #Dependent Variables\r\n           arr_delay) %>% \r\n  drop_na() %>% \r\n  mutate_if(is.character, as.factor)\r\n\r\nflight_data\r\n\r\n# A tibble: 72,734 x 19\r\n   time_hour           flight date       dep_time origin dest  air_t~1\r\n   <dttm>               <int> <date>        <int> <fct>  <fct>   <dbl>\r\n 1 2013-01-01 05:00:00   1714 2013-01-01      533 LGA    IAH       227\r\n 2 2013-01-01 06:00:00    461 2013-01-01      554 LGA    ATL       116\r\n 3 2013-01-01 06:00:00   5708 2013-01-01      557 LGA    IAD        53\r\n 4 2013-01-01 06:00:00    301 2013-01-01      558 LGA    ORD       138\r\n 5 2013-01-01 06:00:00    707 2013-01-01      559 LGA    DFW       257\r\n 6 2013-01-01 06:00:00    371 2013-01-01      600 LGA    FLL       152\r\n 7 2013-01-01 06:00:00   4650 2013-01-01      600 LGA    ATL       134\r\n 8 2013-01-01 06:00:00   1919 2013-01-01      602 LGA    MSP       170\r\n 9 2013-01-01 06:00:00   4401 2013-01-01      602 LGA    DTW       105\r\n10 2013-01-01 06:00:00   1837 2013-01-01      623 LGA    MIA       153\r\n# ... with 72,724 more rows, 12 more variables: distance <dbl>,\r\n#   carrier <fct>, temp <dbl>, dewp <dbl>, humid <dbl>,\r\n#   wind_dir <dbl>, wind_speed <dbl>, wind_gust <dbl>, precip <dbl>,\r\n#   pressure <dbl>, visib <dbl>, arr_delay <fct>, and abbreviated\r\n#   variable name 1: air_time\r\n# i Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\r\n\r\nUsing the dplyr::count function we can quickly seeh ow\r\nbalanced our data is. About 2/10 were late. This would be class\r\nimbalance, something to consider when training our models.\r\n\r\n\r\nflight_data %>% \r\n  dplyr::count(arr_delay)  %>% \r\n  mutate(prop = n/sum(n))\r\n\r\n# A tibble: 2 x 3\r\n  arr_delay     n  prop\r\n  <fct>     <int> <dbl>\r\n1 late      11695 0.161\r\n2 on_time   61039 0.839\r\n\r\n\r\n\r\nglimpse(flight_data)\r\n\r\nRows: 72,734\r\nColumns: 19\r\n$ time_hour  <dttm> 2013-01-01 05:00:00, 2013-01-01 06:00:00, 2013-0~\r\n$ flight     <int> 1714, 461, 5708, 301, 707, 371, 4650, 1919, 4401,~\r\n$ date       <date> 2013-01-01, 2013-01-01, 2013-01-01, 2013-01-01, ~\r\n$ dep_time   <int> 533, 554, 557, 558, 559, 600, 600, 602, 602, 623,~\r\n$ origin     <fct> LGA, LGA, LGA, LGA, LGA, LGA, LGA, LGA, LGA, LGA,~\r\n$ dest       <fct> IAH, ATL, IAD, ORD, DFW, FLL, ATL, MSP, DTW, MIA,~\r\n$ air_time   <dbl> 227, 116, 53, 138, 257, 152, 134, 170, 105, 153, ~\r\n$ distance   <dbl> 1416, 762, 229, 733, 1389, 1076, 762, 1020, 502, ~\r\n$ carrier    <fct> UA, DL, EV, AA, AA, B6, MQ, DL, MQ, AA, UA, MQ, A~\r\n$ temp       <dbl> 39.92, 39.92, 39.92, 39.92, 39.92, 39.92, 39.92, ~\r\n$ dewp       <dbl> 24.98, 24.98, 24.98, 24.98, 24.98, 24.98, 24.98, ~\r\n$ humid      <dbl> 54.81, 54.81, 54.81, 54.81, 54.81, 54.81, 54.81, ~\r\n$ wind_dir   <dbl> 250, 260, 260, 260, 260, 260, 260, 260, 260, 260,~\r\n$ wind_speed <dbl> 14.96014, 16.11092, 16.11092, 16.11092, 16.11092,~\r\n$ wind_gust  <dbl> 21.86482, 23.01560, 23.01560, 23.01560, 23.01560,~\r\n$ precip     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\r\n$ pressure   <dbl> 1011.4, 1011.7, 1011.7, 1011.7, 1011.7, 1011.7, 1~\r\n$ visib      <dbl> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1~\r\n$ arr_delay  <fct> on_time, on_time, on_time, on_time, late, on_time~\r\n\r\nI had never heard of the skimr package, but it’s pretty\r\nslick. You pass in some variables you want it to skim over and it will\r\nreturn a data frame with tons of great information on it, like top\r\ncounts, missing rows, etc. I really like the top_counts one.\r\n\r\n\r\ntmp = flight_data %>% skimr::skim(dest, carrier)\r\ntmp\r\n\r\nTable 1: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n72734\r\nNumber of columns\r\n19\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nfactor\r\n2\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\ndest\r\n0\r\n1\r\nFALSE\r\n104\r\nATL: 4043, ORD: 3823, LAX: 3180, BOS:\r\n3168\r\ncarrier\r\n0\r\n1\r\nFALSE\r\n16\r\nUA: 13580, EV: 12310, DL: 10388, B6:\r\n10040\r\n\r\nWe can extract that information in different ways using\r\ntidyr. Be mindful, scales are different.\r\n\r\n\r\nworked = tmp %>% \r\n  tidyr::separate(factor.top_counts, sep = \", \", into = paste(\"Top_\", 1:4, sep=\"\")) %>% \r\n  tidyr::gather(., key = \"Top_Rank\", value = \"AirportAndQuantity\", -c(skim_type, skim_variable, n_missing, complete_rate, factor.ordered, factor.n_unique)) %>% \r\n  tidyr::separate(., col = AirportAndQuantity, sep = \": \", into = c(\"Source\",\"Count\")) %>% \r\n  dplyr::mutate(Count = as.numeric(Count),\r\n                skim_variable = as.factor(skim_variable))\r\n\r\npar(mfrow = c(1,2))\r\nworked %>% filter(skim_variable == \"dest\") %>%\r\n  ggplot(., aes(x = reorder(Source, -Count), y = Count, fill = Count)) +\r\n    scale_fill_gradient(low = \"green\", high = \"red\") +\r\n    geom_col() +\r\n    labs(fill = \"Number of Flights\") +\r\n    xlab(\"Source\") +\r\n    ylab(\"Number of Flights\") +\r\n    ggtitle(\"Destination Airports Counts\") +\r\n    theme_light()\r\n\r\n\r\nworked %>% filter(skim_variable == \"carrier\") %>% \r\n  ggplot(., aes(x = reorder(Source, -Count), y = Count, fill = Count)) + \r\n    scale_fill_gradient(low = \"green\", high = \"red\") +\r\n    geom_col() +\r\n    labs(fill = \"Number of Flights\") +\r\n    xlab(\"Source\") +\r\n    ylab(\"Number of Flights\") +\r\n    ggtitle(\"Airline Carrier Counts\") +\r\n    theme_light()\r\n\r\n\r\n\r\nData splitting\r\nUsing the built in ecosystem, rsample is used to get the\r\ntrain test split.\r\n\r\n\r\nset.seed(222)\r\n\r\ndata_split = rsample::initial_split(flight_data, prop = 3/4)\r\ntrain_data = rsample::training(data_split)\r\ntest_data = rsample::testing(data_split)\r\n\r\n\r\nCreate recipe and roles\r\nBuilding up a recipe is easy. Now we can quickly throw a formula at\r\nthe recipe and it can start to do work for us. As long as the recipe\r\nreceives the same information as the model, all should be well. Tagging\r\nthe ID variables is nice for debugging on the backend. It will not model\r\nthose variables flagged as ID.\r\n\r\n\r\nflights_rec = \r\n  recipes::recipe(arr_delay ~ ., data = train_data) %>% \r\n  recipes::update_role(flight, time_hour, new_role = \"ID\")\r\nsummary(flights_rec)\r\n\r\n# A tibble: 19 x 4\r\n   variable   type    role      source  \r\n   <chr>      <chr>   <chr>     <chr>   \r\n 1 time_hour  date    ID        original\r\n 2 flight     numeric ID        original\r\n 3 date       date    predictor original\r\n 4 dep_time   numeric predictor original\r\n 5 origin     nominal predictor original\r\n 6 dest       nominal predictor original\r\n 7 air_time   numeric predictor original\r\n 8 distance   numeric predictor original\r\n 9 carrier    nominal predictor original\r\n10 temp       numeric predictor original\r\n11 dewp       numeric predictor original\r\n12 humid      numeric predictor original\r\n13 wind_dir   numeric predictor original\r\n14 wind_speed numeric predictor original\r\n15 wind_gust  numeric predictor original\r\n16 precip     numeric predictor original\r\n17 pressure   numeric predictor original\r\n18 visib      numeric predictor original\r\n19 arr_delay  nominal outcome   original\r\n\r\nCreate features\r\nThis is probably the most magical of steps.\r\nCreate the roles for ID variables\r\nEngineer the day of the week and month variables\r\nEngineer the holiday indicators\r\nEngineer dummies for every other feature (because some models\r\nrequire this, others don’t)\r\nClean all features that have only one value (see nzv for those whose\r\nvariation is so small we may want to drop them also).\r\n\r\n\r\nflights_rec <- \r\n  # Recipe for all variables in the table\r\n  recipe(arr_delay ~ ., data = train_data) %>% \r\n  \r\n  # Make ID variables explit (not modeled)\r\n  update_role(flight, time_hour, new_role = \"ID\") %>%\r\n  \r\n  # Create features from the `lubridate` column, explore what options exist\r\n  step_date(date, features = c(\"dow\", \"month\")) %>%               \r\n  \r\n  # Create holiday indicator for `lubdridate` column\r\n  step_holiday(date, \r\n               holidays = timeDate::listHolidays(\"US\"), \r\n               keep_original_cols = FALSE) %>% \r\n  # Create dummy variables for `nominal` columns\r\n  step_dummy(all_nominal_predictors()) %>% \r\n  \r\n  # Remove zero-variance values; i.e., only 1 value in the column\r\n  step_zv(all_numeric_predictors())\r\nsummary(flights_rec)\r\n\r\n# A tibble: 19 x 4\r\n   variable   type    role      source  \r\n   <chr>      <chr>   <chr>     <chr>   \r\n 1 time_hour  date    ID        original\r\n 2 flight     numeric ID        original\r\n 3 date       date    predictor original\r\n 4 dep_time   numeric predictor original\r\n 5 origin     nominal predictor original\r\n 6 dest       nominal predictor original\r\n 7 air_time   numeric predictor original\r\n 8 distance   numeric predictor original\r\n 9 carrier    nominal predictor original\r\n10 temp       numeric predictor original\r\n11 dewp       numeric predictor original\r\n12 humid      numeric predictor original\r\n13 wind_dir   numeric predictor original\r\n14 wind_speed numeric predictor original\r\n15 wind_gust  numeric predictor original\r\n16 precip     numeric predictor original\r\n17 pressure   numeric predictor original\r\n18 visib      numeric predictor original\r\n19 arr_delay  nominal outcome   original\r\n\r\n# This will make sure that all of the test data `dest` have the same levels as the train data.\r\n# test_data %>% \r\n#   distinct(dest) %>% \r\n#   anti_join(train_data)\r\n\r\n\r\nFit a model with recipe\r\nWe can now build a logistic regression model. We first create a\r\nworkflow, then pass in our model, and our recipe, which has\r\nseveral steps. The workflow will show us each of them.\r\n\r\n\r\nlr_mod <- logistic_reg() %>% \r\n  set_engine(\"glm\")\r\n\r\n\r\nflights_workflow = workflows::workflow() %>% \r\n  workflows::add_model(lr_mod) %>% \r\n  workflows::add_recipe(flights_rec)\r\nflights_workflow\r\n\r\n== Workflow ==========================================================\r\nPreprocessor: Recipe\r\nModel: logistic_reg()\r\n\r\n-- Preprocessor ------------------------------------------------------\r\n4 Recipe Steps\r\n\r\n* step_date()\r\n* step_holiday()\r\n* step_dummy()\r\n* step_zv()\r\n\r\n-- Model -------------------------------------------------------------\r\nLogistic Regression Model Specification (classification)\r\n\r\nComputational engine: glm \r\n\r\nNow we can build a\r\n\r\n\r\nflights_fit = flights_workflow %>% \r\n  fit(data = train_data)\r\n\r\nflights_fit\r\n\r\n== Workflow [trained] ================================================\r\nPreprocessor: Recipe\r\nModel: logistic_reg()\r\n\r\n-- Preprocessor ------------------------------------------------------\r\n4 Recipe Steps\r\n\r\n* step_date()\r\n* step_holiday()\r\n* step_dummy()\r\n* step_zv()\r\n\r\n-- Model -------------------------------------------------------------\r\n\r\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\r\n\r\nCoefficients:\r\n                 (Intercept)                      dep_time  \r\n                  -2.269e+01                    -2.134e-03  \r\n                    air_time                      distance  \r\n                  -4.247e-02                     1.367e-02  \r\n                        temp                          dewp  \r\n                   3.478e-02                    -4.644e-02  \r\n                       humid                      wind_dir  \r\n                   9.772e-03                     7.221e-04  \r\n                  wind_speed                     wind_gust  \r\n                  -1.301e-02                    -1.279e-02  \r\n                      precip                      pressure  \r\n                  -1.911e+00                     1.229e-02  \r\n                       visib           date_USChristmasDay  \r\n                   1.499e-01                    -2.031e+00  \r\n    date_USCPulaskisBirthday  date_USDecorationMemorialDay  \r\n                   8.748e-01                     3.103e-01  \r\n          date_USElectionDay             date_USGoodFriday  \r\n                  -2.106e+00                     1.168e+00  \r\n      date_USInaugurationDay        date_USIndependenceDay  \r\n                   2.807e-02                     3.249e+00  \r\n     date_USLincolnsBirthday            date_USMemorialDay  \r\n                   3.487e-01                     1.025e+00  \r\n      date_USMLKingsBirthday            date_USNewYearsDay  \r\n                   4.201e-01                     5.512e-01  \r\n        date_USPresidentsDay        date_USThanksgivingDay  \r\n                  -5.196e-01                    -6.331e-01  \r\n          date_USVeteransDay                    origin_JFK  \r\n                   1.172e+00                     6.691e-02  \r\n                  origin_LGA                      dest_ACK  \r\n                  -1.200e-01                     1.358e+01  \r\n                    dest_ALB                      dest_ANC  \r\n                   1.336e+01                    -4.113e+00  \r\n                    dest_ATL                      dest_AUS  \r\n                   8.015e+00                     2.170e+00  \r\n                    dest_AVL                      dest_BDL  \r\n                   9.514e+00                     1.344e+01  \r\n                    dest_BGR                      dest_BHM  \r\n                   1.170e+01                     7.628e+00  \r\n                    dest_BNA                      dest_BOS  \r\n                   8.237e+00                     1.292e+01  \r\n                    dest_BQN                      dest_BTV  \r\n                   1.497e+00                     1.196e+01  \r\n                    dest_BUF                      dest_BUR  \r\n                   1.189e+01                    -5.157e+00  \r\n                    dest_BWI                      dest_BZN  \r\n                   1.273e+01                    -3.337e+00  \r\n\r\n...\r\nand 122 more lines.\r\n\r\nThe final model contains the recipe and the model, so we can\r\nextract them out individually, but do not have to.\r\n\r\n\r\nflights_fit %>% \r\n  extract_fit_parsnip() %>% # not needed, but useful to know\r\n  tidy()\r\n\r\n# A tibble: 164 x 5\r\n   term          estimate std.error statistic   p.value\r\n   <chr>            <dbl>     <dbl>     <dbl>     <dbl>\r\n 1 (Intercept) -22.7      6.76          -3.36 7.85e-  4\r\n 2 dep_time     -0.00213  0.0000380    -56.2  0        \r\n 3 air_time     -0.0425   0.00129      -33.0  3.06e-238\r\n 4 distance      0.0137   0.00346        3.95 7.96e-  5\r\n 5 temp          0.0348   0.00857        4.06 4.98e-  5\r\n 6 dewp         -0.0464   0.00928       -5.00 5.66e-  7\r\n 7 humid         0.00977  0.00505        1.93 5.31e-  2\r\n 8 wind_dir      0.000722 0.000200       3.62 2.96e-  4\r\n 9 wind_speed   -0.0130   0.00546       -2.38 1.72e-  2\r\n10 wind_gust    -0.0128   0.00464       -2.76 5.85e-  3\r\n# ... with 154 more rows\r\n# i Use `print(n = ...)` to see more rows\r\n\r\n\r\n\r\ntidy(flights_fit) %>% filter(p.value <= 0.00001) %>% \r\n  ggplot(., aes(fill=p.value)) +\r\n  geom_col(aes(x=term, y=estimate)) +\r\n  coord_flip() +\r\n  scale_fill_gradient(low = \"green\", high = \"red\") +\r\n  theme_bw() +\r\n  labs(fill = \"Statistical p-value\", x = \"Term\", y = \"Estimate\") +\r\n  ggtitle(\"NYC Flights Signifiant Coeffients\")\r\n\r\n\r\n\r\nUse a trained workflow to\r\npredict\r\n\r\n\r\npredict(flights_fit, test_data)\r\n\r\n# A tibble: 18,184 x 1\r\n   .pred_class\r\n   <fct>      \r\n 1 on_time    \r\n 2 on_time    \r\n 3 on_time    \r\n 4 on_time    \r\n 5 on_time    \r\n 6 on_time    \r\n 7 on_time    \r\n 8 on_time    \r\n 9 on_time    \r\n10 on_time    \r\n# ... with 18,174 more rows\r\n# i Use `print(n = ...)` to see more rows\r\n\r\nThis amazing little function will give us not only our predictions,\r\nbut the probability presented from the model, and all of the original\r\nrow data from the fit (pre-receipe). This will be useful for\r\nplotting.\r\n\r\n\r\nflights_aug = broom.mixed::augment(flights_fit, test_data)\r\nflights_aug %>% head\r\n\r\n# A tibble: 6 x 22\r\n  time_hour           flight date       dep_time origin dest  air_time\r\n  <dttm>               <int> <date>        <int> <fct>  <fct>    <dbl>\r\n1 2013-01-01 06:00:00   4650 2013-01-01      600 LGA    ATL        134\r\n2 2013-01-01 06:00:00   4401 2013-01-01      602 LGA    DTW        105\r\n3 2013-01-01 06:00:00   4599 2013-01-01      624 LGA    MSP        166\r\n4 2013-01-01 06:00:00    303 2013-01-01      629 LGA    ORD        140\r\n5 2013-01-01 06:00:00   4646 2013-01-01      629 LGA    BWI         40\r\n6 2013-01-01 07:00:00    305 2013-01-01      656 LGA    ORD        143\r\n# ... with 15 more variables: distance <dbl>, carrier <fct>,\r\n#   temp <dbl>, dewp <dbl>, humid <dbl>, wind_dir <dbl>,\r\n#   wind_speed <dbl>, wind_gust <dbl>, precip <dbl>, pressure <dbl>,\r\n#   visib <dbl>, arr_delay <fct>, .pred_class <fct>,\r\n#   .pred_late <dbl>, .pred_on_time <dbl>\r\n# i Use `colnames()` to see all variable names\r\n\r\nflights_aug %>% select(arr_delay : .pred_on_time)\r\n\r\n# A tibble: 18,184 x 4\r\n   arr_delay .pred_class .pred_late .pred_on_time\r\n   <fct>     <fct>            <dbl>         <dbl>\r\n 1 on_time   on_time         0.0376         0.962\r\n 2 on_time   on_time         0.0296         0.970\r\n 3 on_time   on_time         0.0333         0.967\r\n 4 on_time   on_time         0.0302         0.970\r\n 5 on_time   on_time         0.0177         0.982\r\n 6 on_time   on_time         0.0378         0.962\r\n 7 on_time   on_time         0.0183         0.982\r\n 8 on_time   on_time         0.0350         0.965\r\n 9 on_time   on_time         0.0374         0.963\r\n10 on_time   on_time         0.0559         0.944\r\n# ... with 18,174 more rows\r\n# i Use `print(n = ...)` to see more rows\r\n\r\n\r\n\r\nflights_aug %>% \r\n  roc_curve(truth = arr_delay, .pred_late)\r\n\r\n# A tibble: 18,186 x 3\r\n      .threshold specificity sensitivity\r\n           <dbl>       <dbl>       <dbl>\r\n 1 -Inf            0                   1\r\n 2    0.00000102   0                   1\r\n 3    0.00000282   0.0000656           1\r\n 4    0.000678     0.000131            1\r\n 5    0.000697     0.000197            1\r\n 6    0.00158      0.000262            1\r\n 7    0.00174      0.000328            1\r\n 8    0.00199      0.000393            1\r\n 9    0.00208      0.000459            1\r\n10    0.00223      0.000525            1\r\n# ... with 18,176 more rows\r\n# i Use `print(n = ...)` to see more rows\r\n\r\nflights_aug %>% \r\n  roc_curve(truth = arr_delay, .pred_late) %>% \r\n  autoplot()\r\n\r\n\r\n\r\nAs a reminder\r\nSensitivity = TP / (TP + FN)\r\nSpecificity = TN / (TN + FP)\r\nSensitivity is goodness against the first row of the confusion\r\nmatrix. Specificity is goodness against the second row of the confusion\r\nmatrix.\r\nEvaluate your model with\r\nresampling\r\nGetting data\r\n\r\n\r\nlibrary(tidymodels)\r\nlibrary(modeldata)\r\n\r\n\r\n\r\n\r\ndata(cells, package = \"modeldata\")\r\ncells %>% glimpse\r\n\r\nRows: 2,019\r\nColumns: 58\r\n$ case                         <fct> Test, Train, Train, Train, Test~\r\n$ class                        <fct> PS, PS, WS, PS, PS, WS, WS, PS,~\r\n$ angle_ch_1                   <dbl> 143.247705, 133.752037, 106.646~\r\n$ area_ch_1                    <int> 185, 819, 431, 298, 285, 172, 1~\r\n$ avg_inten_ch_1               <dbl> 15.71186, 31.92327, 28.03883, 1~\r\n$ avg_inten_ch_2               <dbl> 4.954802, 206.878517, 116.31553~\r\n$ avg_inten_ch_3               <dbl> 9.548023, 69.916880, 63.941748,~\r\n$ avg_inten_ch_4               <dbl> 2.214689, 164.153453, 106.69660~\r\n$ convex_hull_area_ratio_ch_1  <dbl> 1.124509, 1.263158, 1.053310, 1~\r\n$ convex_hull_perim_ratio_ch_1 <dbl> 0.9196827, 0.7970801, 0.9354750~\r\n$ diff_inten_density_ch_1      <dbl> 29.51923, 31.87500, 32.48771, 2~\r\n$ diff_inten_density_ch_3      <dbl> 13.77564, 43.12228, 35.98577, 2~\r\n$ diff_inten_density_ch_4      <dbl> 6.826923, 79.308424, 51.357050,~\r\n$ entropy_inten_ch_1           <dbl> 4.969781, 6.087592, 5.883557, 5~\r\n$ entropy_inten_ch_3           <dbl> 4.371017, 6.642761, 6.683000, 5~\r\n$ entropy_inten_ch_4           <dbl> 2.718884, 7.880155, 7.144601, 5~\r\n$ eq_circ_diam_ch_1            <dbl> 15.36954, 32.30558, 23.44892, 1~\r\n$ eq_ellipse_lwr_ch_1          <dbl> 3.060676, 1.558394, 1.375386, 3~\r\n$ eq_ellipse_oblate_vol_ch_1   <dbl> 336.9691, 2232.9055, 802.1945, ~\r\n$ eq_ellipse_prolate_vol_ch_1  <dbl> 110.0963, 1432.8246, 583.2504, ~\r\n$ eq_sphere_area_ch_1          <dbl> 742.1156, 3278.7256, 1727.4104,~\r\n$ eq_sphere_vol_ch_1           <dbl> 1900.996, 17653.525, 6750.985, ~\r\n$ fiber_align_2_ch_3           <dbl> 1.000000, 1.487935, 1.300522, 1~\r\n$ fiber_align_2_ch_4           <dbl> 1.000000, 1.352374, 1.522316, 1~\r\n$ fiber_length_ch_1            <dbl> 26.98132, 64.28230, 21.14115, 4~\r\n$ fiber_width_ch_1             <dbl> 7.410365, 13.167079, 21.141150,~\r\n$ inten_cooc_asm_ch_3          <dbl> 0.011183899, 0.028051061, 0.006~\r\n$ inten_cooc_asm_ch_4          <dbl> 0.050448005, 0.012594975, 0.006~\r\n$ inten_cooc_contrast_ch_3     <dbl> 40.751777, 8.227953, 14.446074,~\r\n$ inten_cooc_contrast_ch_4     <dbl> 13.895439, 6.984046, 16.700843,~\r\n$ inten_cooc_entropy_ch_3      <dbl> 7.199458, 6.822138, 7.580100, 6~\r\n$ inten_cooc_entropy_ch_4      <dbl> 5.249744, 7.098988, 7.671478, 7~\r\n$ inten_cooc_max_ch_3          <dbl> 0.07741935, 0.15321477, 0.02835~\r\n$ inten_cooc_max_ch_4          <dbl> 0.17197452, 0.07387141, 0.02319~\r\n$ kurt_inten_ch_1              <dbl> -0.656744087, -0.248769067, -0.~\r\n$ kurt_inten_ch_3              <dbl> -0.608058268, -0.330783900, 1.0~\r\n$ kurt_inten_ch_4              <dbl> 0.7258145, -0.2652638, 0.150614~\r\n$ length_ch_1                  <dbl> 26.20779, 47.21855, 28.14303, 3~\r\n$ neighbor_avg_dist_ch_1       <dbl> 370.4543, 174.4442, 158.4774, 2~\r\n$ neighbor_min_dist_ch_1       <dbl> 99.10349, 30.11114, 34.94477, 3~\r\n$ neighbor_var_dist_ch_1       <dbl> 127.96080, 81.38063, 90.43768, ~\r\n$ perim_ch_1                   <dbl> 68.78338, 154.89876, 84.56460, ~\r\n$ shape_bfr_ch_1               <dbl> 0.6651480, 0.5397584, 0.7243116~\r\n$ shape_lwr_ch_1               <dbl> 2.462450, 1.468181, 1.328408, 2~\r\n$ shape_p_2_a_ch_1             <dbl> 1.883006, 2.255810, 1.272193, 2~\r\n$ skew_inten_ch_1              <dbl> 0.45450484, 0.39870467, 0.47248~\r\n$ skew_inten_ch_3              <dbl> 0.46039340, 0.61973079, 0.97137~\r\n$ skew_inten_ch_4              <dbl> 1.2327736, 0.5272631, 0.3247065~\r\n$ spot_fiber_count_ch_3        <int> 1, 4, 2, 4, 1, 1, 0, 2, 1, 1, 1~\r\n$ spot_fiber_count_ch_4        <dbl> 5, 12, 7, 8, 8, 5, 5, 8, 12, 8,~\r\n$ total_inten_ch_1             <int> 2781, 24964, 11552, 5545, 6603,~\r\n$ total_inten_ch_2             <dbl> 701, 160998, 47511, 28870, 3030~\r\n$ total_inten_ch_3             <int> 1690, 54675, 26344, 8042, 5569,~\r\n$ total_inten_ch_4             <int> 392, 128368, 43959, 8843, 11037~\r\n$ var_inten_ch_1               <dbl> 12.47468, 18.80923, 17.29564, 1~\r\n$ var_inten_ch_3               <dbl> 7.609035, 56.715352, 37.671053,~\r\n$ var_inten_ch_4               <dbl> 2.714100, 118.388139, 49.470524~\r\n$ width_ch_1                   <dbl> 10.64297, 32.16126, 21.18553, 1~\r\n\r\n\r\n\r\ncells %>% \r\n  count(class) %>% \r\n  mutate(prop = n/sum(n))\r\n\r\n# A tibble: 2 x 3\r\n  class     n  prop\r\n  <fct> <int> <dbl>\r\n1 PS     1300 0.644\r\n2 WS      719 0.356\r\n\r\nData splitting\r\nThe rsample package is used to preserve splits for a\r\nfuture date. The seed guarentees we can reproduce the same random\r\nnumbers if we should choose to return to this step.\r\n\r\n\r\nset.seed(123)\r\ncell_split = rsample::initial_split(cells %>% select(-case),\r\n                                    strata = class)\r\n\r\n\r\nThe strata argument keeps the class proportion balanced\r\nin the train and test split.\r\n\r\n\r\ncell_train = rsample::training(cell_split)\r\ncell_test = rsample::testing(cell_split)\r\n\r\nnrow(cell_train)\r\n\r\n[1] 1514\r\n\r\nnrow(cell_train)/nrow(cells)\r\n\r\n[1] 0.7498762\r\n\r\nnrow(cell_test)\r\n\r\n[1] 505\r\n\r\nnrow(cell_test)/nrow(cells)\r\n\r\n[1] 0.2501238\r\n\r\ncell_train %>% \r\n  count(class) %>% \r\n  mutate(prop = n/sum(n))\r\n\r\n# A tibble: 2 x 3\r\n  class     n  prop\r\n  <fct> <int> <dbl>\r\n1 PS      975 0.644\r\n2 WS      539 0.356\r\n\r\ncell_test %>% \r\n  count(class) %>% \r\n  mutate(prop = n/sum(n))\r\n\r\n# A tibble: 2 x 3\r\n  class     n  prop\r\n  <fct> <int> <dbl>\r\n1 PS      325 0.644\r\n2 WS      180 0.356\r\n\r\nModeling\r\nTime to build a random forest model. We won’t be building a\r\nrecipe because very little pre-processing is needed for\r\nrandom forests and decision trees to be useful out of the box.\r\n\r\n\r\nrf_mod =\r\n  rand_forest(trees = 1000) %>% \r\n  set_engine(\"ranger\") %>% \r\n  set_mode(\"classification\")\r\n\r\nset.seed(234)\r\nrf_fit =\r\n  rf_mod %>% \r\n  fit(class ~ ., data = cell_train)\r\n\r\nrf_fit\r\n\r\nparsnip model object\r\n\r\nRanger result\r\n\r\nCall:\r\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \r\n\r\nType:                             Probability estimation \r\nNumber of trees:                  1000 \r\nSample size:                      1514 \r\nNumber of independent variables:  56 \r\nMtry:                             7 \r\nTarget node size:                 10 \r\nVariable importance mode:         none \r\nSplitrule:                        gini \r\nOOB prediction error (Brier s.):  0.1187479 \r\n\r\nEstimating performance\r\nThe yardstick package can be used to measure the\r\nroc_auc() to see how our model performs\r\n\r\n\r\nrf_training_pred = \r\n  predict(rf_fit, cell_train) %>% \r\n  dplyr::bind_cols(predict(rf_fit, cell_train, type=\"prob\")) %>% \r\n  dplyr::bind_cols(cell_train %>% select(class))\r\nrf_training_pred\r\n\r\n# A tibble: 1,514 x 4\r\n   .pred_class .pred_PS .pred_WS class\r\n   <fct>          <dbl>    <dbl> <fct>\r\n 1 PS             0.740  0.260   PS   \r\n 2 PS             0.940  0.0597  PS   \r\n 3 PS             0.929  0.0707  PS   \r\n 4 PS             0.959  0.0406  PS   \r\n 5 PS             0.926  0.0738  PS   \r\n 6 PS             0.681  0.319   PS   \r\n 7 PS             0.991  0.00861 PS   \r\n 8 PS             0.769  0.231   PS   \r\n 9 PS             0.860  0.140   PS   \r\n10 PS             0.751  0.249   PS   \r\n# ... with 1,504 more rows\r\n# i Use `print(n = ...)` to see more rows\r\n\r\n\r\n\r\nrf_training_pred %>% \r\n  roc_auc(truth = class, .pred_PS)\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 roc_auc binary          1.00\r\n\r\n#auc = 0.99 - lol\r\n\r\nrf_training_pred %>% \r\n  accuracy(truth = class, .pred_class)\r\n\r\n# A tibble: 1 x 3\r\n  .metric  .estimator .estimate\r\n  <chr>    <chr>          <dbl>\r\n1 accuracy binary         0.990\r\n\r\n# acc = 0.99 - lol\r\n\r\n\r\n\r\n\r\nrf_testing_pred = \r\n  predict(rf_fit, cell_test) %>% \r\n  dplyr::bind_cols(predict(rf_fit, cell_test, type=\"prob\")) %>% \r\n  dplyr::bind_cols(cell_test %>% select(class))\r\n\r\nrf_testing_pred %>% head\r\n\r\n# A tibble: 6 x 4\r\n  .pred_class .pred_PS .pred_WS class\r\n  <fct>          <dbl>    <dbl> <fct>\r\n1 PS            0.893    0.107  PS   \r\n2 PS            0.920    0.0797 PS   \r\n3 WS            0.0801   0.920  WS   \r\n4 PS            0.817    0.183  WS   \r\n5 PS            0.752    0.248  PS   \r\n6 WS            0.279    0.721  WS   \r\n\r\nrf_testing_pred %>% \r\n  roc_auc(truth = class, .pred_PS)\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 roc_auc binary         0.891\r\n\r\nrf_testing_pred %>% \r\n  accuracy(truth = class, .pred_class)\r\n\r\n# A tibble: 1 x 3\r\n  .metric  .estimator .estimate\r\n  <chr>    <chr>          <dbl>\r\n1 accuracy binary         0.814\r\n\r\nThis shows that predicting on the training set only shows what the\r\nmodel actually knows. It essentially memorizes the data it has seen.\r\nResampling\r\nCross-validation, bootstrap, and empirical simulations can remedy\r\nthis!\r\nThese create a series of data sets that can be used to measure\r\nperformance against numerous folds of data, giving a much\r\nbetter indicator of performance.\r\nWe will use 10-fold cross validation. This essential chunks the data\r\nup into 10 slices (remember, only the training data), then for 10\r\niterations loops through it with 9/10 of the chunks as training and 1/10\r\nfor testing. You can repeat this process multiple times since it is\r\nstochastic in nature (chunks can be the same if resampled). This is a\r\nvery robust approach as it uses your data as a representative sample\r\nfrom the population.\r\n\r\n\r\nset.seed(345)\r\nfolds = vfold_cv(cell_train, v = 10)\r\nfolds\r\n\r\n#  10-fold cross-validation \r\n# A tibble: 10 x 2\r\n   splits             id    \r\n   <list>             <chr> \r\n 1 <split [1362/152]> Fold01\r\n 2 <split [1362/152]> Fold02\r\n 3 <split [1362/152]> Fold03\r\n 4 <split [1362/152]> Fold04\r\n 5 <split [1363/151]> Fold05\r\n 6 <split [1363/151]> Fold06\r\n 7 <split [1363/151]> Fold07\r\n 8 <split [1363/151]> Fold08\r\n 9 <split [1363/151]> Fold09\r\n10 <split [1363/151]> Fold10\r\n\r\n\r\n\r\nsplit = folds %>% select(splits) %>% filter(row_number() == 1) %>% pull(splits)\r\nsplit_1_analysis_set = split[[1]] %>% analysis\r\nsplit_1_analysis_set %>% head\r\n\r\n# A tibble: 6 x 57\r\n  class angle_ch_1 area_ch_1 avg_int~1 avg_i~2 avg_i~3 avg_i~4 conve~5\r\n  <fct>      <dbl>     <int>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\r\n1 PS         134.        819      31.9  207.      69.9  164.      1.26\r\n2 PS          69.2       298      19.5  102.      28.2   31.0     1.20\r\n3 PS         180.        251      18.3    5.73    17.2    1.55    1.20\r\n4 PS          52.2       236      18.2    6.33    17.1    1.91    1.29\r\n5 PS         104.        258      17.6  125.      22.5   71.2     1.08\r\n6 PS          78.0       358      42.3  218.      42.3   67.5     1.04\r\n# ... with 49 more variables: convex_hull_perim_ratio_ch_1 <dbl>,\r\n#   diff_inten_density_ch_1 <dbl>, diff_inten_density_ch_3 <dbl>,\r\n#   diff_inten_density_ch_4 <dbl>, entropy_inten_ch_1 <dbl>,\r\n#   entropy_inten_ch_3 <dbl>, entropy_inten_ch_4 <dbl>,\r\n#   eq_circ_diam_ch_1 <dbl>, eq_ellipse_lwr_ch_1 <dbl>,\r\n#   eq_ellipse_oblate_vol_ch_1 <dbl>,\r\n#   eq_ellipse_prolate_vol_ch_1 <dbl>, eq_sphere_area_ch_1 <dbl>, ...\r\n# i Use `colnames()` to see all variable names\r\n\r\nsplit_1_assessment_set = split[[1]] %>% assessment\r\nsplit_1_assessment_set %>% head\r\n\r\n# A tibble: 6 x 57\r\n  class angle_ch_1 area_ch_1 avg_int~1 avg_i~2 avg_i~3 avg_i~4 conve~5\r\n  <fct>      <dbl>     <int>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\r\n1 PS         139.        771      21.0    22.5  40.4     142.     1.44\r\n2 PS          31.2       166      95.0   345.   41.5     188.     1.15\r\n3 PS          22.6       221     129.    127.  284.       62.0    1.12\r\n4 PS          65.4       295      19.9   193.    0.560    29.6    1.21\r\n5 PS          99.1       253      24.4    33.3  48.8      56.2    1.62\r\n6 PS          85.6       645      61.9   282.   73.1     232.     1.20\r\n# ... with 49 more variables: convex_hull_perim_ratio_ch_1 <dbl>,\r\n#   diff_inten_density_ch_1 <dbl>, diff_inten_density_ch_3 <dbl>,\r\n#   diff_inten_density_ch_4 <dbl>, entropy_inten_ch_1 <dbl>,\r\n#   entropy_inten_ch_3 <dbl>, entropy_inten_ch_4 <dbl>,\r\n#   eq_circ_diam_ch_1 <dbl>, eq_ellipse_lwr_ch_1 <dbl>,\r\n#   eq_ellipse_oblate_vol_ch_1 <dbl>,\r\n#   eq_ellipse_prolate_vol_ch_1 <dbl>, eq_sphere_area_ch_1 <dbl>, ...\r\n# i Use `colnames()` to see all variable names\r\n\r\nFit a model with resampling\r\n\r\n\r\nrf_wf =\r\n  workflow() %>% \r\n  add_model(rf_mod) %>% \r\n  add_formula(class ~ .)\r\n\r\nset.seed(456)\r\nrf_fit_rs = \r\n  rf_wf %>% \r\n  fit_resamples(folds)\r\n\r\nrf_fit_rs\r\n\r\n# Resampling results\r\n# 10-fold cross-validation \r\n# A tibble: 10 x 4\r\n   splits             id     .metrics         .notes          \r\n   <list>             <chr>  <list>           <list>          \r\n 1 <split [1362/152]> Fold01 <tibble [2 x 4]> <tibble [0 x 3]>\r\n 2 <split [1362/152]> Fold02 <tibble [2 x 4]> <tibble [0 x 3]>\r\n 3 <split [1362/152]> Fold03 <tibble [2 x 4]> <tibble [0 x 3]>\r\n 4 <split [1362/152]> Fold04 <tibble [2 x 4]> <tibble [0 x 3]>\r\n 5 <split [1363/151]> Fold05 <tibble [2 x 4]> <tibble [0 x 3]>\r\n 6 <split [1363/151]> Fold06 <tibble [2 x 4]> <tibble [0 x 3]>\r\n 7 <split [1363/151]> Fold07 <tibble [2 x 4]> <tibble [0 x 3]>\r\n 8 <split [1363/151]> Fold08 <tibble [2 x 4]> <tibble [0 x 3]>\r\n 9 <split [1363/151]> Fold09 <tibble [2 x 4]> <tibble [0 x 3]>\r\n10 <split [1363/151]> Fold10 <tibble [2 x 4]> <tibble [0 x 3]>\r\n\r\n\r\n\r\ncollect_metrics(rf_fit_rs)\r\n\r\n# A tibble: 2 x 6\r\n  .metric  .estimator  mean     n std_err .config             \r\n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \r\n1 accuracy binary     0.833    10 0.00876 Preprocessor1_Model1\r\n2 roc_auc  binary     0.905    10 0.00627 Preprocessor1_Model1\r\n\r\nThis is a much better representation than the original metrics\r\nproduced from testing on our training data. This is always\r\nover-optimistic. Using cross-validation, we can simulate how well our\r\nmodel will perform on actual test data. Then finally once we are done we\r\ncan use our test set as the final unbiased check for the model’s\r\nperformance.\r\nTune model parameters\r\nLastly, we can explore how to tune the model parameters to make our\r\nmodels even better.\r\n\r\n\r\nlibrary(tidymodels)\r\nlibrary(rpart.plot)\r\nlibrary(vip)\r\n\r\n\r\nGetting data\r\nData splitting\r\n\r\n\r\nset.seed(123)\r\ncell_split <- initial_split(cells %>% select(-case), \r\n                            strata = class)\r\ncell_train <- training(cell_split)\r\ncell_test  <- testing(cell_split)\r\n\r\n\r\nTuning hyperparameters\r\n\r\n\r\n# model with tunings specifications\r\ntune_spec =\r\n  decision_tree(\r\n    cost_complexity = tune(), \r\n    tree_depth = tune()\r\n  ) %>% \r\n  set_engine(\"rpart\") %>% \r\n  set_mode(\"classification\")\r\n\r\ntune_spec\r\n\r\nDecision Tree Model Specification (classification)\r\n\r\nMain Arguments:\r\n  cost_complexity = tune()\r\n  tree_depth = tune()\r\n\r\nComputational engine: rpart \r\n\r\n\r\n\r\ntree_grid = grid_regular(cost_complexity(), \r\n                         tree_depth(),\r\n                         levels = 5)\r\ntree_grid\r\n\r\n# A tibble: 25 x 2\r\n   cost_complexity tree_depth\r\n             <dbl>      <int>\r\n 1    0.0000000001          1\r\n 2    0.0000000178          1\r\n 3    0.00000316            1\r\n 4    0.000562              1\r\n 5    0.1                   1\r\n 6    0.0000000001          4\r\n 7    0.0000000178          4\r\n 8    0.00000316            4\r\n 9    0.000562              4\r\n10    0.1                   4\r\n# ... with 15 more rows\r\n# i Use `print(n = ...)` to see more rows\r\n\r\ntree_grid %>% count(tree_depth)\r\n\r\n# A tibble: 5 x 2\r\n  tree_depth     n\r\n       <int> <int>\r\n1          1     5\r\n2          4     5\r\n3          8     5\r\n4         11     5\r\n5         15     5\r\n\r\n\r\n\r\nset.seed(234)\r\ncell_folds = vfold_cv(cell_train)\r\n\r\n\r\nModel tuning with a grid\r\n\r\n\r\nset.seed(345)\r\ntree_wf = workflow() %>% \r\n  add_model(tune_spec) %>% \r\n  add_formula(class ~ .)\r\ntree_wf\r\n\r\n== Workflow ==========================================================\r\nPreprocessor: Formula\r\nModel: decision_tree()\r\n\r\n-- Preprocessor ------------------------------------------------------\r\nclass ~ .\r\n\r\n-- Model -------------------------------------------------------------\r\nDecision Tree Model Specification (classification)\r\n\r\nMain Arguments:\r\n  cost_complexity = tune()\r\n  tree_depth = tune()\r\n\r\nComputational engine: rpart \r\n\r\ntree_res =\r\n  tree_wf %>% \r\n  tune_grid(\r\n    resamples = cell_folds,\r\n    grid = tree_grid\r\n  )\r\ntree_res\r\n\r\n# Tuning results\r\n# 10-fold cross-validation \r\n# A tibble: 10 x 4\r\n   splits             id     .metrics          .notes          \r\n   <list>             <chr>  <list>            <list>          \r\n 1 <split [1362/152]> Fold01 <tibble [50 x 6]> <tibble [0 x 3]>\r\n 2 <split [1362/152]> Fold02 <tibble [50 x 6]> <tibble [0 x 3]>\r\n 3 <split [1362/152]> Fold03 <tibble [50 x 6]> <tibble [0 x 3]>\r\n 4 <split [1362/152]> Fold04 <tibble [50 x 6]> <tibble [0 x 3]>\r\n 5 <split [1363/151]> Fold05 <tibble [50 x 6]> <tibble [0 x 3]>\r\n 6 <split [1363/151]> Fold06 <tibble [50 x 6]> <tibble [0 x 3]>\r\n 7 <split [1363/151]> Fold07 <tibble [50 x 6]> <tibble [0 x 3]>\r\n 8 <split [1363/151]> Fold08 <tibble [50 x 6]> <tibble [0 x 3]>\r\n 9 <split [1363/151]> Fold09 <tibble [50 x 6]> <tibble [0 x 3]>\r\n10 <split [1363/151]> Fold10 <tibble [50 x 6]> <tibble [0 x 3]>\r\n\r\n\r\n\r\ntree_res %>% \r\n  collect_metrics()\r\n\r\n# A tibble: 50 x 8\r\n   cost_complexity tree_~1 .metric .esti~2  mean     n std_err .config\r\n             <dbl>   <int> <chr>   <chr>   <dbl> <int>   <dbl> <chr>  \r\n 1    0.0000000001       1 accura~ binary  0.732    10  0.0148 Prepro~\r\n 2    0.0000000001       1 roc_auc binary  0.777    10  0.0107 Prepro~\r\n 3    0.0000000178       1 accura~ binary  0.732    10  0.0148 Prepro~\r\n 4    0.0000000178       1 roc_auc binary  0.777    10  0.0107 Prepro~\r\n 5    0.00000316         1 accura~ binary  0.732    10  0.0148 Prepro~\r\n 6    0.00000316         1 roc_auc binary  0.777    10  0.0107 Prepro~\r\n 7    0.000562           1 accura~ binary  0.732    10  0.0148 Prepro~\r\n 8    0.000562           1 roc_auc binary  0.777    10  0.0107 Prepro~\r\n 9    0.1                1 accura~ binary  0.732    10  0.0148 Prepro~\r\n10    0.1                1 roc_auc binary  0.777    10  0.0107 Prepro~\r\n# ... with 40 more rows, and abbreviated variable names\r\n#   1: tree_depth, 2: .estimator\r\n# i Use `print(n = ...)` to see more rows\r\n\r\ntree_res %>% \r\n  collect_metrics() %>% \r\n  mutate(tree_depth = factor(tree_depth)) %>% \r\n  ggplot(aes(x=cost_complexity, y=mean, color=tree_depth)) +\r\n  geom_line(size = 1.5, alpha = 0.5) +\r\n  geom_point(size = 2) +\r\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\r\n  scale_x_log10(labels = scales::label_number()) +\r\n  scale_color_viridis_d(option = \"plasma\", begin = 0.9, end = 0)\r\n\r\n\r\n\r\nWe can quickly identify the best best parameter configuration as\r\ntree_depth=4 on any cost_complexity.\r\n\r\n\r\ntree_res %>% show_best(\"roc_auc\")\r\n\r\n# A tibble: 5 x 8\r\n  cost_complexity tree_d~1 .metric .esti~2  mean     n std_err .config\r\n            <dbl>    <int> <chr>   <chr>   <dbl> <int>   <dbl> <chr>  \r\n1    0.000562           11 roc_auc binary  0.855    10  0.0147 Prepro~\r\n2    0.0000000001       11 roc_auc binary  0.854    10  0.0146 Prepro~\r\n3    0.0000000178       11 roc_auc binary  0.854    10  0.0146 Prepro~\r\n4    0.00000316         11 roc_auc binary  0.854    10  0.0146 Prepro~\r\n5    0.000562            8 roc_auc binary  0.854    10  0.0145 Prepro~\r\n# ... with abbreviated variable names 1: tree_depth, 2: .estimator\r\n\r\ntree_res %>% show_best(\"accuracy\")\r\n\r\n# A tibble: 5 x 8\r\n  cost_complexity tree_d~1 .metric .esti~2  mean     n std_err .config\r\n            <dbl>    <int> <chr>   <chr>   <dbl> <int>   <dbl> <chr>  \r\n1    0.0000000001        4 accura~ binary  0.807    10  0.0119 Prepro~\r\n2    0.0000000178        4 accura~ binary  0.807    10  0.0119 Prepro~\r\n3    0.00000316          4 accura~ binary  0.807    10  0.0119 Prepro~\r\n4    0.000562            4 accura~ binary  0.807    10  0.0119 Prepro~\r\n5    0.1                 4 accura~ binary  0.786    10  0.0124 Prepro~\r\n# ... with abbreviated variable names 1: tree_depth, 2: .estimator\r\n\r\nThe show_best is pretty sweet. It gives us just the\r\nparameter configurations that performed the best at the top.\r\nselect_best() will actually get that model\r\nconfiguration.\r\n\r\n\r\nbest_tree <- tree_res %>%\r\n  select_best(\"accuracy\")\r\n\r\nbest_tree\r\n\r\n# A tibble: 1 x 3\r\n  cost_complexity tree_depth .config              \r\n            <dbl>      <int> <chr>                \r\n1    0.0000000001          4 Preprocessor1_Model06\r\n\r\nFinalizing our model\r\nThe finalize_workflow() will accept the optimal\r\nparameter, pluck it out, and save our best model for us in the final\r\nworkflow object. Now we just need one last fit with that excellent final\r\nmodel.\r\n\r\n\r\nfinal_wf =\r\n  tree_wf %>% \r\n  finalize_workflow(best_tree)\r\n\r\nfinal_wf\r\n\r\n== Workflow ==========================================================\r\nPreprocessor: Formula\r\nModel: decision_tree()\r\n\r\n-- Preprocessor ------------------------------------------------------\r\nclass ~ .\r\n\r\n-- Model -------------------------------------------------------------\r\nDecision Tree Model Specification (classification)\r\n\r\nMain Arguments:\r\n  cost_complexity = 1e-10\r\n  tree_depth = 4\r\n\r\nComputational engine: rpart \r\n\r\n\r\n\r\nfinal_fit =\r\n  final_wf %>% \r\n  last_fit(cell_split)\r\n\r\nfinal_fit %>% \r\n  collect_metrics()\r\n\r\n# A tibble: 2 x 4\r\n  .metric  .estimator .estimate .config             \r\n  <chr>    <chr>          <dbl> <chr>               \r\n1 accuracy binary         0.802 Preprocessor1_Model1\r\n2 roc_auc  binary         0.840 Preprocessor1_Model1\r\n\r\nfinal_fit %>% \r\n  collect_predictions() %>% \r\n  roc_curve(truth = class, .pred_PS) %>% \r\n  autoplot()\r\n\r\n\r\n\r\n\r\n\r\nfinal_tree <- extract_workflow(final_fit)\r\nfinal_tree\r\n\r\n== Workflow [trained] ================================================\r\nPreprocessor: Formula\r\nModel: decision_tree()\r\n\r\n-- Preprocessor ------------------------------------------------------\r\nclass ~ .\r\n\r\n-- Model -------------------------------------------------------------\r\nn= 1514 \r\n\r\nnode), split, n, loss, yval, (yprob)\r\n      * denotes terminal node\r\n\r\n 1) root 1514 539 PS (0.64398943 0.35601057)  \r\n   2) total_inten_ch_2< 41732.5 642  33 PS (0.94859813 0.05140187)  \r\n     4) shape_p_2_a_ch_1>=1.251801 631  27 PS (0.95721078 0.04278922) *\r\n     5) shape_p_2_a_ch_1< 1.251801 11   5 WS (0.45454545 0.54545455) *\r\n   3) total_inten_ch_2>=41732.5 872 366 WS (0.41972477 0.58027523)  \r\n     6) fiber_width_ch_1< 11.37318 406 160 PS (0.60591133 0.39408867)  \r\n      12) avg_inten_ch_1< 145.4883 293  85 PS (0.70989761 0.29010239) *\r\n      13) avg_inten_ch_1>=145.4883 113  38 WS (0.33628319 0.66371681)  \r\n        26) total_inten_ch_3>=57919.5 33  10 PS (0.69696970 0.30303030) *\r\n        27) total_inten_ch_3< 57919.5 80  15 WS (0.18750000 0.81250000) *\r\n     7) fiber_width_ch_1>=11.37318 466 120 WS (0.25751073 0.74248927)  \r\n      14) eq_ellipse_oblate_vol_ch_1>=1673.942 30   8 PS (0.73333333 0.26666667)  \r\n        28) var_inten_ch_3>=41.10858 20   2 PS (0.90000000 0.10000000) *\r\n        29) var_inten_ch_3< 41.10858 10   4 WS (0.40000000 0.60000000) *\r\n      15) eq_ellipse_oblate_vol_ch_1< 1673.942 436  98 WS (0.22477064 0.77522936) *\r\n\r\n\r\n\r\nlibrary(rpart.plot)\r\nfinal_tree %>%\r\n  extract_fit_engine() %>%\r\n  rpart.plot(roundint = FALSE)\r\n\r\n\r\n\r\n\r\n\r\nlibrary(vip)\r\nfinal_tree %>% \r\n  extract_fit_parsnip() %>% \r\n  vip()\r\n\r\n\r\n\r\n\r\n\r\nargs(decision_tree)\r\n\r\nfunction (mode = \"unknown\", engine = \"rpart\", cost_complexity = NULL, \r\n    tree_depth = NULL, min_n = NULL) \r\nNULL\r\n\r\nSummary\r\nIn summary, a typical tidymodels workflow will look like\r\nthis:\r\nCreate an initial_split for your data\r\nCreate a vfold_cv() with the number of folds to pull\r\nfrom the training data.\r\nCreate a model object, set_engine and\r\nset_mode.\r\nadd the tune() argument to the model parameters if\r\ntuning desired.\r\nadd a grid_regular with the amount of parameters to\r\ntune for.\r\nfit the model object on your data using a formula.\r\nCreate a workflow()\r\nadd_recipe (with formula in recipe) OR\r\nadd_formula\r\npass the workflow into tune_grid() with the\r\nresamples=folds and grid=grid_regular().\r\ncollect_metrics()\r\nshow_best()\r\nselect_best()\r\nfinalize_workflow()\r\nlast_fit()\r\ncollect_prediction\r\nroc_curve()\r\nextrac_workflow()\r\nextract_fit_engine() or\r\nextract_fit_parsnip() and plot if needed.\r\nReferences\r\nhttps://www.tidymodels.org\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-21-tidymodels-in-r/tidymodels-in-r_files/figure-html5/unnamed-chunk-55-1.png",
    "last_modified": "2022-07-23T10:02:37-04:00",
    "input_file": "tidymodels-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-17-create-custom-ml-models-using-keras-in-r/",
    "title": "Create Custom ML Models using Keras in R",
    "description": "In this post we will explore automatic differentiation as applied to custom machine learning model generation using keras in R.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-18",
    "categories": [
      "machine learning",
      "deep learning",
      "non-linear programming",
      "automatic differentiation",
      "keras",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nBuilding Custom Models\r\nFitting a linear model\r\nDefine a loss\r\nObtain training data\r\nDefine a training loop\r\nCustom Logistic Regression\r\nLog likelihood\r\nTrain\r\nVisualize\r\nLearning populations\r\nPredictions\r\n\r\n\r\nReferences\r\n\r\nBuilding Custom Models\r\nIn the post on Automatic Differentiation with Keras in R\r\nthere was a walk through some of the documentation and an example of\r\nusing the GradientTape API. There is no creativity in this\r\npost, just a repeat from the keras documentation. The goal\r\nhere is to take an example, play around with it, and explore the API a\r\nbit.\r\nFitting a linear model\r\n\r\n\r\nlibrary(tensorflow)\r\nModel <- R6::R6Class(\r\n  classname = \"Model\",\r\n  public = list(\r\n    W = NULL,\r\n    b = NULL,\r\n    \r\n    initialize = function(){\r\n      self$W <- tf$Variable(5) # Initialize weight to 5\r\n      self$b <- tf$Variable(0) # Initialize bias to 0 \r\n      \r\n    },\r\n    \r\n    call = function(x){\r\n      self$W*x + self$b\r\n    }\r\n  )\r\n)\r\nmodel <- Model$new()\r\nmodel$call(3)\r\n\r\ntf.Tensor(15.0, shape=(), dtype=float32)\r\n\r\nDefine a loss\r\n\r\n\r\nloss <- function(y_pred, y_true) {\r\n  tf$reduce_mean(tf$square(y_pred - y_true))\r\n}\r\n\r\n\r\nObtain training data\r\n\r\n\r\nTRUE_W = 3.0\r\nTRUE_b = 2.0\r\n\r\nNUM_EXAMPLES = 1000\r\n\r\n# Random 1-d vector of inputs\r\ninputs = tf$random$normal(shape = shape(NUM_EXAMPLES))\r\n# Random 1-d noise\r\nnoise = tf$random$normal(shape = shape(NUM_EXAMPLES))\r\n\r\n# True 1-d output target population\r\noutputs = inputs * TRUE_W + TRUE_b + noise\r\n\r\n\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\ndf = tibble(\r\n  inputs = as.numeric(inputs),\r\n  outputs = as.numeric(outputs),\r\n  predicted = as.numeric(model$call(inputs))\r\n) \r\n\r\ndf %>% ggplot(., aes(x=inputs)) +\r\n  geom_point(aes(y = outputs)) +\r\n  geom_line(aes(y = predicted), color = \"blue\")\r\n\r\n\r\n\r\nDefine a training loop\r\n\r\n\r\ntrain <- function(model, inputs, outputs, learning_rate) {\r\n  with (tf$GradientTape() %as% t, {\r\n    current_loss = loss(model$call(inputs), outputs)\r\n  })\r\n  \r\n  d <- t$gradient(current_loss, list(model$W, model$b))\r\n  \r\n  model$W$assign_sub(learning_rate * d[[1]])\r\n  model$b$assign_sub(learning_rate * d[[2]])\r\n  current_loss\r\n}\r\n\r\n\r\n\r\n\r\nlibrary(glue)\r\n\r\nmodel <- Model$new()\r\n\r\nWs <- bs <- c()\r\n\r\nfor(epoch in seq_len(20)){\r\n  \r\n  Ws[epoch] <- as.numeric(model$W)\r\n  bs[epoch] <- as.numeric(model$b)\r\n  \r\n  current_loss <- train(model, inputs, outputs, learning_rate = 0.1)\r\n  cat(glue::glue(\"Epoch: {epoch}, Loss: {as.numeric(current_loss)}\"), \"\\n\")\r\n}\r\n\r\nEpoch: 1, Loss: 9.27498435974121 \r\nEpoch: 2, Loss: 6.29132127761841 \r\nEpoch: 3, Loss: 4.38681507110596 \r\nEpoch: 4, Loss: 3.17101907730103 \r\nEpoch: 5, Loss: 2.39479780197144 \r\nEpoch: 6, Loss: 1.89916634559631 \r\nEpoch: 7, Loss: 1.58266150951385 \r\nEpoch: 8, Loss: 1.38052105903625 \r\nEpoch: 9, Loss: 1.25140619277954 \r\nEpoch: 10, Loss: 1.16892516613007 \r\nEpoch: 11, Loss: 1.11622834205627 \r\nEpoch: 12, Loss: 1.08255589008331 \r\nEpoch: 13, Loss: 1.0610374212265 \r\nEpoch: 14, Loss: 1.04728376865387 \r\nEpoch: 15, Loss: 1.0384920835495 \r\nEpoch: 16, Loss: 1.03287136554718 \r\nEpoch: 17, Loss: 1.02927708625793 \r\nEpoch: 18, Loss: 1.0269787311554 \r\nEpoch: 19, Loss: 1.02550864219666 \r\nEpoch: 20, Loss: 1.02456820011139 \r\n\r\n\r\n\r\nlibrary(tidyr)\r\n\r\n# Variables per column\r\ndf = tibble(\r\n  epoch = 1:20,\r\n  Ws = Ws,\r\n  bs = bs\r\n)\r\n\r\n# Melt on multiple columns! Name their similarity..\r\ndf = df %>% tidyr::pivot_longer(c(Ws, bs), \r\n                           names_to = \"parameter\", \r\n                           values_to = \"estimate\")\r\n\r\nggplot(df, aes(x=epoch, y=estimate)) +\r\n  geom_line() +\r\n  facet_wrap(~parameter, scales = \"free\")\r\n\r\n\r\n\r\nCustom Logistic Regression\r\nNow let’s build a custom logistic regression model.\r\n\r\n\r\n# Generate data\r\ndf <- data.frame(matrix(rnorm(1000), nrow=100, ncol=2))\r\ndf$Y <- rbinom(n = 100, size = 1, prob = 0.25)\r\n\r\nX <- df %>% \r\n  select(-Y)\r\ny <- df %>% select(Y) %>% \r\n  dplyr::pull(Y) %>% \r\n  as.matrix\r\n\r\n\r\nNotice the data is collected from the following distributions:\r\nIndependent Variables \\[\r\nX \\sim \\mathcal{N}(\\mu=0,\\,\\sigma^{2}=1)\\\r\n\\] Dependent Variables \\[\r\ny \\sim \\mathcal{B}(n=1, p=0.25)\\\r\n\\] ### Build the model\r\n\r\n\r\nlibrary(tensorflow)\r\nlibrary(dplyr)\r\n\r\nModel <- R6::R6Class(\r\n  classname = \"Model\",\r\n  public = list(\r\n    W = NULL,\r\n    b = NULL,\r\n    \r\n    initialize = function(D){\r\n      \r\n      # Manually runs great, but won't knit..\r\n      self$W <- tf$Variable(matrix(rnorm(n = D), nrow=1, ncol=D))\r\n      self$b <- tf$Variable(matrix(rnorm(n = 1), nrow=1, ncol=1))\r\n\r\n    },\r\n    \r\n    call = function(x){\r\n      1/(1 + exp(-(tf$matmul(x, t(self$W)) + self$b)))\r\n    }\r\n  )\r\n)\r\n\r\n\r\nmodel <- Model$new(ncol(X))\r\nmodel$call(X) %>% head\r\n\r\ntf.Tensor(\r\n[[0.90854418]\r\n [0.94750512]\r\n [0.72264755]\r\n [0.69501559]\r\n [0.97674013]\r\n [0.85342092]], shape=(6, 1), dtype=float64)\r\n\r\n\\[\r\nL(X; W, b) = \\prod_{i=1}^n{\\hat y_{i}^{y_{i}} (1-\\hat y_{i})^{1-y_{i}} }\r\n\\]\r\nLog likelihood\r\n\r\n\r\n# Monotonic increasing, value function\r\nlog_likelihood <- function(y_pred, y_true){\r\n  ll <- tf$reduce_sum(y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))\r\n  ll\r\n}\r\n\r\n# y_pred = model$call(X)\r\n\r\n# log_likelihood(model$call(X), y)\r\n\r\n\r\nTrain\r\n\r\n\r\n# Generate data\r\ndf <- data.frame(matrix(rnorm(1000), nrow=100, ncol=2))\r\ndf$Y <- rbinom(n = 100, size = 1, prob = 0.25)\r\n\r\nX <- df %>% \r\n  select(-Y)\r\ny <- df %>% select(Y) %>% \r\n  dplyr::pull(Y) %>% \r\n  as.matrix\r\n\r\n\r\nD = ncol(X)\r\n\r\n# Create a fresh model\r\nlearning_rate = 0.1\r\nW = tf$Variable(matrix(rnorm(D), nrow=1, ncol=D))\r\nb = tf$Variable(matrix(rnorm(1), nrow=1, ncol=1))\r\n\r\nsigmoid = function(z){\r\n  1/(1+exp(-z))\r\n}\r\n\r\npredict.sigmoid = function(x, W, b){\r\n  y_pred = sigmoid(tf$matmul(X, t(W)) + b)\r\n  y_pred\r\n}\r\n\r\nlosses <- c()\r\n\r\nfor(epoch in seq_len(20)){\r\n  \r\n  \r\n  with (tf$GradientTape() %as% t, {\r\n    t$watch(W)\r\n    t$watch(b)\r\n    y_pred = predict.sigmoid(X, W, b)\r\n    \r\n    L = log_likelihood(y_pred, y)\r\n  })\r\n  \r\n  d <- t$gradient(L, list(W, b))\r\n    \r\n  # Gradient ascent\r\n  W$assign_add(learning_rate * d[[1]])\r\n  b$assign_add(learning_rate * d[[2]])\r\n  \r\n  current_loss = L\r\n  \r\n  cat(glue::glue(\"Epoch: {epoch}, Loss: {as.numeric(current_loss)}\"), \"\\n\")\r\n\r\n  losses[epoch] <- as.numeric(current_loss)\r\n    \r\n}\r\n\r\nEpoch: 1, Loss: -64.884168727686 \r\nEpoch: 2, Loss: -52.0970964369214 \r\nEpoch: 3, Loss: -40.7580844729695 \r\nEpoch: 4, Loss: -39.6961741609297 \r\nEpoch: 5, Loss: -39.6138575017734 \r\nEpoch: 6, Loss: -39.5851677656405 \r\nEpoch: 7, Loss: -39.5785648703051 \r\nEpoch: 8, Loss: -39.5763458015593 \r\nEpoch: 9, Loss: -39.5757237923894 \r\nEpoch: 10, Loss: -39.5755299326785 \r\nEpoch: 11, Loss: -39.5754728446656 \r\nEpoch: 12, Loss: -39.5754554963127 \r\nEpoch: 13, Loss: -39.5754503139949 \r\nEpoch: 14, Loss: -39.5754487512676 \r\nEpoch: 15, Loss: -39.5754482824529 \r\nEpoch: 16, Loss: -39.5754481414108 \r\nEpoch: 17, Loss: -39.5754480990443 \r\nEpoch: 18, Loss: -39.5754480863074 \r\nEpoch: 19, Loss: -39.5754480824799 \r\nEpoch: 20, Loss: -39.5754480813295 \r\n\r\nVisualize\r\n\r\n\r\nlibrary(ggplot2)\r\ntibble(\r\n  epoch = 1:20,\r\n  loss = losses\r\n) %>% \r\n  ggplot(., aes(x = epoch)) +\r\n  geom_line(aes(y=loss))\r\n\r\n\r\n\r\nLearning populations\r\n\r\n\r\ndf <- tibble(\r\n  y_true = y,\r\n  y_pred = predict.sigmoid(X, W, b) %>% as.matrix\r\n) %>% \r\n  mutate(y_pred_class = if_else(condition = y_pred > 0.5, true = 1, false = 0))\r\n\r\ndf %>% \r\n  ggplot(.,) + \r\n  geom_density(aes(x=y_true, fill=\"Target\"), alpha = 0.5) +\r\n  geom_density(aes(x=y_pred, fill=\"Prediction\"), alpha = 0.5) +\r\n  labs(fill = \"Response\") +\r\n  ggtitle(\"Target vs. Prediction Distribution\") +\r\n  theme_classic()\r\n\r\n\r\n\r\nIt looks like training in this way the\r\nLogistic Classifier was able to discover the population\r\nparameter for our underlying response distribution.\r\n\\[\r\n\\hat p =0.25\r\n\\]\r\nPredictions\r\nBut how well did it predict on the training data? Notice the\r\nover prediction on the class 0 and the\r\nunder prediction on the class 1. This is an infamous\r\nillustration of the class imbalance problem and how we\r\nlearned to maximize a the majority, which was not the target class.\r\n\r\n\r\ndf %>% ggplot() + \r\n  geom_bar(aes(x=y_true, fill=\"Target\"), alpha = 0.5) +\r\n  geom_bar(aes(x=y_pred_class, fill = \"Prediction\"), alpha = 0.5) +\r\n  labs(fill = \"Response\", x = \"Class\", y=\"Appearances\") +\r\n  ggtitle(\"Target vs. Prediction\")\r\n\r\n\r\n\r\nReferences\r\nhttps://tensorflow.rstudio.com/tutorials/advanced/customization/custom-training/\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-17-create-custom-ml-models-using-keras-in-r/create-custom-ml-models-using-keras-in-r_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-07-19T10:06:59-04:00",
    "input_file": "create-custom-ml-models-using-keras-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-17-automatic-differentiation-with-keras-in-r/",
    "title": "Automatic Differentiation with Keras in R",
    "description": "In this post we will explore the keras API and how it handles low level mathematical operations to perform automatic differentiation.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-17",
    "categories": [
      "machine learning",
      "deep learning",
      "non-linear programming",
      "keras",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nAutomatic\r\nDifferentiation Examples\r\nBasic Example\r\nAdvanced Example\r\n\r\nSandbox\r\nReferences\r\n\r\nAutomatic Differentiation\r\nExamples\r\nAutomatic differentiation is arguable one of the greatest inventions\r\nof all time. It has completely changed the face of modern deep learning.\r\nSome of the worlds hardest problems, like self-driving cars, actually\r\nbecome computationally feasible (and with ease) because of this\r\nbreakthrough. I personally never dove into using it, but gently knew of\r\nit from years ago being mentioned. Today the goal is to walk through\r\nsome boiler plate from the developers themselves, then try to expand on\r\nit with a small example to open up the mind. Let’s get started.\r\n\r\n\r\nlibrary(tensorflow)\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\n\r\n\r\nBasic Example\r\nWe are going to walk through several examples of using the\r\nGradientTape function built in the keras API. These\r\nsnippets are graciously taken from the keras tutorial in the references.\r\nThe Sandbox section will go into an example using some of\r\nthese ideas for an example use case.\r\n\r\n\r\n# A variable matrix x, initialized to ones\r\nx <- tf$ones(shape(2, 2))\r\nx\r\n\r\ntf.Tensor(\r\n[[1. 1.]\r\n [1. 1.]], shape=(2, 2), dtype=float32)\r\n\r\nwith(tf$GradientTape() %as% tape, {\r\n  \r\n  # Observe operations on x\r\n  tape$watch(x)\r\n  \r\n  y <- tf$reduce_sum(x) # y(x) = sum(x)\r\n  z <- tf$multiply(y, y) # z(y) = y(x)^2\r\n  # z = sum{x}^2\r\n})\r\n\r\n# Now z is a function of x on the tape\r\ndz_dx = tape$gradient(z, x) # dz = 2*y(x)dy_dx = 2*4*1 = 8; for all x [2x2]\r\ndz_dx\r\n\r\ntf.Tensor(\r\n[[8. 8.]\r\n [8. 8.]], shape=(2, 2), dtype=float32)\r\n\r\n\\[\r\n\\frac{dz}{dy} = 2*y(x)*\\frac{dy}{dx} =\r\n2*y(x)(\\frac{dy}{dx_j}\\sum_{i}{x_i})=2*4*1=8 \\:\\forall x_j\r\n\\]\r\n\r\n\r\nx <- tf$ones(tensorflow::shape(2,2))\r\n\r\nwith(tf$GradientTape() %as% tape, {\r\n  tape$watch(x)\r\n  y <- tf$reduce_sum(x) # y(x) = sum(x)\r\n  z <- tf$multiply(y, y) # z(y) = y(x)^2\r\n})\r\n\r\ndz_dy = tape$gradient(z, y) # dz_dy = 2*y(x) = 2*4 = 8; for all y [just 1]\r\ndz_dy\r\n\r\ntf.Tensor(8.0, shape=(), dtype=float32)\r\n\r\n\\[\r\n\\frac{dz}{dy} = 2*y(x) = 2\\sum{x}=2 * 4 = 8\r\n\\]\r\n\r\n\r\nx <- tf$constant(3)\r\n\r\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\r\n  tape$watch(x)\r\n  y <- x * x # y(x) = x^2\r\n  z <- y * y # z(y) = y(x)^2\r\n})\r\n\r\ndz_dx <- tape$gradient(z, x) # dz_dx = 2*y(x)*dy_dx = 2*x^2*2*x = 2*(3^2)*2*3\r\ndz_dx\r\n\r\ntf.Tensor(108.0, shape=(), dtype=float32)\r\n\r\n\\[\r\n\\frac{dz}{dx} = 2*y(x)*\\frac{dy}{dx} = 2*x^2*2*x = 2*(3^2)*2*3=108\r\n\\]\r\n\r\n\r\ndy_dx <- tape$gradient(y, x) # 6.0\r\ndy_dx\r\n\r\ntf.Tensor(6.0, shape=(), dtype=float32)\r\n\r\n\\[\r\n\\frac{dy}{dx} = 2x=2*3=6\r\n\\]\r\n\r\n\r\nf <- function(x, y) {\r\n  output <- 1\r\n  for (i in seq_len(y)) {\r\n    if (i > 2 & i <= 5)\r\n      output = tf$multiply(output, x)\r\n  }\r\n  output\r\n}\r\n\r\ngrad <- function(x, y) {\r\n  with(tf$GradientTape() %as% t, {\r\n    t$watch(x)\r\n    out <- f(x, y)\r\n  })\r\n  t$gradient(out, x)\r\n}\r\n\r\nx <- tf$constant(2)\r\ngrad(x, 6)\r\n\r\ntf.Tensor(12.0, shape=(), dtype=float32)\r\n\r\ngrad(x, 5)\r\n\r\ntf.Tensor(12.0, shape=(), dtype=float32)\r\n\r\ngrad(x, 4)\r\n\r\ntf.Tensor(4.0, shape=(), dtype=float32)\r\n\r\n\r\n\r\nx <- tf$Variable(1.0)  # Create a Tensorflow variable initialized to 1.0\r\n\r\n\r\nAdvanced Example\r\n\r\n\r\nx <- tf$Variable(1)  # Create a Tensorflow variable initialized to 1.0\r\nx\r\n\r\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\r\n\r\nwith(tf$GradientTape() %as% t, {\r\n  \r\n  with(tf$GradientTape() %as% t2, {\r\n    y <- x*x*x\r\n  })\r\n  \r\n  # Compute the gradient inside the 't' context manager\r\n  # which means the gradient computation is differentiable as well.\r\n  dy_dx <- t2$gradient(y, x)\r\n  \r\n})\r\n\r\nd2y_dx <- t$gradient(dy_dx, x)\r\n\r\n# Momentum of change\r\nd2y_dx\r\n\r\ntf.Tensor(6.0, shape=(), dtype=float32)\r\n\r\n# Rate of change\r\ndy_dx\r\n\r\ntf.Tensor(3.0, shape=(), dtype=float32)\r\n\r\nSandbox\r\nLet’s imagine for a second we had a really complex function, and we\r\nknew the type of variables that we were going to feed into it, but\r\ncalculating the gradient might be extremely difficult. Let’s say, we\r\nwanted to calculate a matrix of derivatives (or a tensor) of 10x10.\r\n\r\n\r\nX = tf$Variable(matrix(rnorm(100), nrow=10, ncol=10))\r\nX\r\n\r\n<tf.Variable 'Variable:0' shape=(10, 10) dtype=float64, numpy=\r\narray([[-1.73309684, -0.08322157,  0.54220219, -2.74995815,  1.51813321,\r\n         0.54784681,  0.15225096,  1.13755715,  0.38011554,  0.24894704],\r\n       [-2.41029604,  0.09891428, -0.35209566, -0.57222969, -0.53634412,\r\n         0.71086853, -0.7006611 , -1.0918934 , -0.518368  ,  0.47158885],\r\n       [ 1.95843168, -0.38393546, -0.17446505, -0.75403039,  0.52051699,\r\n        -0.59089743, -0.08355002,  1.84192727, -1.67958722, -0.03590348],\r\n       [ 0.64239372,  0.96948581,  1.18550871,  0.31853438, -0.01353529,\r\n        -1.50215088,  1.16325942,  2.53564633, -0.14070148,  0.80219709],\r\n       [-1.58774512, -0.36109838, -1.17602407,  2.56813718,  2.17165653,\r\n         0.31225676, -0.07452   , -2.58692503,  0.24899347, -1.11514666],\r\n       [ 0.16152645,  0.71037364,  0.00398511, -0.30448759,  0.2536505 ,\r\n         0.62954953, -1.05880117,  0.26962006,  0.05717959, -0.72830774],\r\n       [-0.39773135, -0.82909675, -0.25358638, -0.59576887, -1.21472404,\r\n         0.23462385,  0.64637421, -1.13077943,  0.12405466,  1.48119644],\r\n       [ 1.2187064 ,  0.9232517 , -1.30404339,  0.21532797, -1.01642429,\r\n        -0.57886662,  0.37538024, -0.50451042, -0.61835005,  1.41595008],\r\n       [-0.09359497, -1.87030098,  2.09480425, -0.5700717 , -0.71985003,\r\n        -1.42181604, -0.45254259,  0.04929961,  1.39375553, -0.82066383],\r\n       [ 1.55365989, -0.47480949, -0.08264721, -1.49560904,  0.95268159,\r\n        -0.08616004,  0.13853932,  0.45007053, -0.82517614, -0.07131134]])>\r\n\r\nThese are the initial values of the matrix, but we want this matrix\r\nto match a response matrix as closely as possible.\r\n\r\n\r\nR = matrix(rbinom(100, 4, 0.25), nrow = 10, ncol = 10)\r\nR\r\n\r\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\r\n [1,]    3    0    1    2    0    1    2    2    1     1\r\n [2,]    2    2    0    0    1    1    0    1    2     0\r\n [3,]    2    2    2    2    0    1    0    1    1     0\r\n [4,]    1    0    1    1    1    3    1    1    2     2\r\n [5,]    0    2    2    1    1    1    0    0    1     2\r\n [6,]    1    1    0    0    1    1    2    0    1     1\r\n [7,]    2    2    1    0    0    1    0    1    0     0\r\n [8,]    0    0    2    0    1    2    2    0    1     1\r\n [9,]    1    2    0    2    0    0    1    3    0     2\r\n[10,]    1    1    1    1    0    2    2    2    0     0\r\n\r\nSo the goal is to fill in the gaps, for say, recommender systems or\r\nsomething. Let’s build a loss function and go for it.\r\n\r\n\r\nX = tf$Variable(matrix(rnorm(100), nrow=10, ncol=10))\r\n\r\n\r\n# Consider a loss function L\r\nwith(tf$GradientTape(persistent = T) %as% t, {\r\n  t$watch(X)\r\n  \r\n  L <- tf$reduce_sum(\r\n          tf$square(\r\n            tf$subtract(R,X)\r\n          )\r\n      )\r\n  \r\n  \r\n  lambda = 0.2\r\n  reg <- lambda*tf$reduce_sum(tf$sqrt(tf$square(X)))\r\n  \r\n  L <- tf$add(L, reg)\r\n})\r\n  \r\n# For 100 steps, go descend\r\nfor(i in 1:1000){\r\n  \r\n  # learning rate\r\n  alpha = 0.1/i\r\n  \r\n  # gradient\r\n  dL_dX <- t$gradient(L, X)\r\n  \r\n  # derivative\r\n  X$assign_sub(alpha*dL_dX)\r\n}\r\n\r\nX\r\n\r\n<tf.Variable 'Variable:0' shape=(10, 10) dtype=float64, numpy=\r\narray([[ 5.02293261,  0.30973319,  0.99186468,  3.41146814,  0.31291584,\r\n         1.34556413,  3.44241753,  2.49430854,  1.22441611,  2.15290765],\r\n       [ 3.48498694,  3.89164399,  0.18661404,  0.32108998,  0.76755848,\r\n         2.19275776,  0.19756001,  1.83416693,  3.15623643,  0.60512031],\r\n       [ 3.20935864,  3.6486702 ,  3.15430177,  4.32273572, -0.65953477,\r\n         1.21454801, -0.25736091,  1.65204631,  1.75664996, -0.85084353],\r\n       [ 1.99710618, -0.58446189,  1.84274513,  2.23608402,  0.58083008,\r\n         4.89535097,  0.82008945,  1.96283637,  4.25830029,  2.60318369],\r\n       [ 0.4709209 ,  3.21099017,  3.75649051,  1.31540595,  0.24795211,\r\n         1.23813381,  0.29598465, -0.19368636,  1.05168273,  2.51631485],\r\n       [ 2.28950946,  1.78471934, -0.30089427,  0.22914342,  0.57931684,\r\n         1.71634635,  2.10374215, -0.60281344,  1.23714651,  0.53926587],\r\n       [ 3.21948567,  2.61709118,  1.73669846,  0.18467006, -0.82253731,\r\n         0.89098891, -0.31231532,  1.74370668, -0.19812191, -0.79820555],\r\n       [-0.33663196,  0.16062981,  3.35775843,  0.47308834,  1.01396689,\r\n         3.37754752,  2.47726712, -0.49023551,  1.11100834,  0.79643726],\r\n       [ 0.78515   ,  3.17968732, -0.6235009 ,  2.72424966, -0.22137149,\r\n        -0.7216236 ,  1.85288206,  2.83845993, -0.50285423,  3.79311637],\r\n       [ 1.310561  ,  1.96721478,  0.71569936,  1.20825038,  0.59959347,\r\n         3.1721326 ,  2.63536597,  3.76609742, -0.48195342,  1.0897197 ]])>\r\n\r\n\r\n\r\nR\r\n\r\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\r\n [1,]    3    0    1    2    0    1    2    2    1     1\r\n [2,]    2    2    0    0    1    1    0    1    2     0\r\n [3,]    2    2    2    2    0    1    0    1    1     0\r\n [4,]    1    0    1    1    1    3    1    1    2     2\r\n [5,]    0    2    2    1    1    1    0    0    1     2\r\n [6,]    1    1    0    0    1    1    2    0    1     1\r\n [7,]    2    2    1    0    0    1    0    1    0     0\r\n [8,]    0    0    2    0    1    2    2    0    1     1\r\n [9,]    1    2    0    2    0    0    1    3    0     2\r\n[10,]    1    1    1    1    0    2    2    2    0     0\r\n\r\n\r\n\r\n# Final loss\r\nE <- (as.matrix(X) - R)^2 %>%\r\n  as.data.frame %>%\r\n  dplyr::mutate(from=paste(\"V\",row_number(),sep=\"\")) %>% \r\n  tidyr::gather(., key = \"to\", value = \"error\", -from)\r\n\r\n\r\n\r\n\r\nggplot(data=E) + \r\n  geom_density(aes(x=error, fill=to)) +\r\n  facet_wrap(~to) + theme_classic() +\r\n  labs(fill=\"Destination Variable\") +\r\n  theme_classic() +\r\n  xlab(\"Error\") +\r\n  ylab(\"Probability\") +\r\n  ggtitle(\"Reconstruction Matrix Error\", subtitle = paste(\"Total Error: \", round(sum((as.matrix(X) - R)^2), 2), \" units\", sep=\"\" ))\r\n\r\n\r\n\r\n\r\n\r\nE %>% ggplot(., aes(x=to, y=from, fill=error)) + \r\n  geom_tile() + \r\n  scale_fill_gradient(low = \"lightgreen\", high = \"red\") +\r\n  labs(fill = \"Reconstruction Error\") +\r\n  theme_classic() +\r\n  ggtitle(\"Reconstruction Matrix Error\", subtitle = paste(\"Total Error: \", round(sum((as.matrix(X) - R)^2), 2), \" units\", sep=\"\" ))\r\n\r\n\r\n\r\nPretty remarkable. It went out and learned how to match our\r\nR data matrix to minimize squared error with a penalty for\r\nlarge noisy matrices.\r\nReferences\r\nhttps://tensorflow.rstudio.com/tutorials/advanced/customization/autodiff/\r\nhttps://keras.rstudio.com/articles/eager_guide.html\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-17-automatic-differentiation-with-keras-in-r/automatic-differentiation-with-keras-in-r_files/figure-html5/unnamed-chunk-14-1.png",
    "last_modified": "2022-07-17T20:28:56-04:00",
    "input_file": "automatic-differentiation-with-keras-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-16-recommender-systems-in-r/",
    "title": "Recommender Systems in R",
    "description": "In this post we will describe the sub-field of recommender systems, simulate some data, and solve the problem analytically using iterative regression.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-16",
    "categories": [
      "machine learning",
      "recommender systems",
      "regression",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nRecommender Systems as a\r\nField\r\nContent-based\r\nRecommendation\r\nCollaborative Filtering\r\n\r\nProblem Setup\r\nModel Parameters\r\nLoss Function\r\nIterative Regression\r\nSimulating Data\r\n\r\n\r\n(User, Movie, Rating) Tuples\r\nOrdinary Least Squares\r\nCreate the U matrix\r\nCreate the V matrix\r\nLearn the U matrix\r\nLearn the V matrix\r\n\r\nLearning From\r\nUsers and Item Embeddings\r\nRecommendations\r\n\r\n\r\nRecommender Systems as a\r\nField\r\nRecommender Systems as such is often referred to as a\r\nsub-field of machine learning. This is a class of learning problems that\r\nis entirely based on recommendation. As such, this sub-field has it’s\r\nown unique flavor of data. In short, there is some form of\r\nrecommendation as the objective of the problem. Whether it be a movie,\r\nsong, or some other object of potential interest to someone who has yet\r\nto actually experience it. The most popular paradigms to approaching the\r\nproblem are:\r\nContent-based Recommendation\r\nCollaborative Filtering\r\nContent-based Recommendation\r\nIn Content-based Recommendation the goal is acquire\r\nhighly intelligent information about the domain. By obtaining a\r\nfeature vector on the items, as users experience those\r\nitems we can train based on their preferences, then predict out new\r\nobservations using the standard classification paradigm. This is very\r\nconvenient, and powerful, as classification is time tested\r\nand can be produce great results. Some challenges this faces are that of\r\nsmall datasets, as users will have to experience things first, which may\r\ntake hours, days, or weeks in some cases.\r\nFeature Vector \\[\r\n\\phi(x) \\rightarrow \\mathbb{R^d}\r\n\\] The feature vector will take all of the items and produce a\r\nhighly articulate representation of it. How would you describe features\r\nof music? What about a movie? How about a trip to a state park? Though\r\ncomplex, many companies have had success at this. Pandora pays experts\r\nto accurately tag down a high dimensional space for this. As users\r\nexperience a movie they populate one of these d-vectors and\r\ntheir positive or negative experience to it will drill into the feature\r\nspace of preferences, making recommendation possible.\r\nData Set \\[\r\nD_{items}=\\{(\\phi(x_i), y_i)\\}_{i=1..n}\r\n\\] The data set becomes what item a user experiences as x and\r\ntheir response as y. Apply a simple transformation to make x tabular and\r\nboom … you’ve got a classification problem.\r\nCollaborative Filtering\r\nA far more common approach to solving the problem is that of\r\nCollaborative Filtering. The data set for this is an\r\naccumulation of all user experience. The data set has one\r\nform with two common notations.\r\nUser Rating Matrix \\[\r\nR_{i,j}\\in \\mathbb{R^{n,m}} \\: \\forall i \\in 1..n, \\: \\forall j \\in 1..m\r\n\\] The user rating matrix contains every user i and\r\nevery item j. Populated typically with a\r\n1 to 5 type of ordinal ranking. This matrix becomes HUGE\r\nvery quickly. Seldom can you actually load this into memory. We will\r\ndiscuss the tricks required in order to extract meaningful information\r\nthis later.\r\nDatabase \\[\r\nD=\\{(i,j,r)\\}_{i=1..l}\r\n\\]\r\nSince the above matrix will become so sparse, the database is often a\r\nbook keeping data structure to contain all of the non-zero elements of\r\nthis matrix. For some user i with some rating\r\nr for item j. We assume there are\r\nl of these. Clearly, we expect l to be\r\nsignificantly smaller than n*m which is the total number of\r\nentries in this matrix.\r\nProblem Setup\r\nWe would like to construct a matrix that will essentially fill in the\r\ngaps for us with a highly suggestive amount of potential rating, then\r\nargmax over any particular user and provide the top k as a\r\nrecommendation. We could construct a loss function for this, but it\r\nmight not be too useful. Instead, we will stand on the shoulders of\r\ngiants and set up our problem as follows:\r\nModel Parameters\r\nUser Latent Representation \\[\r\nU \\in \\mathbb{R^{n,k+1}}\r\n\\] Item Latent Representation \\[\r\nV \\in \\mathbb{R^{k+1,n}}\r\n\\]\r\nThe plus one represents the bias for each user and item respectively\r\n(e.g., how pessimistic I am, and in general how well the movie is liked\r\nby folks).\r\nU will contain information about each user lodged up in\r\nk+1 mystical components. V will contain information about\r\neach item lodged up in k+1 mystical components. These\r\nbecome a feature vector of sorts for users and items respectively.\r\nLoss Function\r\nThe loss function is as follows:\r\n\\[\r\nJ(U,V)=[\\frac{1}{2}\\sum_{(i,j,r)\\in D}{(u_i^Tv_j + b_{u_{i}} + b_{v_{j}}\r\n- r)^2}] + \\frac{\\lambda}{2}\\sum_{i=1..n}{||U_i||^2}+\r\n\\frac{\\lambda}{2}\\sum_{j=1..m}{||V_j||^2}\r\n\\]\r\nAt a glance this looks like typical linear regression. Don’t be\r\nfooled. Try taking the derivative and setting to zero you end up with a\r\nnasty set of equations in both directions. One clever trick to solve\r\nthis problem is what is known as iterative regression.\r\nIterative Regression\r\nIn iterative regression we will leverage the fact that we know how to\r\nsolve a simple regression problem analytically. If we hold\r\nV constant and solve for U using the typical\r\nordinary least squares model, we will arrive at cofficients for some\r\nuser. We can loop this for all users to obtain their estimates. Then, we\r\ncan hold these estimates U constant and go solve for\r\nV for all items. We will leverage the fact that we know the\r\nfollowing as true for any data set X, response column\r\nY and coefficient estimate theta from least\r\nsquares.\r\nOrdinary Least Squares Solution \\[\r\n\\theta^*=(X^TX)^{-1}X^TY\r\n\\]\r\nSimulating Data\r\nLet’s see if we can tackle this monster with some fake data.\r\n\r\n\r\n# Items\r\nM = 3\r\n\r\n# Users\r\nN = 10\r\n\r\n# Populate ratings matrix\r\nR = matrix(0, nrow=N, ncol=M)\r\n\r\n# Populating ratings matrix\r\nR[1,] <- c(2,0,5)\r\nR[2,] <- c(1,5,3)\r\nR[3,] <- c(0,0,5)\r\nR[4,] <- c(4,1,5)\r\nR[5,] <- c(0,0,2)\r\nR[6,] <- c(2,4,0)\r\nR[7,] <- c(0,3,5)\r\nR[8,] <- c(2,0,1)\r\nR[9,] <- c(0,1,2)\r\nR[10,] <- c(2,0,0)\r\n\r\n# for(n in 1:N){\r\n#   user_n_ratings = rbinom(M, 5, 0.5)\r\n#   \r\n#   R[n, ] = user_n_ratings\r\n#   \r\n#   # Put some holes in the data\r\n#   R[n, floor(runif(1, min=0, max=4))] = 0\r\n#   # R[n, floor(runif(1, min=0, max=4))] = 0\r\n# }\r\n\r\nR\r\n\r\n      [,1] [,2] [,3]\r\n [1,]    2    0    5\r\n [2,]    1    5    3\r\n [3,]    0    0    5\r\n [4,]    4    1    5\r\n [5,]    0    0    2\r\n [6,]    2    4    0\r\n [7,]    0    3    5\r\n [8,]    2    0    1\r\n [9,]    0    1    2\r\n[10,]    2    0    0\r\n\r\n(User, Movie, Rating) Tuples\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\n\r\n\r\nWe are all fully aware this matrix will never fit into memory, so\r\nlet’s make it a structure we will probably actually be working with in\r\nthe real world.\r\n\r\n\r\nD <- R %>% as.data.frame %>%\r\n  setNames(nm = 1:M) %>% \r\n  dplyr::mutate(user_id=row_number()) %>% \r\n  tidyr::gather(., key = \"item_id\", value =\"rating\", -user_id) %>% \r\n  dplyr::mutate(user_name = paste(\"User_\", user_id, sep=\"\"),\r\n                item_name = paste(\"Item_\", item_id, sep=\"\")) %>% \r\n  dplyr::filter(rating != 0)\r\nD\r\n\r\n   user_id item_id rating user_name item_name\r\n1        1       1      2    User_1    Item_1\r\n2        2       1      1    User_2    Item_1\r\n3        4       1      4    User_4    Item_1\r\n4        6       1      2    User_6    Item_1\r\n5        8       1      2    User_8    Item_1\r\n6       10       1      2   User_10    Item_1\r\n7        2       2      5    User_2    Item_2\r\n8        4       2      1    User_4    Item_2\r\n9        6       2      4    User_6    Item_2\r\n10       7       2      3    User_7    Item_2\r\n11       9       2      1    User_9    Item_2\r\n12       1       3      5    User_1    Item_3\r\n13       2       3      3    User_2    Item_3\r\n14       3       3      5    User_3    Item_3\r\n15       4       3      5    User_4    Item_3\r\n16       5       3      2    User_5    Item_3\r\n17       7       3      5    User_7    Item_3\r\n18       8       3      1    User_8    Item_3\r\n19       9       3      2    User_9    Item_3\r\n\r\nOrdinary Least Squares\r\nThat looks better. Now let’s see if we can do some iterative least\r\nsquares to solve this beast.\r\n\r\n\r\nlibrary(MASS)\r\n\r\nsolve.ols <- function(X,y){\r\n  theta = (ginv(t(X) %*% X) %*% t(X)) %*% y\r\n}\r\n\r\nX = matrix(rep(rnorm(2), 10), nrow = 10)\r\ny = matrix(rnorm(10), ncol=1)\r\ntheta = solve.ols(X,y)\r\ntheta\r\n\r\n             [,1]\r\n[1,] 0.0007774986\r\n[2,] 0.0007774986\r\n\r\nCreate the U matrix\r\nNeat-o, we can solve a single OLS problem. Let’s define our\r\nparameters.\r\n\r\n\r\n# Latent dimensions\r\nK = 3\r\n\r\n# User matrix\r\nU = matrix(rnorm(N*K), nrow=N, ncol=K)\r\n\r\n# Don't forget to append the bias vector for the items\r\nU = cbind(U, 1)\r\n\r\n# # Don't forget to append the bias vector\r\nU = cbind(U, rnorm(N))\r\n\r\n\r\nU\r\n\r\n            [,1]        [,2]        [,3] [,4]       [,5]\r\n [1,] -1.3270366 -1.64229706 -0.74543293    1 -0.3785241\r\n [2,]  0.7363001  0.51817104  1.68335141    1 -0.7233356\r\n [3,]  0.8175293 -0.06825585 -0.25251234    1 -1.3509502\r\n [4,]  0.3253387  1.31086086  0.48109986    1 -0.8145052\r\n [5,]  0.3341991 -1.40574984 -0.97059620    1  0.4811544\r\n [6,]  0.4674589 -0.82871156  1.51013370    1 -0.4384462\r\n [7,]  0.4585841  0.90775656 -0.18085370    1  1.5732527\r\n [8,] -0.5184098 -0.87080546  0.00712267    1  0.1115782\r\n [9,] -0.0118098  1.33809955  2.14143620    1 -0.3801498\r\n[10,]  0.7271083  0.71103304  1.23705950    1  0.2636276\r\n\r\nCreate the V matrix\r\n\r\n\r\n# Item matrix\r\nV = matrix(rnorm(K*M), nrow=K, ncol=M)\r\n\r\n# Don't forget to append the bias vector\r\nV = rbind(V, rnorm(M))\r\n\r\n# Don't forget to append the bias vector for the users\r\nV = rbind(V, 1)\r\n\r\nV\r\n\r\n           [,1]       [,2]       [,3]\r\n[1,] 0.22583268 -0.7298176 -1.5101003\r\n[2,] 0.10789959 -1.0803082  0.6284866\r\n[3,] 0.04698169 -0.5567730 -0.4943936\r\n[4,] 1.48580304 -0.1077601 -0.7066269\r\n[5,] 1.00000000  1.0000000  1.0000000\r\n\r\nI believe by alternating those columns of ones like that we will\r\nbundle up the bias terms inside both matrices. So then when we get our\r\ndot product from the problem, it will hit a bias term for the user and a\r\nbias term for the item with ones of alternating locations. Just remember\r\nthey are at the end. :)\r\nExcellent so far, now we need to perform loops in succession to solve\r\nthe following sub-problem.\r\n\\[\r\nU_i = \\frac{1}{2}\\sum_{(j | (i,j,r) \\in D)}{(u_i^Tv_j - r)^2}\r\n\\] Once we solve the above problem for all users i,\r\nwe will alternative over and solve the inverse problem of all items.\r\nThis is as follows.\r\n\\[\r\nV_j = \\frac{1}{2}\\sum_{(i | (i,j,r) \\in D)}{(u_i^Tv_j - r)^2}\r\n\\] Same exact setup, this just alternates which role will be data\r\nand which will be parameters. So for the user problem, the items will\r\nserve as data X and u will be the theta. On\r\nthe item side the users will serve as data X and items will\r\nbe the theta v. Let’s give it a shot and cause some\r\ntrouble.\r\nLearn the U matrix\r\n\r\n\r\n# Solve for U\r\nn = unique(D$user_id)\r\nfor(uid in as.numeric(n)){\r\n  \r\n  # The user id\r\n  # cat(uid, \"\\n\")\r\n  \r\n  # Get all the items now\r\n  all_items <- D %>% dplyr::filter(user_id ==  uid) %>% dplyr::pull(item_id)\r\n  m <- length(all_items)\r\n  \r\n  tmp_X <- matrix(0, nrow=m, ncol=K+2)\r\n  tmp_y <- matrix(0, nrow=m, ncol=1)\r\n  \r\n  idx = 1\r\n  for(iid in as.numeric(all_items)){\r\n    \r\n    # The item id\r\n    # cat(\" \", iid, \" \")\r\n    \r\n    # Store away the item vector for this record\r\n    tmp_X[idx, ] = V[,iid]\r\n    \r\n    # Store away the response element for this record\r\n    r = D %>% dplyr::filter(user_id == uid, item_id == iid) %>% dplyr::pull(rating)\r\n    tmp_y[idx] = r\r\n    \r\n    # Iterate to the next item\r\n    idx = idx + 1\r\n    \r\n  }\r\n  # cat(\"\\n\")\r\n  # break\r\n  \r\n  U[uid,] = solve.ols(tmp_X, tmp_y)\r\n}\r\n\r\nU\r\n\r\n            [,1]        [,2]        [,3]       [,4]      [,5]\r\n [1,] -1.6293749  0.82692416 -0.55330880  0.2547490 1.9262298\r\n [2,] -1.2218737 -1.74068943 -0.91970519 -0.1309888 1.7015915\r\n [3,] -1.7085888  0.71109522 -0.55937700 -0.7995063 1.1314406\r\n [4,] -1.4344886  2.12956316 -0.23031705  1.2422230 2.2592973\r\n [5,] -0.6834355  0.28443809 -0.22375080 -0.3198025 0.4525762\r\n [6,] -0.8218100 -1.30872541 -0.67801104  0.4704885 1.6596034\r\n [7,] -1.7616312  0.13835765 -0.71333716 -0.7238713 1.3886281\r\n [8,] -0.2728864  0.24286636 -0.10660255  0.7561736 0.9169049\r\n [9,] -0.6939606  0.17079081 -0.25430084 -0.3047944 0.5036095\r\n[10,]  0.1380201  0.06594401  0.02871337  0.9080647 0.6111609\r\n\r\nLearn the V matrix\r\nWe got the U matrix after performing i=1..n\r\nregressions and building up our vectors from the item data. Now let’s do\r\nit for the ratings matrix.\r\n\r\n\r\n# U[,M+1] = 1\r\n\r\n# Solve for V\r\nm = unique(D$item_id)\r\nfor(iid in as.numeric(m)){\r\n  \r\n  # The item id\r\n  # cat(iid, \"\\n\")\r\n  \r\n  # Get all the users now\r\n  all_users <- D %>% dplyr::filter(item_id ==  iid) %>% dplyr::pull(user_id)\r\n  n <- length(all_users)\r\n  \r\n  tmp_X <- matrix(0, nrow=n, ncol=K+2)\r\n  tmp_y <- matrix(0, nrow=n, ncol=1)\r\n  \r\n  idx = 1\r\n  for(uid in as.numeric(all_users)){\r\n    \r\n    # The user id\r\n    # cat(\" \", uid, \" \")\r\n    \r\n    # Store away the user vector for this record\r\n    tmp_X[idx, ] = U[uid,]\r\n    \r\n    # Store away the response element for this record, same as last time\r\n    r = D %>% dplyr::filter(user_id == uid, item_id == iid) %>% dplyr::pull(rating)\r\n    tmp_y[idx] = r\r\n    \r\n    # Iterate to the next item\r\n    idx = idx + 1\r\n    \r\n  }\r\n\r\n  V[,iid] = solve.ols(tmp_X, tmp_y)\r\n}\r\n\r\nV\r\n\r\n           [,1]       [,2]       [,3]\r\n[1,] 0.22583268 -0.7298176 -1.5101003\r\n[2,] 0.10789959 -1.0803082  0.6284866\r\n[3,] 0.04698169 -0.5567730 -0.4943936\r\n[4,] 1.48580304 -0.1077601 -0.7066269\r\n[5,] 1.00000000  1.0000000  1.0000000\r\n\r\n\r\n\r\nt(V)\r\n\r\n           [,1]       [,2]        [,3]       [,4] [,5]\r\n[1,]  0.2258327  0.1078996  0.04698169  1.4858030    1\r\n[2,] -0.7298176 -1.0803082 -0.55677298 -0.1077601    1\r\n[3,] -1.5101003  0.6284866 -0.49439360 -0.7066269    1\r\n\r\nU\r\n\r\n            [,1]        [,2]        [,3]       [,4]      [,5]\r\n [1,] -1.6293749  0.82692416 -0.55330880  0.2547490 1.9262298\r\n [2,] -1.2218737 -1.74068943 -0.91970519 -0.1309888 1.7015915\r\n [3,] -1.7085888  0.71109522 -0.55937700 -0.7995063 1.1314406\r\n [4,] -1.4344886  2.12956316 -0.23031705  1.2422230 2.2592973\r\n [5,] -0.6834355  0.28443809 -0.22375080 -0.3198025 0.4525762\r\n [6,] -0.8218100 -1.30872541 -0.67801104  0.4704885 1.6596034\r\n [7,] -1.7616312  0.13835765 -0.71333716 -0.7238713 1.3886281\r\n [8,] -0.2728864  0.24286636 -0.10660255  0.7561736 0.9169049\r\n [9,] -0.6939606  0.17079081 -0.25430084 -0.3047944 0.5036095\r\n[10,]  0.1380201  0.06594401  0.02871337  0.9080647 0.6111609\r\n\r\nLearning From Users and\r\nItem Embeddings\r\nPretty sweet. Now we have two matrices U and\r\nV. U for our users. V for our\r\nitems. But they are SUBSTANTIALLY smaller than our initial matrix\r\n\r\n\r\nU %>% dim\r\n\r\n[1] 10  5\r\n\r\nV %>% dim\r\n\r\n[1] 5 3\r\n\r\nR %>% dim\r\n\r\n[1] 10  3\r\n\r\ncat(\"\\n\")\r\n\r\n\r\nnrow(U) * ncol(U)\r\n\r\n[1] 50\r\n\r\nnrow(V) * ncol(V)\r\n\r\n[1] 15\r\n\r\nnrow(R) * ncol(R)\r\n\r\n[1] 30\r\n\r\nWe didn’t really gas the problem, but we literally cut it in half.\r\nThat is 550 entries of data to learn instead of 1000. Imagine if we had\r\nTONS of items (which most do) and TONS of users (which most do). There\r\nit is folks. Now let’s see what we’ve learned\r\nFor users the bias column was in the last slot. Let’s see what each\r\nusers bias rating is.\r\n\r\n\r\nlibrary(ggplot2)\r\n\r\n\r\nWe want to be careful about interpreting that intercept necessarily\r\nas the basic, because it is ultimately created as a projection with many\r\nof things being factored in. Namely V1..V3. With each\r\nvariable it collectively minimizes things with that offset.\r\n\r\n\r\nU_df <- U %>% \r\n  as.data.frame %>%\r\n  dplyr::rename(user_bias = V4) %>%\r\n  dplyr::mutate(user = paste(\"User_\", row_number(), sep=\"\"),\r\n                user_actual_mean_rating = apply(R, 1, mean)) \r\n\r\nrow.names(U_df) <- U_df$user\r\nU_df\r\n\r\n                V1          V2          V3  user_bias        V5\r\nUser_1  -1.6293749  0.82692416 -0.55330880  0.2547490 1.9262298\r\nUser_2  -1.2218737 -1.74068943 -0.91970519 -0.1309888 1.7015915\r\nUser_3  -1.7085888  0.71109522 -0.55937700 -0.7995063 1.1314406\r\nUser_4  -1.4344886  2.12956316 -0.23031705  1.2422230 2.2592973\r\nUser_5  -0.6834355  0.28443809 -0.22375080 -0.3198025 0.4525762\r\nUser_6  -0.8218100 -1.30872541 -0.67801104  0.4704885 1.6596034\r\nUser_7  -1.7616312  0.13835765 -0.71333716 -0.7238713 1.3886281\r\nUser_8  -0.2728864  0.24286636 -0.10660255  0.7561736 0.9169049\r\nUser_9  -0.6939606  0.17079081 -0.25430084 -0.3047944 0.5036095\r\nUser_10  0.1380201  0.06594401  0.02871337  0.9080647 0.6111609\r\n           user user_actual_mean_rating\r\nUser_1   User_1               2.3333333\r\nUser_2   User_2               3.0000000\r\nUser_3   User_3               1.6666667\r\nUser_4   User_4               3.3333333\r\nUser_5   User_5               0.6666667\r\nUser_6   User_6               2.0000000\r\nUser_7   User_7               2.6666667\r\nUser_8   User_8               1.0000000\r\nUser_9   User_9               1.0000000\r\nUser_10 User_10               0.6666667\r\n\r\n\r\n\r\nggplot(data = U_df, aes(label = user_bias)) +\r\n  geom_col(aes(x=user, y=user_actual_mean_rating, fill = \"Average Rating\"), alpha = 0.5) +\r\n  geom_col(aes(x=user, y=user_bias, fill=\"User Bias\"), alpha = 0.5) +\r\n  ggtitle(\"Latent User Bias vs. Average Rating\") +\r\n  labs(fill = \"User Centrality\") +\r\n  theme_bw() +\r\n  theme(axis.text.x = element_text(angle = 90)) +\r\n  xlab(\"User\") +\r\n  ylab(\"Rating\")\r\n\r\n\r\n\r\nSince the iterative regression trains the users first, we will lose\r\nbias information for the\r\nRecommendations\r\nWe can now construct our rating matrix at once, or piece by piece\r\nsince we have U and V.\r\n\r\n\r\nR_reconstructed <- U %*% V\r\n\r\nR_reconstructed\r\n\r\n             [,1]      [,2]      [,3]\r\n [1,]  2.00000000 2.5026590  5.000000\r\n [2,]  1.00000000 5.0000000  3.000000\r\n [3,] -0.39187712 2.0077978  5.000000\r\n [4,]  4.00000000 1.0000000  5.000000\r\n [5,] -0.15675085 0.8031191  2.000000\r\n [6,]  2.00000000 4.0000000  2.080847\r\n [7,] -0.10332101 3.0000000  5.000000\r\n [8,]  2.00000000 0.8315597  1.000000\r\n [9,] -0.09949318 1.0000000  2.000000\r\n[10,]  2.00000000 0.3253515 -0.211677\r\n\r\nR\r\n\r\n      [,1] [,2] [,3]\r\n [1,]    2    0    5\r\n [2,]    1    5    3\r\n [3,]    0    0    5\r\n [4,]    4    1    5\r\n [5,]    0    0    2\r\n [6,]    2    4    0\r\n [7,]    0    3    5\r\n [8,]    2    0    1\r\n [9,]    0    1    2\r\n[10,]    2    0    0\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-16-recommender-systems-in-r/recommender-systems-in-r_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2022-07-16T16:54:12-04:00",
    "input_file": "recommender-systems-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-15-regression-with-keras-in-r/",
    "title": "Regression with Keras in R",
    "description": "In this post we will explore how to do deep regression in R using Keras.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-15",
    "categories": [
      "deep learning",
      "machine learning",
      "regression",
      "keras",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nBoston Housing Prices\r\nVisualize Correlation\r\nVisualize Serious\r\nCorrelation\r\nStandardize the data\r\nModel\r\nEvaluate\r\nVisualize\r\n\r\nDropping Correlated\r\nFeatures\r\nInferentials\r\n\r\nReferences\r\n\r\nBoston Housing Prices\r\nThe objective of this post is to walk through doing regression on\r\ntabular data in keras. There are an abundance of tutorials to explore\r\nthe mechanics of this, our goal is to open the mind with examples.\r\n\r\n\r\nlibrary(tfdatasets)\r\nlibrary(keras)\r\nlibrary(tidyr)\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\n\r\n\r\nboston_housing <- dataset_boston_housing()\r\ntrain_data <- boston_housing$train$x\r\ntrain_labels <- boston_housing$train$y\r\n\r\ntest_data <- boston_housing$test$x\r\ntest_labels <- boston_housing$test$y\r\n\r\npaste0(\"Training entries: \", length(train_data), \", labels: \", length(train_labels))\r\n\r\n[1] \"Training entries: 5252, labels: 404\"\r\n\r\n\r\n\r\ncolumn_names <- c('CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \r\n                  'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT')\r\n\r\ntrain_df <- train_data %>% \r\n  dplyr::as_tibble(.name_repair = \"minimal\") %>% \r\n  stats::setNames(., column_names) %>% \r\n  dplyr::mutate(label = train_labels)\r\ntrain_df\r\n\r\n# A tibble: 404 x 14\r\n     CRIM    ZN INDUS  CHAS   NOX    RM   AGE   DIS   RAD   TAX\r\n    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\r\n 1 1.23     0    8.14     0 0.538  6.14  91.7  3.98     4   307\r\n 2 0.0218  82.5  2.03     0 0.415  7.61  15.7  6.27     2   348\r\n 3 4.90     0   18.1      0 0.631  4.97 100    1.33    24   666\r\n 4 0.0396   0    5.19     0 0.515  6.04  34.5  5.99     5   224\r\n 5 3.69     0   18.1      0 0.713  6.38  88.4  2.57    24   666\r\n 6 0.284    0    7.38     0 0.493  5.71  74.3  4.72     5   287\r\n 7 9.19     0   18.1      0 0.7    5.54 100    1.58    24   666\r\n 8 4.10     0   19.6      0 0.871  5.47 100    1.41     5   403\r\n 9 2.16     0   19.6      0 0.871  5.63 100    1.52     5   403\r\n10 1.63     0   21.9      0 0.624  5.02 100    1.44     4   437\r\n# ... with 394 more rows, and 4 more variables: PTRATIO <dbl>,\r\n#   B <dbl>, LSTAT <dbl>, label <dbl>\r\n\r\ntest_df <- test_data %>% \r\n  dplyr::as_tibble(.name_repair = \"minimal\") %>% \r\n  stats::setNames(., column_names) %>% \r\n  dplyr::mutate(label = test_labels)\r\ntest_df\r\n\r\n# A tibble: 102 x 14\r\n      CRIM    ZN INDUS  CHAS   NOX    RM   AGE   DIS   RAD   TAX\r\n     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\r\n 1 18.1        0 18.1      0 0.679  6.43 100    1.83    24   666\r\n 2  0.123      0 10.0      0 0.547  5.91  92.9  2.35     6   432\r\n 3  0.0550     0  5.19     0 0.515  5.98  45.4  4.81     5   224\r\n 4  1.27       0 19.6      1 0.605  6.25  92.6  1.80     5   403\r\n 5  0.0715     0  4.49     0 0.449  6.12  56.8  3.75     3   247\r\n 6  0.280      0  9.69     0 0.585  5.93  42.6  2.38     6   391\r\n 7  0.0305    55  3.78     0 0.484  6.87  28.1  6.47     5   370\r\n 8  0.0355    25  4.86     0 0.426  6.17  46.7  5.40     4   281\r\n 9  0.0930     0 25.6      0 0.581  5.96  92.9  2.09     2   188\r\n10  3.57       0 18.1      0 0.58   6.44  75    2.90    24   666\r\n# ... with 92 more rows, and 4 more variables: PTRATIO <dbl>,\r\n#   B <dbl>, LSTAT <dbl>, label <dbl>\r\n\r\n\r\n\r\ntrain_labels[1:10]\r\n\r\n [1] 15.2 42.3 50.0 21.1 17.7 18.5 11.3 15.6 15.6 14.4\r\n\r\nVisualize Correlation\r\n\r\n\r\n# See what features are correlated\r\n# Center and normalize\r\nXtrain <- train_df %>% select(-label)\r\nXs <- scale(Xtrain, center = TRUE, scale = TRUE)\r\n\r\n\r\n# Tidy the data\r\nC <- cor(Xs) %>% as.data.frame\r\nC$from <- row.names(C)\r\nC <- C %>% tidyr::gather(., key = \"to\", value = \"correlation\", -from)\r\n\r\n# Present a heatmap\r\nplt <- ggplot(data = C) + \r\n  geom_tile(aes(x=to, y=from, fill=correlation)) + \r\n  scale_fill_gradient(low = \"red\", high = \"green\") +\r\n  ggtitle(\"Correlation between Boston House features\") +\r\n  theme(axis.text.x = element_text(angle = 90)) \r\n\r\nplt\r\n\r\n\r\n\r\nA few areas of concern are those with greater than about 0.5\r\ncorrelation. This could mean they are excessive information and\r\nredundant. Let’s look at those who have a magnitude greater than between\r\n0.25 and 0.5.\r\nVisualize Serious\r\nCorrelation\r\nBelow is an analysis of some of the correlations. I performed a\r\ndetailed analysis on a pre-centered correlation matrix, which I will\r\nleave below for kicks. But ultimately, high correlated features\r\nshould be considered for removal from the model.\r\n\r\n\r\n# Present a heatmap\r\nplt <- ggplot(data = C %>% filter(abs(correlation) > 0.7)) + \r\n  geom_tile(aes(x=to, y=from, fill=correlation)) + \r\n  scale_fill_gradient(low = \"red\", high = \"green\") +\r\n  ggtitle(\"Correlations between Boston House features\") +\r\n  theme_bw() + \r\n  theme(axis.text.x = element_text(angle = 90))\r\n\r\nplt\r\n\r\n\r\n\r\nThere are several columns that could be dropped due to their high\r\ncorrelation with other variables. Let’s keep track of these for a hot\r\nminute. We will drop these and remodel later to see if this helps\r\nany.\r\n\r\n\r\nEXCESSIVE_FEATURES <- c(\"DIS\", \"NOX\", \"TAX\")\r\nEXCESSIVE_FEATURES\r\n\r\n[1] \"DIS\" \"NOX\" \"TAX\"\r\n\r\nStandardize the data\r\n\r\n\r\n# Center and normalize\r\nXtrain <- train_df %>% select(-label)\r\nXs_train <- scale(Xtrain, center = T, scale = T)\r\ntrain_df_scaled <- cbind(Xs_train, train_df %>% select(label))\r\n\r\nXtest <- test_df %>% select(-label)\r\nXs_test <- scale(Xtest, center = T, scale = T)\r\ntest_df_scaled <- cbind(Xs_test, test_df %>% select(label))\r\n\r\n\r\nModel\r\nSome cheat codes are required to make keras work with\r\ndataframes. First, you have to make a multi-head model. Define an\r\ninput with your dimensionality of the data frame. Second,\r\ncreate your other layers to pass it through. Merge them using the\r\nkeras_model function. R is great, but can be very painful\r\nat times. This was not pleasant. Further, only feed in\r\nmatrices. Don’t even fight with throwing in a data frame. This\r\nAPI isn’t smart like that.\r\n\r\n\r\n# Input head\r\ninput <- keras::layer_input(shape = c(13))\r\n\r\n# Output head\r\noutput <- input %>% \r\n  keras::layer_batch_normalization() %>%  \r\n  keras::layer_dense(units = 1)\r\n\r\n# Join input and output heads\r\nmodel <- keras::keras_model(input, output)\r\n\r\nmodel %>% keras::compile(\r\n  loss = \"mse\",\r\n  optimizer = \"adam\",\r\n  metrics = list(\"mean_absolute_error\")\r\n)\r\n\r\nhistory = model %>% keras::fit(\r\n  x = train_df %>% select(-label) %>% as.matrix,\r\n  y = train_df$label %>% as.matrix,\r\n  validation_split = 0.1,\r\n  epochs = 100,\r\n  verbose=0\r\n)\r\n\r\n\r\nplot(history)\r\n\r\n\r\n\r\nEvaluate\r\n\r\n\r\nevaluate(model,\r\n         x = test_df %>% select(-label) %>% as.matrix, \r\n         y = test_df$label %>% as.matrix)\r\n\r\n               loss mean_absolute_error \r\n          29.055779            3.862492 \r\n\r\nVisualize\r\n\r\n\r\nypred <- predict(model, \r\n                x = test_df %>% select(-label) %>% as.matrix)\r\n\r\ntmp <- data.frame(id = 1:length(ypred),\r\n                  prediction = ypred,\r\n                  target = test_df$label)\r\ntmp <- tmp %>% dplyr::mutate(mse = sqrt((tmp$prediction - tmp$target)**2),\r\n                             error = tmp$prediction - tmp$target)\r\n\r\nplt <- ggplot( data = tmp) + \r\n  geom_point(aes(x=id, y=prediction, color=\"prediction\")) +\r\n  geom_point(aes(x=id, y=target, color=\"target\")) +\r\n  theme_classic()\r\n\r\nggplot(tmp) + \r\n  geom_histogram(aes(x=mse, fill=\"mse\"), alpha = 0.4) +\r\n  geom_histogram(aes(x=error, fill=\"error\"), alpha=0.4) + \r\n  labs(fill = \"Error Distribution\") +\r\n  ggtitle(\"Test Error\") +\r\n  xlab(\"Error\") +\r\n  ylab(\"Occurances\") +\r\n  theme_light()\r\n\r\n\r\nshapiro.test(tmp$error) # This is probably normal. We can do inferential statistics on the population!\r\n\r\n\r\n    Shapiro-Wilk normality test\r\n\r\ndata:  tmp$error\r\nW = 0.92301, p-value = 1.706e-05\r\n\r\nDropping Correlated Features\r\nIn efforts to save time and effort. The experiment to drop\r\ncorrelation variables EXCESSIVE FEATURES turned out to\r\nactually make the model worse. This is probably due to the fact that\r\nthey store valuable information in this relatively small data set.\r\nInferentials\r\nSince we built our regression model with just one layer, the weights\r\nin that layer are in fact our parameters\r\n\r\n\r\noutput_layer <- model$layers[[3]]\r\n\r\nweights <- output_layer$get_weights()\r\ntheta <- weights[[1]]\r\nbias <- weights[[2]]\r\ntheta\r\n\r\n           [,1]\r\n [1,] -1.101465\r\n [2,] -1.401569\r\n [3,]  1.574454\r\n [4,] -1.463762\r\n [5,]  1.619609\r\n [6,]  1.167189\r\n [7,] -1.666827\r\n [8,]  1.558088\r\n [9,] -1.257356\r\n[10,] -1.094540\r\n[11,]  1.392148\r\n[12,]  1.659781\r\n[13,] -1.575807\r\n\r\nest_df <- data.frame(variable = c(names(train_df %>% select(-label)), \"intercept\"),\r\n                     estimate = c(theta, bias)\r\n          )\r\n\r\n\r\nggplot(data = est_df) + \r\n  geom_col(aes(x=variable, y=estimate, fill=variable)) + \r\n  coord_flip() +\r\n  labs(fill = \"Variable Estimate\") +\r\n  ggtitle(\"Boston Housing Factors\") +\r\n  ylab(\"Coefficient Estimate\") +\r\n  xlab(\"Variable\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nReferences\r\nhttps://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_regression/\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-15-regression-with-keras-in-r/regression-with-keras-in-r_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2022-07-17T20:32:29-04:00",
    "input_file": "regression-with-keras-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-16-text-classification-via-keras-in-r/",
    "title": "Text Classification with Keras in R",
    "description": "In this post we will walk through text classification using keras in R.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-15",
    "categories": [
      "deep learning",
      "machine learning",
      "classification",
      "natural language processing",
      "keras",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nMovie Reviews\r\nData Landscape\r\nSentence Length\r\nVisualization\r\nVectorizer\r\nAdapt\r\nSample Sentence\r\n\r\nBuild the Model\r\nLoss Function and\r\nOptimizer\r\nTrain the model\r\nEvaluate\r\nPlot Metrics\r\nWhat’s in that layer?\r\n\r\nReferences\r\n\r\nMovie Reviews\r\nThis problem is a binary classification problem. So we have a\r\ntext-based data set and a binary response variable.\r\n\r\n\r\nlibrary(keras)\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\nlibrary(purrr)\r\n\r\n\r\n\r\n\r\npath <- \"C:/Users/blake/Desktop/blog_data/movie_review.csv\"\r\n\r\ndf <- readr::read_csv(path)\r\ndf %>% dplyr::glimpse()\r\n\r\nRows: 64,720\r\nColumns: 6\r\n$ fold_id <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\r\n$ cv_tag  <chr> \"cv000\", \"cv000\", \"cv000\", \"cv000\", \"cv000\", \"cv000\"~\r\n$ html_id <chr> \"29590\", \"29590\", \"29590\", \"29590\", \"29590\", \"29590\"~\r\n$ sent_id <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15~\r\n$ text    <chr> \"films adapted from comic books have had plenty of s~\r\n$ tag     <chr> \"pos\", \"pos\", \"pos\", \"pos\", \"pos\", \"pos\", \"pos\", \"po~\r\n\r\nData Landscape\r\nThe data appears to have over 60,000 rows, each containing a movie\r\nreview in the text column with a response in the\r\ntag column for pos or neg (of the\r\nreview).\r\n\r\n\r\n# The spread on the response looks pretty even\r\ndf %>% dplyr::count(tag)\r\n\r\n# A tibble: 2 x 2\r\n  tag       n\r\n  <chr> <int>\r\n1 neg   31783\r\n2 pos   32937\r\n\r\n\r\n\r\ndf$text[1]\r\n\r\n[1] \"films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before .\"\r\n\r\n\r\n\r\ntraining_id <- sample(nrow(df), size = nrow(df)*.8)\r\ntraining <- df[training_id,]\r\ntesting <- df[-training_id,]\r\n\r\n\r\nIt might be useful to know the number of words in each review.\r\nSentence Length\r\nVisualization\r\n\r\n\r\ndf$text %>% \r\n  strsplit(., \" \") %>% \r\n  sapply(., length) %>% \r\n  head(10)\r\n\r\n [1] 52 36 27 30 13 27 52 16 37 36\r\n\r\n  # hist(main = \"Distribution of Words Per Sentiment\", xlab = \"Number of Words\")\r\n\r\n\r\nWe will make the text a tensor. The most common 10,000\r\nwords will be specified by an integer. Every sequence will be\r\nrepresented by a sequence of integers.\r\nVectorizer\r\n\r\n\r\n# total words to account ids for, otherwise drop - this becomes the [UNK] token to represent unknown characters.\r\nnum_words <- 10000 \r\n\r\n# per sequence, how many words to keep, or pad to\r\nmax_length <- 50 \r\n\r\n# Fit the parameters into the vectorizer\r\ntext_vectorization <- keras::layer_text_vectorization(\r\n  max_tokens = num_words,\r\n  output_sequence_length = max_length\r\n)\r\n\r\n\r\nAdapt\r\nNext we will adapt the Text Vectorization. Once we\r\nadapt the layer will learn unique words in our dataset and\r\nassign integer values for each.\r\n\r\n\r\n# Now it knows the vocab of 10,000\r\n# Each will become a size 50 vector, of shape Nx50 now.\r\n\r\ntext_vectorization %>% \r\n  adapt(df$text)\r\n\r\n\r\n\r\n\r\n# Notice they are all tokeninzed and lower cased nice and neat.\r\nget_vocabulary(text_vectorization)[1:100]\r\n\r\n  [1] \"\"           \"[UNK]\"      \"the\"        \"a\"          \"and\"       \r\n  [6] \"of\"         \"to\"         \"is\"         \"in\"         \"that\"      \r\n [11] \"it\"         \"as\"         \"with\"       \"for\"        \"his\"       \r\n [16] \"this\"       \"film\"       \"but\"        \"he\"         \"i\"         \r\n [21] \"on\"         \"are\"        \"by\"         \"be\"         \"its\"       \r\n [26] \"an\"         \"not\"        \"one\"        \"movie\"      \"who\"       \r\n [31] \"from\"       \"at\"         \"was\"        \"have\"       \"has\"       \r\n [36] \"her\"        \"you\"        \"they\"       \"all\"        \"so\"        \r\n [41] \"like\"       \"about\"      \"out\"        \"more\"       \"when\"      \r\n [46] \"which\"      \"their\"      \"up\"         \"or\"         \"what\"      \r\n [51] \"some\"       \"just\"       \"if\"         \"there\"      \"she\"       \r\n [56] \"him\"        \"into\"       \"even\"       \"only\"       \"than\"      \r\n [61] \"no\"         \"we\"         \"good\"       \"most\"       \"time\"      \r\n [66] \"can\"        \"will\"       \"story\"      \"films\"      \"been\"      \r\n [71] \"would\"      \"much\"       \"also\"       \"characters\" \"other\"     \r\n [76] \"get\"        \"character\"  \"do\"         \"them\"       \"very\"      \r\n [81] \"two\"        \"first\"      \"after\"      \"see\"        \"well\"      \r\n [86] \"because\"    \"way\"        \"make\"       \"any\"        \"does\"      \r\n [91] \"really\"     \"had\"        \"too\"        \"while\"      \"how\"       \r\n [96] \"little\"     \"life\"       \"where\"      \"were\"       \"plot\"      \r\n\r\nSample Sentence\r\nLets convert one sample. Since tensorflow will only accept matrices,\r\nwe will transform that sample into a matrix and pass it in. Out comes\r\nthe 1x50 vector transformation.\r\n\r\n\r\n# Lets convert one sample\r\nx <- matrix(df$text[1], ncol=1)\r\n\r\n# Since tensorflow will ONLY accept matrices, we have to make it a fat 1x1 matrix and throw it in. Out comes a 1x50\r\ntext_vectorization(x)\r\n\r\ntf.Tensor(\r\n[[  68 2835   30  359 1662   33   91 1056    5  632  631  321   41 7803\r\n   709 4865 1767   48 7600 1337  398 5161   48    2    1 1808 1800  148\r\n    17  140  109   90   69    3  359  408   40   30  503  142    0    0\r\n     0    0    0    0    0    0    0    0]], shape=(1, 50), dtype=int64)\r\n\r\n# Out comes a 1x50. It looks to be rightside padded.\r\n\r\n\r\nBuild the Model\r\n\r\n\r\ninput <- layer_input(shape = c(1), dtype = \"string\")\r\n\r\noutput <- input %>% \r\n  #custom layer, input text; output tensor\r\n  text_vectorization() %>% \r\n  # produce 16 new dimensions for each sentiment \r\n  layer_embedding(input_dim = num_words, output_dim = 16) %>% \r\n  # average over the new 16 dimensions\r\n  layer_global_average_pooling_1d() %>% \r\n  layer_dense(units = 16, activation = \"relu\") %>% \r\n  layer_dropout(0.5) %>% \r\n  layer_dense(units = 1, activation = \"sigmoid\")\r\n\r\nmodel <- keras_model(input, output)\r\n\r\n\r\nI politely steal this from the documentation,\r\n“The first layer is an embedding layer. This layer takes the\r\ninteger-encoded vocabulary and looks up the embedding vector for each\r\nword-index. These vectors are learned as the model trains. The\r\nvectors add a dimension to the output array. The resulting dimensions\r\nare: (batch, sequence, embedding).”\r\nSo the word embedding layerdoes indeed\r\nexpand the dimensions for each word index. It\r\nlearns these as it trains. Once complete, we can go back and pluck this\r\nlayer out and go start to visualize words from our corpus against one\r\nanother. Maybe we could perform the famous\r\nman - woman = king - queen example? More to follow.\r\n“Next, a global_average_pooling_1d layer returns a\r\nfixed-length output vector for each example by averaging over the\r\nsequence dimension. This allows the model to handle input of\r\nvariable length, in the simplest way possible.”\r\nI gladly steal more excellent interpretation\r\n” HIDDEN UNITS The above model has two intermediate or “hidden”\r\nlayers, between the input and output. The number of outputs (units,\r\nnodes, or neurons) is the dimension of the representational space for\r\nthe layer. In other words, the amount of freedom the network is allowed\r\nwhen learning an internal representation.\r\nIf a model has more hidden units (a higher-dimensional representation\r\nspace), and/or more layers, then the network can learn more complex\r\nrepresentations. However, it makes the network more computationally\r\nexpensive and may lead to learning unwanted patterns — patterns that\r\nimprove performance on training data but not on the test data. This is\r\ncalled overfitting, and we’ll explore it later”\r\nLoss Function and Optimizer\r\n\r\n\r\nmodel %>% compile(\r\n  optimizer = 'adam',\r\n  loss = 'binary_crossentropy',\r\n  metrics = list(\"accuracy\")\r\n)\r\n\r\n\r\nTrain the model\r\n\r\n\r\nhistory <- model %>% fit(\r\n  x = training$text,\r\n  y = as.numeric(training$tag == \"pos\"),\r\n  epochs = 10,\r\n  batch_size = 512,\r\n  validation_split = 0.2,\r\n  verbose = 2\r\n)\r\n\r\n\r\nEvaluate\r\n\r\n\r\nevaluate(model, testing$text, \r\n         as.numeric(testing$tag == \"pos\"), \r\n         verbose = 0)\r\n\r\n     loss  accuracy \r\n0.5966193 0.6841007 \r\n\r\nThis fairly naive approach achieves an accuracy of about 68%. With\r\nmore advanced approaches, the model should get closer to 85%.\r\nPlot Metrics\r\n\r\n\r\nplot(history)\r\n\r\n\r\n\r\nWhat’s in that layer?\r\nThat’s the magic question. Time to take a peak.\r\n\r\n\r\nembeddings <- model$layers[[3]]\r\n\r\n\r\n\r\n\r\nX <- embeddings$embeddings %>% \r\n  as.matrix %>% \r\n  as.data.frame %>% \r\n  select(V1, V2, V3) %>% \r\n  mutate(word = get_vocabulary(text_vectorization))\r\n\r\nggplot(data = X,\r\n       aes(x=V1, y=V2, color = V3, label = word)) + \r\n  geom_point() + \r\n  geom_text() + \r\n  theme_bw() +\r\n  xlab(\"Embedding 1\") +\r\n  ylab(\"Embedding 2\") +\r\n  labs(color = \"Embedding 3\") +\r\n  ggtitle(\"Word Embeddings\", subtitle = \"First Three Embeddings\") +\r\n  geom_vline(xintercept=min(X$V1), color=\"red\") +\r\n  geom_vline(xintercept=-0.01, color=\"red\", linetype=\"dotted\") +\r\n  geom_vline(xintercept=max(X$V1), color=\"green\") +\r\n  geom_vline(xintercept=0.01, color=\"green\", linetype=\"dotted\")\r\n\r\n\r\n# Maybe we can drill in\r\nggplot(data = X %>% filter(V1 < -0.4, V2 >-0.5),\r\n       aes(x=V1, y=V2, color = V3, label = word)) +\r\n  geom_point() +\r\n  geom_text() +\r\n  xlab(\"Embedding 1\") +\r\n  ylab(\"Embedding 2\") +\r\n  labs(color = \"Embedding 3\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nKind of amazing. Words such as …\r\nOutstanding\r\nBreathtaking\r\nFantastic\r\nGuido (which is usually a term for awesome)\r\nDamon (probably for Matt Damon, who is pretty awesome)\r\n… are all in just this little region. We learned a lot in this little\r\nmodel. Let’s look at one more visual with just our first two dimensions\r\nof word embeddings without pca.\r\n\r\n\r\n# What a treasure trove. For each word, we have a 16 dimensional representation it learned.\r\nX <- embeddings$embeddings %>% as.matrix\r\ndim(X)\r\n\r\n[1] 10000    16\r\n\r\ncomponents <- prcomp(X)\r\nPC <- components$x %>% as.data.frame %>% select(PC1, PC2)\r\nPC <- PC %>% dplyr::mutate(word = get_vocabulary(text_vectorization))\r\n\r\nggplot(data = PC, aes(x=PC1, y=PC2, label = word)) + \r\n  geom_point() + \r\n  geom_text() + \r\n  theme_bw() +\r\n  ggtitle(\"Word Embeddings\", subtitle = \"PCA Dimensionality Reduction\")+\r\n  geom_vline(xintercept=min(PC$PC1), color=\"red\") +\r\n  geom_vline(xintercept=-0.1, color=\"red\", linetype=\"dotted\") +\r\n  geom_vline(xintercept=max(PC$PC1), color=\"green\") +\r\n  geom_vline(xintercept=0.1, color=\"green\", linetype=\"dotted\")\r\n\r\n\r\n# What about those just in a certain region?\r\nggplot(data = PC %>% filter(PC1 < 2, PC1 > 1.5,\r\n                            PC2 < 0.1, PC2 > -0.1),\r\n       aes(x=PC1, y=PC2, label = word)) +\r\n  geom_point() +\r\n  geom_text() +\r\n  theme_bw()\r\n\r\n\r\n\r\nPretty amazing, even just the first few layers of word embeddings\r\nshow the same thing. Words like:\r\nawful\r\nwaste\r\njawbreaker\r\nrediculous\r\nstupid\r\nstupidity\r\nIncredible. This language model has learned words from our custom\r\ndomain.\r\nOne can only imagine that there are two populations of words because\r\nthis domain was specifically trained on them.\r\nAnother exercise is to flow these files from the directory\r\nstructure.\r\nReferences\r\nhttps://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification/\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-16-text-classification-via-keras-in-r/text-classification-via-keras-in-r_files/figure-html5/unnamed-chunk-17-1.png",
    "last_modified": "2022-07-16T17:10:27-04:00",
    "input_file": "text-classification-via-keras-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-14-q-learning-in-r/",
    "title": "Q-learning in R",
    "description": "In this post we will walk through the basics of Q-learning and source supporting functions.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-14",
    "categories": [
      "reinforcement learning",
      "markov decision process",
      "probability",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nQ-Learning House\r\nNavigation\r\nValue Iteration\r\n\r\nThe Problem\r\nReward Matrix\r\nEpisilon-greedy\r\n\r\nReferences\r\n\r\nQ-Learning House Navigation\r\nReinforcement learning is a relatively new field in machine learning,\r\nwith ties in control systems, decision theory, probability, and\r\nsimulation. A Markov Decision Process (MDP) is a process\r\nthat relies on previous information to make new decisions with maximum\r\nprobability of a meritorious outcome. One of the simplest algorithms to\r\ncompute this is called Value Iteration. We will be working\r\nthrough this with a simple housing example where we hope to\r\nlearn the optimal policy that will navigate an agent\r\nthrough the house. In general, these policies are often lodged in our\r\nown mental models, but once put on paper (or in a matrix), they are a\r\nlot more interesting, and sometimes calibrate even our own thinking.\r\nAfter all, the field of Deep Reinfrocement Learning has\r\nbeaten renouned championed in chess, alpha-go, and even star craft. So\r\nwhatever it is learning in those deep models can certainly augment our\r\nintelligence.\r\nThe simplicity of MCPs make them very interpretable, but at the cost\r\nof more power. So let’s take a look and see if we can cook up a function\r\nto solve the problem.\r\n\r\n\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\n\r\n\r\nValue Iteration\r\nThe idea of value iteration is that it is a recursive function with a\r\nfew well known things.\r\nS = State-space\r\nA = Action-space\r\nR = Reward function\r\nThe problem is set up as an objective. The real goal is to acquire a\r\npolicy that acts optimally. By acting optimally, this is\r\nscoped to maximizing a univariate reward function. How is this done?\r\nLet’s take a look at the math.\r\nValue Iteration \\[\r\nQ(s_1,a_1)=R(s_1,a_1) + \\gamma\\sum_{s_2 \\neq s_1 \\in S}{P(s_2 | s_1,\r\na_1) \\: max(Q(s_2, a_2) \\: \\forall a_2 \\in A)}\r\n\\]\r\nAt each iteration, a reward is known, and the future reward is also\r\ncalculated for every other state.\r\nBellman Equation \\[\r\nQ_{now}(s_1,a_1) = Q_{old}(s_1,a_1) +\\alpha \\: (r + \\gamma \\:\r\nmax_{a_2}(s_2, a_2)-Q_{old}(s_1, a_1))\r\n\\]\r\nThe Bellman Equation is also well known for solving\r\nproblems like this, and it doesn’t require the transition probability\r\nabove.\r\nThe Problem\r\nWe hope to mimic a house navigation agent. The goal is to be able to\r\nreview the optimal policy at the end that teaches the agent to travel\r\nfrom any room to any other room.\r\nHouse floorplanReward Matrix\r\nThere are three criteria,\r\n-1 if “you can’t get there from here”\r\n0 if the destination is not the target state\r\n100 if the destination is the target state\r\n\r\n\r\nR <- matrix(c(-1, -1, -1, -1, 0, 1,\r\n       -1, -1, -1, 0, -1, 0,\r\n       -1, -1, -1, 0, -1, -1, \r\n       -1, 0, 0, -1, 0, -1,\r\n        0, -1, -1, 0, -1, 0,\r\n       -1, 100, -1, -1, 100, 100), nrow=6, ncol=6, byrow=TRUE) %>% t\r\n\r\nR\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6]\r\n[1,]   -1   -1   -1   -1    0   -1\r\n[2,]   -1   -1   -1    0   -1  100\r\n[3,]   -1   -1   -1    0   -1   -1\r\n[4,]   -1    0    0   -1    0   -1\r\n[5,]    0   -1   -1    0   -1  100\r\n[6,]    1    0   -1   -1    0  100\r\n\r\n\r\n\r\ntmp <- R %>% as.data.frame()\r\ntmp <- tmp %>% dplyr::mutate(from = names(tmp))\r\ntmp <- tmp %>% tidyr::gather(., key = \"to\", \"reward\", -from)\r\ntmp <- tmp %>% dplyr::arrange(desc(reward))\r\n\r\nggplot(data = tmp) + \r\n  geom_tile(aes(x=from, y=to, fill=as.factor(reward))) + \r\n  labs(fill = \"Reward\") + theme_classic() + \r\n  ggtitle(\"Reward Matrix\")\r\n\r\n\r\n\r\n\r\n\r\nsource(\"https://raw.githubusercontent.com/NicoleRadziwill/R-Functions/master/qlearn.R\")\r\n\r\nresults <- q.learn(R,10000,alpha=0.1,gamma=0.8,tgt.state=6) \r\nround(results)\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6]\r\n[1,]    0    0    0    0   80    0\r\n[2,]    0    0    0   64    0  100\r\n[3,]    0    0    0   64    0    0\r\n[4,]    0   80   51    0   80    0\r\n[5,]   64    0    0   64    0  100\r\n[6,]   64   80    0    0   80  100\r\n\r\nThe table produced tells us the average value to obtain policies. A\r\npolicy is a path through the states. One can quickly observe going from\r\nroom 0 to room 5 which is the solution. You\r\ncan walk through these and choose the argmax which along\r\nthe columns that provides the decision for your next row of choice.\r\n\r\n\r\ntmp <- results %>% as.data.frame()\r\ntmp <- tmp %>% dplyr::mutate(from = names(tmp))\r\ntmp <- tmp %>% tidyr::gather(., key = \"to\", \"reward\", -from)\r\ntmp <- tmp %>% dplyr::arrange(desc(reward))\r\n\r\nggplot(data = tmp) + \r\n  geom_tile(aes(x=to, y=from, fill=reward)) + \r\n  labs(fill = \"Average Value (%)\") + theme_classic() + \r\n  ggtitle(\"Policy Decision Matrix\") +\r\n  scale_fill_gradient(low = \"red\", high = \"green\") +\r\n  ylab(\"State From\") + \r\n  xlab(\"Action To\")\r\n\r\n\r\n\r\nPretty cool!\r\nEpisilon-greedy\r\nOne common way of sampling an action is the\r\nepislon-greedy approach. This tells us to exploit with\r\nepsilon probability and randomly explore with 1-epsilon. Let’s take a\r\nlook.\r\n\r\n\r\nq.learn.epsilon.greedy <- function(R, epochs = 10000, alpha = 0.1,gamma = 0.8, epsilon = 0.75,tgt.state = 6, testme = T){\r\n  \r\n  Q = matrix(0, nrow = nrow(R), ncol=ncol(R))\r\n  \r\n  epsilon.greedy <- function(state, Q, epsilon = 0.75, test = F){\r\n    random_number <- runif(n = 1, min = 0, max = 1)\r\n    if(random_number < epsilon){\r\n      \r\n      if(test)\r\n        paste(\"exploit\")\r\n      else\r\n        which.max(Q[state,])\r\n    }\r\n    else{\r\n      \r\n      if(test)\r\n        paste(\"explore\")\r\n      else\r\n        sample(which(Q[state, ] != state), 1)\r\n      \r\n    }\r\n  }\r\n  \r\n  test.epsilon.greedy <- function(epsilon = 0.75){\r\n    memory <- c()\r\n    tmp <- c()\r\n    for(j in 1:100){\r\n      for(i in 1:100){\r\n      # Start at a random state\r\n      cs = sample(1:nrow(Q), 1)\r\n      ns = epsilon.greedy(cs, Q, epsilon = epsilon, test = T) \r\n      tmp <- c(tmp, ns) # we expect 75/100 to be exploits\r\n      } \r\n      memory <- c(memory, mean(tmp == \"exploit\"))\r\n    }\r\n    \r\n    # looks good.\r\n    memory %>% mean\r\n  }\r\n  \r\n  # Test it\r\n  if(testme){\r\n    \r\n    t1<-test.epsilon.greedy(epsilon=0.75)\r\n    t2<-test.epsilon.greedy(epsilon=0.5)\r\n    t3<-test.epsilon.greedy(epsilon=0.25)\r\n    print(paste(round(t1,2),\" == 0.75\"))\r\n    print(paste(round(t2,2),\" == 0.50\"))\r\n    print(paste(round(t3,2),\" == 0.25\"))\r\n  }\r\n  \r\n  \r\n  learning_rate = alpha\r\n  discount_rate = gamma\r\n  dest.state = tgt.state\r\n  \r\n  for(i in 1:epochs){\r\n    \r\n    # Start at a random state\r\n    cs = sample(1:nrow(Q), 1)\r\n    cs    \r\n  \r\n    while(TRUE){\r\n        \r\n        \r\n        # Take an action\r\n        a = epsilon.greedy(cs, Q, epsilon = 0.8, test = F)\r\n        \r\n        \r\n        # Assess the reward\r\n        r = R[cs, a]\r\n        \r\n        # Update policy matrix\r\n        Q[cs, a] = learning_rate * (Q[cs, a]) + (1 - learning_rate) * (r + discount_rate * max(Q[a, ]) )\r\n         # Q[cs,a] <- Q[cs,a] + learning_rate*(R[cs,a] + discount_rate*max(Q[a, ]) - Q[cs,a])\r\n        \r\n         # Update the current state\r\n        cs = a\r\n        \r\n        \r\n        if(a == dest.state){\r\n          break\r\n        }\r\n    }\r\n  }\r\n  Q*100/max(Q)\r\n}\r\n\r\nQ = q.learn.epsilon.greedy(R, epochs = 10000, alpha = 0.1,gamma = 0.8, epsilon = 0.75, tgt.state = 6, testme=T)\r\n\r\n[1] \"0.75  == 0.75\"\r\n[1] \"0.51  == 0.50\"\r\n[1] \"0.26  == 0.25\"\r\n\r\nidx = apply(Q, 1, which.max)\r\nI <- matrix(0, nrow=nrow(Q), ncol=ncol(Q))\r\nfor(i in 1:nrow(I)){\r\n  I[i,idx[i]] = 1\r\n}\r\nI\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6]\r\n[1,]    0    0    0    0    1    0\r\n[2,]    0    0    0    0    0    1\r\n[3,]    0    1    0    0    0    0\r\n[4,]    0    1    0    0    0    0\r\n[5,]    0    0    0    0    0    1\r\n[6,]    0    0    0    0    0    1\r\n\r\nOur resulting matrix from an epsilon greedy algorithm\r\ndoes something similar to the 100% exploit algorithm sourced in from the\r\ngithub above. The policy is as follows:\r\nIf in room 0 go to room 4, then to room 5 for the victory. (2\r\nsteps)\r\nIf in room 4 go to room 5 for the victory. (1 step)\r\nIf in room 1 go to room 5 for the victory. (1 step)\r\nIf in room 2 go to room 1, then go to room 5 for the victory. (2\r\nsteps)\r\nIf in room 3 go to room 1, then go to room 5 for the victory. (2\r\nsteps)\r\nIf in room 4, go to room 5 for the victory. (1 step)\r\nIf in room 5 .. victory! (0 steps)\r\nFor convenience, an indicator matrix is also built to illustrate the\r\nbest possibile choice to and from any state with zero ambiguity. At a\r\nglance, outside of room 5, room 1 is a favorable place to be. It seems\r\nlike it discovered a good outlet to arrive at the optimal, that is not\r\noptimal itself. Very interesting!\r\nReferences\r\nhttps://www.r-bloggers.com/2017/12/a-simple-intro-to-q-learning-in-r-floor-plan-navigation/#google_vignette\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-14-q-learning-in-r/q-learning-in-r_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-07-15T13:29:15-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-13-image-recognition-in-r/",
    "title": "Image Recognition in R",
    "description": "In this post we will explore image classification in keras for several datasets and how transfer learning can be readily applied to improve well known models.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-13",
    "categories": [
      "image recognition",
      "deep learning",
      "transfer learning",
      "keras",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nRecognize Handwritten\r\nDigits\r\nLibraries\r\nData Retrieval\r\nData Shapes\r\nVisualize Images\r\n\r\nModel\r\nSimple Dense Model\r\nSummary\r\nCompile\r\nFit\r\nVisualize\r\n\r\nAnother way\r\nSummary\r\nCompile\r\nFit\r\nVisualize\r\nPredictions\r\nEvaluate\r\nSave Model\r\nReload Model\r\n\r\n\r\nRecognize Fashion\r\nLoad the data\r\nPreprocess the data\r\nBuild the model\r\nEvaluate Accuracy\r\nMake predictions\r\nHow well did we do?\r\n\r\nRecognize Animals and\r\nObjects\r\nConvolutional Neural\r\nNetwork\r\nTransfer Learning\r\nVisualize performance\r\nSave the model\r\nEvaluate the model\r\n\r\nReferences\r\n\r\nRecognize Handwritten Digits\r\nComputer vision as a sub-field of deep learning has exploaded over\r\nthe last decade. The advent of better computers, readily available data\r\nsources, and explosively intelligent models with very little code has\r\nmade the unthinkable doable, and quickly.\r\nLibraries\r\n\r\n\r\nlibrary(keras)\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\nFirst we will grab the MNIST dataset. This consists of an array of\r\n28x28 images with 10 classification labels.\r\nData Retrieval\r\n\r\n\r\n\r\n\r\n\r\nmnist %>% names\r\n\r\n[1] \"train\" \"test\" \r\n\r\nWe can save the shapes and number of classes for later.\r\nData Shapes\r\n\r\n\r\n# Get the width and height\r\nWIDTH = dim(mnist$train$x)[[2]]\r\nHEIGHT = dim(mnist$train$x)[[3]]\r\n\r\n\r\n# Get unique number of classes\r\nCLASSES = length(unique(mnist$train$y))\r\n\r\nmnist$train$x %>% dim\r\n\r\n[1] 60000    28    28\r\n\r\nmnist$train$y %>% dim\r\n\r\n[1] 60000\r\n\r\nmnist$test$x %>% dim\r\n\r\n[1] 10000    28    28\r\n\r\nmnist$test$y %>% dim\r\n\r\n[1] 10000\r\n\r\nVisualize Images\r\nNext we can visualize a few images using the plot\r\nfunction in r. This was a little weird at first, because the images\r\nsometimes need standardized for rgb values depending on the function and\r\ndata shape.\r\n\r\n\r\nlibrary(raster)\r\nplot_a_few <- function(x,y, a_few = 3, rgb_dim=FALSE){\r\n    # Render a few images\r\n    rand_image_index = sample(1:dim(x)[[1]], size = a_few)\r\n    par(mar=c(0, 0, 0, 0))\r\n    for(i in rand_image_index){\r\n      if(rgb_dim){\r\n        img = x[i,,,]\r\n      }\r\n      else{\r\n        img = x[i,,]\r\n        # image(img, useRaster=TRUE, axes=FALSE)\r\n      }\r\n      \r\n      plot(as.raster(img))\r\n      label = y[i]\r\n      print(label)\r\n    }\r\n}\r\n\r\nplot_a_few(mnist$train$x, mnist$train$y, a_few=3)\r\n\r\n\r\n[1] 3\r\n\r\n[1] 2\r\n\r\n[1] 5\r\n\r\nModel\r\nSimple Dense Model\r\nThe simplest model will take the image tensor and flatten it into the\r\nstandard feed forward format. The prediction is over our\r\nCLASSES which is 10.\r\n\r\n\r\n# Simple model\r\nmodel <- keras::keras_model_sequential() %>% \r\n            keras::layer_flatten(input_shape = c(WIDTH, HEIGHT), \r\n                                 name = \"mnist_flatten_input\") %>% \r\n            keras::layer_dense(units = 128, activation = \"relu\", \r\n                               name = \"mnist_dense\") %>% \r\n            keras::layer_dropout(0.2, name = \"mnist_dropout\") %>% \r\n            keras::layer_dense(CLASSES, activation = \"softmax\", \r\n                               name = \"mnist_dense_output\")\r\nmodel\r\n\r\nModel: \"sequential\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nmnist_flatten_input (Flatten)  (None, 784)                 0          \r\n______________________________________________________________________\r\nmnist_dense (Dense)            (None, 128)                 100480     \r\n______________________________________________________________________\r\nmnist_dropout (Dropout)        (None, 128)                 0          \r\n______________________________________________________________________\r\nmnist_dense_output (Dense)     (None, 10)                  1290       \r\n======================================================================\r\nTotal params: 101,770\r\nTrainable params: 101,770\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nSummary\r\n\r\n\r\n# Some summary statistics\r\nbase::summary(model)\r\n\r\nModel: \"sequential\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nmnist_flatten_input (Flatten)  (None, 784)                 0          \r\n______________________________________________________________________\r\nmnist_dense (Dense)            (None, 128)                 100480     \r\n______________________________________________________________________\r\nmnist_dropout (Dropout)        (None, 128)                 0          \r\n______________________________________________________________________\r\nmnist_dense_output (Dense)     (None, 10)                  1290       \r\n======================================================================\r\nTotal params: 101,770\r\nTrainable params: 101,770\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nCompile\r\nReminder that sparse_categorical_crossentropy is for\r\nnon-matrix like y values. This will do it for you.\r\nOtherwise, you need to use the to_categorical function to\r\ntransform the y vector into a matrix.\r\n\r\n\r\n# Compile the model\r\nmodel %>% \r\n  keras::compile(\r\n    loss = \"sparse_categorical_crossentropy\",\r\n    optimizer = \"adam\",\r\n    metrics = \"accuracy\"\r\n  )\r\n\r\n\r\nFit\r\n\r\n\r\n# Fit the model\r\nhistory = model %>% \r\n            keras::fit(\r\n              x = mnist$train$x, y = mnist$train$y,\r\n              epochs = 5,\r\n              validation_split = 0.3,\r\n              verbose = 2\r\n            )\r\n\r\n\r\nVisualize\r\n\r\n\r\nplot_history_metrics = function(history){\r\n    # Plot fit results - loss and accuracy for this model\r\n    tmp = data.frame(history$metrics) %>% dplyr::mutate(epoch = row_number())\r\n    plt1 = ggplot(data=tmp) +\r\n            geom_line(aes(x=epoch, y = loss, color=\"training loss\")) +\r\n            geom_line(aes(x=epoch, y = val_loss, color=\"validation loss\")) +\r\n            theme_bw() +\r\n            labs(color=\"Legend\") + \r\n            ggtitle(\"Model Loss\")\r\n    plt1\r\n    \r\n    \r\n    plt2 = ggplot(data=tmp) +\r\n            geom_line(aes(x=epoch, y = accuracy, color=\"training accuracy\")) +\r\n            geom_line(aes(x=epoch, y = val_accuracy, color=\"validation accuracy\")) +\r\n            theme_bw() +\r\n            labs(color=\"Legend\") + \r\n            ggtitle(\"Model Accuracy\")\r\n    plt2\r\n    \r\n    list(loss_plot = plt1, acc_plot = plt2)\r\n}\r\n\r\nplot_history_metrics(history)\r\n\r\n$loss_plot\r\n\r\n\r\n$acc_plot\r\n\r\n\r\nAnother way\r\nAnother equally valid way as oppose to flattening the input as an\r\narray is to do it explicitely on the outside. This can be done use the\r\narray_reshape function. We can also make our y values into\r\na categorical matrix using the to_Categorical function.\r\nThis will change our sparse_categorical_crossentropy into\r\ncategorical_crossentropy. A tricky distinction, but one\r\ndoesn’t expect a matrix, one does.\r\n\r\n\r\nx_train <- keras::array_reshape(mnist$train$x, c(nrow(mnist$train$x), WIDTH*HEIGHT))\r\nx_test <- keras::array_reshape(mnist$test$x, c(nrow(mnist$test$x), WIDTH*HEIGHT))\r\ny_train <- keras::to_categorical(mnist$train$y, 10)\r\ny_test <- keras::to_categorical(mnist$test$y, 10)\r\n\r\nx_test %>% dim\r\n\r\n[1] 10000   784\r\n\r\ny_test %>% head\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\r\n[1,]    0    0    0    0    0    0    0    1    0     0\r\n[2,]    0    0    1    0    0    0    0    0    0     0\r\n[3,]    0    1    0    0    0    0    0    0    0     0\r\n[4,]    1    0    0    0    0    0    0    0    0     0\r\n[5,]    0    0    0    0    1    0    0    0    0     0\r\n[6,]    0    1    0    0    0    0    0    0    0     0\r\n\r\n\r\n\r\n# Model pre-flattened for shape and made categorically long in y\r\nmodel <- keras::keras_model_sequential() %>%  \r\n            keras::layer_dense(input_shape = c(WIDTH*HEIGHT), \r\n                               units = 128, \r\n                               activation = \"relu\", \r\n                               name = \"mnist_dense\") %>% \r\n            keras::layer_dropout(0.2, \r\n                                 name = \"mnist_dropout\") %>% \r\n            keras::layer_dense(CLASSES, \r\n                               activation = \"softmax\", \r\n                               name = \"mnist_dense_output\")\r\nmodel\r\n\r\nModel: \"sequential_1\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nmnist_dense (Dense)            (None, 128)                 100480     \r\n______________________________________________________________________\r\nmnist_dropout (Dropout)        (None, 128)                 0          \r\n______________________________________________________________________\r\nmnist_dense_output (Dense)     (None, 10)                  1290       \r\n======================================================================\r\nTotal params: 101,770\r\nTrainable params: 101,770\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nSummary\r\n\r\n\r\n# Model architectures\r\nbase::summary(model)\r\n\r\nModel: \"sequential_1\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nmnist_dense (Dense)            (None, 128)                 100480     \r\n______________________________________________________________________\r\nmnist_dropout (Dropout)        (None, 128)                 0          \r\n______________________________________________________________________\r\nmnist_dense_output (Dense)     (None, 10)                  1290       \r\n======================================================================\r\nTotal params: 101,770\r\nTrainable params: 101,770\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nCompile\r\n\r\n\r\n# Compile the model\r\nmodel %>% \r\n  keras::compile(\r\n    # loss = \"sparse_categorical_crossentropy\",\r\n    loss = \"categorical_crossentropy\",\r\n    optimizer = \"adam\",\r\n    metrics = \"accuracy\"\r\n  )\r\n\r\n\r\nFit\r\nOnce we configure our model, we can compile it,\r\nfit, then plot to see the performance. Turns\r\nout, you can just do plot(history) and the function to plot\r\nthese metrics is entirely superfluous.\r\n\r\n\r\nhistory = model %>% keras::fit(\r\n  x = x_train, y = y_train,\r\n  validation_split = 0.3,\r\n  epochs = 5,\r\n  verbose = 2\r\n)\r\n\r\n\r\nVisualize\r\n\r\n\r\nplot_history_metrics = function(history){\r\n    # Plot fit results - loss and accuracy for this model\r\n    tmp = data.frame(history$metrics) %>% dplyr::mutate(epoch = row_number())\r\n    plt1 = ggplot(data=tmp) +\r\n            geom_line(aes(x=epoch, y = loss, color=\"training loss\")) +\r\n            geom_line(aes(x=epoch, y = val_loss, color=\"validation loss\")) +\r\n            theme_bw() +\r\n            labs(color=\"Legend\") + \r\n            ggtitle(\"Model Loss\")\r\n    plt1\r\n    \r\n    \r\n    plt2 = ggplot(data=tmp) +\r\n            geom_line(aes(x=epoch, y = accuracy, color=\"training accuracy\")) +\r\n            geom_line(aes(x=epoch, y = val_accuracy, color=\"validation accuracy\")) +\r\n            theme_bw() +\r\n            labs(color=\"Legend\") + \r\n            ggtitle(\"Model Accuracy\")\r\n    plt2\r\n    \r\n    list(loss_plot = plt1, acc_plot = plt2)\r\n}\r\n\r\nplot_history_metrics(history)\r\n\r\n$loss_plot\r\n\r\n\r\n$acc_plot\r\n\r\n\r\nPredictions\r\n\r\n\r\n# Generate some predictions on the unseen data\r\npredictions = stats::predict(model, x_test)\r\npredictions %>% head()\r\n\r\n             [,1]         [,2]         [,3]         [,4]         [,5]\r\n[1,] 2.147511e-06 3.081970e-08 1.363529e-05 6.959682e-04 1.883129e-09\r\n[2,] 4.554735e-07 1.738241e-05 9.998441e-01 5.303881e-05 3.181212e-13\r\n[3,] 3.884773e-06 9.974592e-01 1.834323e-04 8.748658e-05 1.106339e-04\r\n[4,] 9.970744e-01 2.236041e-06 1.048536e-04 6.934661e-05 1.657578e-04\r\n[5,] 2.274211e-06 8.666921e-09 7.863012e-06 9.704200e-08 9.973132e-01\r\n[6,] 3.012697e-07 9.972008e-01 3.240131e-06 6.145242e-06 1.901888e-05\r\n             [,6]         [,7]         [,8]         [,9]        [,10]\r\n[1,] 1.727558e-06 8.484718e-11 9.990858e-01 1.901059e-06 1.988278e-04\r\n[2,] 5.569929e-05 2.090738e-07 1.970074e-10 2.905491e-05 5.533708e-11\r\n[3,] 6.059188e-06 1.315140e-04 1.861338e-03 1.532390e-04 3.115008e-06\r\n[4,] 6.942511e-06 1.050111e-03 8.822395e-04 5.042305e-06 6.392266e-04\r\n[5,] 1.149999e-05 3.039010e-06 5.597093e-05 1.561662e-06 2.604362e-03\r\n[6,] 3.678756e-08 1.334996e-06 2.748028e-03 1.928041e-05 1.795054e-06\r\n\r\nEvaluate\r\n\r\n\r\n# Evaluate performance\r\n# test_results = model %>% \r\n#                   evaluate(mnist$test$x, mnist$test$y, verbose = 0)\r\n# test_results\r\n\r\n\r\nSave Model\r\nOne thing keras makes incredibly easy is the ability to save your\r\nmodel. This will create a folder and allow for easy access to and from\r\nyour model if you need it for predictions in another environment or\r\nAPI.\r\n\r\n\r\n# Serialize the model (it becomes a folder)\r\n# keras::save_model_tf(object = model, filepath = \"mnist_model\")\r\n\r\n\r\nReload Model\r\n\r\n\r\n# Reload the model\r\n# reloaded_model = keras::load_model_tf(\"mnist_model\")\r\n# reloaded_model %>% summary\r\n# base::all.equal(stats::predict(model, x_test), \r\n#                 stats::predict(reloaded_model, x_test))\r\n\r\n\r\nRecognize Fashion\r\nRecognizing other types of objects is just as easy as before. Let’s\r\nrepeat our steps for a new dataset, because practice makes\r\nperfect!\r\nLoad the data\r\n\r\n\r\nfashion_mnist <- dataset_fashion_mnist()\r\n\r\nc(train_images, train_labels) %<-% fashion_mnist$train\r\nc(test_images, test_labels) %<-% fashion_mnist$test\r\n\r\n\r\n\r\n\r\nclass_names = c('T-shirt/top',\r\n                'Trouser',\r\n                'Pullover',\r\n                'Dress',\r\n                'Coat', \r\n                'Sandal',\r\n                'Shirt',\r\n                'Sneaker',\r\n                'Bag',\r\n                'Ankle boot')\r\n\r\n\r\n\r\n\r\ndim(train_images)\r\n\r\n[1] 60000    28    28\r\n\r\ndim(train_labels)\r\n\r\n[1] 60000\r\n\r\ntrain_labels[1:20]\r\n\r\n [1] 9 0 0 3 0 2 7 2 5 5 0 9 5 5 7 9 1 0 6 4\r\n\r\ndim(test_images)\r\n\r\n[1] 10000    28    28\r\n\r\ndim(test_labels)\r\n\r\n[1] 10000\r\n\r\nPreprocess the data\r\n\r\n\r\nlibrary(tidyr)\r\nlibrary(ggplot2)\r\n\r\n\r\nimage_1 <- as.data.frame(train_images[1,,])\r\ncolnames(image_1) <- seq_len(ncol(image_1))\r\nimage_1$y <- seq_len(nrow(image_1))\r\nimage_1 <- tidyr::gather(image_1, key = \"x\", value = \"value\", -y)\r\nimage_1$x <- as.integer(image_1$x)\r\n\r\nggplot(image_1, aes(x=x,y=y,fill=value)) +\r\n  geom_tile() +\r\n  scale_fill_gradient(low = \"white\", high = \"black\", na.value = NA) +\r\n  scale_y_reverse() +\r\n  theme_minimal() +\r\n  theme(panel.grid = element_blank()) +\r\n  theme(aspect.ratio = 1) +\r\n  xlab(\"\") +\r\n  ylab(\"\") +\r\n  ggtitle(paste(class_names[train_labels[1]+1]))\r\n\r\n\r\n\r\n\r\n\r\ntrain_images <- train_images / 255\r\ntest_images <- test_images / 255\r\n\r\npar(mfcol = c(5,5))\r\npar(mar=c(0,0,1.5,0), axs='i', yaxs='i')\r\nfor(i in 1:25){\r\n  img <- train_images[i,,]\r\n  # img <- t(apply(img, 2, rev))\r\n  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n', main = paste(class_names[train_labels[i]+1]))\r\n}\r\n\r\n\r\n\r\nBuild the model\r\n\r\n\r\nmodel <- keras_model_sequential() %>%\r\n  layer_flatten(input_shape = c(28, 28)) %>% \r\n  layer_dense(units = 128, activation = 'relu') %>% \r\n  layer_dense(units = 10, activation = 'softmax')\r\n\r\nmodel %>% compile(\r\n  optimizer = 'adam', \r\n  loss = 'sparse_categorical_crossentropy',\r\n  metrics = c('accuracy')\r\n)\r\n\r\nmodel %>% \r\n  fit(x=train_images, y=train_labels, \r\n      epochs = 5, verbose = 2, validation_split=0.3)\r\n\r\n\r\nEvaluate Accuracy\r\n\r\n\r\nscore <- model %>% evaluate(test_images, test_labels, verbose = 0)\r\n\r\ncat('Test loss:', score[1], \"\\n\")\r\n\r\nTest loss: 0.3637977 \r\n\r\ncat('Test accuracy:', score[2], \"\\n\")\r\n\r\nTest accuracy: 0.8708 \r\n\r\nMake predictions\r\n\r\n\r\npredictions <- model %>% predict(test_images)\r\npredictions %>% head\r\n\r\n             [,1]         [,2]         [,3]         [,4]         [,5]\r\n[1,] 5.944940e-06 3.568362e-09 9.062223e-07 3.813431e-07 7.632132e-07\r\n[2,] 6.902370e-05 6.028386e-09 9.972779e-01 5.225142e-08 3.154729e-04\r\n[3,] 5.386319e-06 9.999939e-01 1.031454e-08 3.366561e-07 2.402087e-07\r\n[4,] 9.895704e-06 9.999171e-01 3.511240e-07 6.675994e-05 4.046826e-06\r\n[5,] 1.955759e-01 2.231707e-04 2.183121e-01 8.242820e-03 1.920183e-02\r\n[6,] 1.318200e-03 9.986019e-01 8.571616e-06 8.032462e-06 3.114295e-05\r\n             [,6]         [,7]         [,8]         [,9]        [,10]\r\n[1,] 1.908284e-02 1.337585e-05 5.273721e-02 1.333138e-04 9.280253e-01\r\n[2,] 3.593275e-11 2.337541e-03 3.329948e-12 3.700688e-08 2.485134e-13\r\n[3,] 1.300379e-13 1.142511e-07 2.336990e-14 3.351031e-08 7.711556e-12\r\n[4,] 4.459056e-11 1.710582e-06 8.355450e-12 1.272454e-07 1.614447e-09\r\n[5,] 5.498093e-05 5.563514e-01 2.239543e-05 2.010975e-03 4.317736e-06\r\n[6,] 2.356733e-10 3.082445e-05 1.095561e-10 1.486991e-06 3.072765e-09\r\n\r\npreds = apply(predictions, 1, which.max)\r\npreds %>% head\r\n\r\n[1] 10  3  2  2  7  2\r\n\r\n#or\r\n\r\npreds = model %>% predict_classes(x = test_images)\r\npreds %>% unique\r\n\r\n [1] 9 2 1 6 4 5 7 3 8 0\r\n\r\nHow well did we do?\r\n\r\n\r\npar(mfcol=c(5,5))\r\npar(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')\r\nfor (i in 1:25) { \r\n  img <- test_images[i, , ]\r\n  img <- t(apply(img, 2, rev)) \r\n  # subtract 1 as labels go from 0 to 9\r\n  predicted_label <- which.max(predictions[i, ]) - 1\r\n  true_label <- test_labels[i]\r\n  if (predicted_label == true_label) {\r\n    color <- '#008800' \r\n  } else {\r\n    color <- '#bb0000'\r\n  }\r\n  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',\r\n        main = paste0(class_names[predicted_label + 1], \" (\",\r\n                      class_names[true_label + 1], \")\"),\r\n        col.main = color)\r\n}\r\n\r\n\r\n\r\n\r\n\r\ntrain_images %>% dim\r\n\r\n[1] 60000    28    28\r\n\r\nRecognize Animals and\r\nObjects\r\n\r\n\r\nlibrary(tensorflow)\r\nlibrary(keras)\r\n\r\ncifar <- dataset_cifar10()\r\n\r\nclass_names <- c('airplane', 'automobile', 'bird', 'cat', 'deer',\r\n               'dog', 'frog', 'horse', 'ship', 'truck')\r\n\r\nindex <- 1:30\r\n\r\npar(mfcol = c(5,6), mar = rep(1,4), oma=rep(0.2, 4))\r\ncifar$train$x[index,,,] %>% \r\n  purrr::array_tree(margin=1) %>% \r\n  purrr::set_names(class_names[cifar$train$y[index] + 1]) %>% \r\n  purrr::map(as.raster, max = 255) %>% \r\n  purrr::iwalk(~{plot(.x); title(.y)})\r\n\r\n\r\n\r\nConvolutional Neural Network\r\n\r\n\r\nmodel <- keras_model_sequential() %>% \r\n  layer_conv_2d(input_shape = c(32, 32, 3),  filters = 32, kernel_size = c(3,3), activation = \"relu\") %>% \r\n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \r\n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = \"relu\") %>% \r\n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \r\n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = \"relu\") %>% \r\n  layer_flatten() %>% \r\n  layer_dense(units = 64, activation = \"relu\") %>% \r\n  layer_dense(units = 10, activation = \"softmax\")\r\n\r\nsummary(model)\r\n\r\nModel: \"sequential_3\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nconv2d_2 (Conv2D)              (None, 30, 30, 32)          896        \r\n______________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2D) (None, 15, 15, 32)          0          \r\n______________________________________________________________________\r\nconv2d_1 (Conv2D)              (None, 13, 13, 64)          18496      \r\n______________________________________________________________________\r\nmax_pooling2d (MaxPooling2D)   (None, 6, 6, 64)            0          \r\n______________________________________________________________________\r\nconv2d (Conv2D)                (None, 4, 4, 64)            36928      \r\n______________________________________________________________________\r\nflatten_1 (Flatten)            (None, 1024)                0          \r\n______________________________________________________________________\r\ndense_3 (Dense)                (None, 64)                  65600      \r\n______________________________________________________________________\r\ndense_2 (Dense)                (None, 10)                  650        \r\n======================================================================\r\nTotal params: 122,570\r\nTrainable params: 122,570\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nmodel %>% compile(\r\n  optimizer = \"adam\",\r\n  loss = \"sparse_categorical_crossentropy\",\r\n  metrics = \"accuracy\"\r\n)\r\n\r\n\r\nhistory <- model %>% \r\n  fit(\r\n    x = cifar$train$x, y = cifar$train$y,\r\n    epochs = 10,\r\n    validation_data = unname(cifar$test),\r\n    verbose = 2\r\n  )\r\n\r\n\r\n\r\n\r\nevaluate(model, cifar$test$x, cifar$test$y, verbose = 0)\r\n\r\n    loss accuracy \r\n1.069247 0.656700 \r\n\r\nTransfer Learning\r\nWe will not actually train here, because it takes about 45 minutes.\r\nBut we will just unload the model previously trained. But the idea is to\r\ntake a layer or many layers from a previouos model and then stack our\r\nmodel on top of it. You don’t have to stack it “on top”, but I chose to\r\nhere for simplicity. By taking what the previous model learned, we then\r\nput our custom output layers there so it can learn to classify new\r\nthings, with old feature vectors it learned from the\r\nimagenet data set.\r\n\r\n\r\nlibrary(devtools)\r\nlibrary(tfhub)\r\nlibrary(keras)\r\nlibrary(reticulate)\r\n\r\nc(train_images, train_labels) %<-% cifar$train\r\nc(test_images, test_labels) %<-% cifar$test\r\n\r\ntrain_images %>% dim\r\n\r\n[1] 50000    32    32     3\r\n\r\ntrain_labels %>% dim\r\n\r\n[1] 50000     1\r\n\r\ntest_images %>% dim\r\n\r\n[1] 10000    32    32     3\r\n\r\ntest_labels %>% dim  \r\n\r\n[1] 10000     1\r\n\r\nimage_shape <- c(32,32,3)\r\n\r\nconv_base <- keras::application_resnet101(weights = \"imagenet\",\r\n                                          include_top = FALSE, \r\n                                          input_shape = c(32,32,3))\r\n\r\n\r\nfreeze_weights(conv_base)\r\n\r\nmodel <- keras_model_sequential() %>%\r\n  conv_base %>%\r\n  layer_flatten() %>%\r\n  # layer_reshape(c(1,2048)) %>% \r\n  layer_dense(units = 256, activation = \"relu\") %>%\r\n  layer_dense(units = 10, activation = \"softmax\")\r\n\r\n\r\nmodel %>% compile(\r\n  optimizer = \"adam\",\r\n  loss = \"sparse_categorical_crossentropy\",\r\n  metrics = \"accuracy\"\r\n)\r\n\r\n\r\n# unfreeze_weights(conv_base, from = \"block5_conv1\")\r\n\r\nhistory <- model %>% fit(\r\n  x=train_images, y=train_labels,\r\n  validation_split = 0.3,\r\n  epochs=3,\r\n  verbose = 2\r\n)\r\n\r\n# model = keras::load_model_tf(\"cifar10_tl_model\")\r\n# model %>% summary\r\n\r\n# summary(model)\r\n# train_images[1,,,] %>% dim\r\n# train_labels[1]\r\n\r\n\r\nVisualize performance\r\n\r\n\r\n# plot(history)\r\n\r\n\r\nSave the model\r\nThe following code is used to serialize the model, since this is\r\nalready done and the process is fairly intensive, we will not be\r\nrepeating it here.\r\n\r\n\r\n# # Serialize the model (it becomes a folder)\r\n# keras::save_model_tf(object = model, filepath = \"cifar10_tl_model\")\r\n# \r\n# # Reload the model\r\n# reloaded_model = keras::load_model_tf(\"cifar10_tl_model\")\r\n# reloaded_model %>% summary\r\n\r\n\r\nEvaluate the model\r\n\r\n\r\nevaluate(model, x = test_images, y = test_labels)\r\n\r\n    loss accuracy \r\n1.188451 0.592300 \r\n\r\nReferences\r\nhttps://tensorflow.rstudio.com/tutorials/beginners/\r\nhttps://tensorflow.rstudio.com/tutorials/advanced/images/cnn/\r\nhttps://tensorflow.rstudio.com/tutorials/advanced/images/transfer-learning-hub/\r\nhttps://keras.rstudio.com/reference/freeze_layers.html\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-13-image-recognition-in-r/image-recognition-in-r_files/figure-html5/visualize-1.png",
    "last_modified": "2022-07-16T16:41:41-04:00",
    "input_file": "image-recognition-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-13-logistic-regression-and-gradient-descent-in-python/",
    "title": "Logistic Regression and Gradient Descent in Python",
    "description": "In this post we will walk through how to train a Logistic Regression model from scratch using Gradient Descent in Python.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-13",
    "categories": [
      "machine learning",
      "non-linear programming",
      "python"
    ],
    "contents": "\r\n\r\nContents\r\nOverview\r\nSigmoid\r\nNegative Log Likelihood\r\nGradient Log Likelihood\r\n\r\nGradient Descent\r\nValidation\r\nTest Data\r\nOptimal Decision\r\nBoundary\r\n\r\n\r\nOverview\r\nOne of the most simple machine learning problems is that of\r\nbinary classification. Further, one of the most simple\r\nnon-linear models is logistic regression. In short, this\r\nmodel takes a set of parameters and seeks a linear combination mapped to\r\na non-linear sigmoid function which maximizes the\r\nlikelihood of fitting the data. The math for this is\r\nbelow,\r\nSigmoid Function\r\n\\[\r\n\\sigma(z)=\\frac{1}{1+exp(-z)}\r\n\\] Maximum Likelihood Estimation\r\n\\[\r\nL(x,y; \\theta)=\\frac{1}{n} \\prod_{i=1}^n{\\sigma(\\theta^Tx)^{y_i} +\r\n(1-\\sigma(\\theta^Tx))^{1-y_i}}\r\n\\] Negative Log Likelihood or Cross Entropy\r\n\\[\r\nl(x,y; \\theta)=-\\frac{1}{n}\\sum_{i=1}^n{{y_i}log(\\sigma(\\theta^Tx)) +\r\n(1-y_i)log(1-\\sigma(\\theta^Tx))}\r\n\\] Gradient of Negative Log Likelihood\r\n\\[\r\n\\nabla_\\theta l(x,y) =\r\n\\frac{1}{n}\\sum_{i=1}^n{x_i(\\sigma(\\theta^Tx)-y_i)}\r\n\\]\r\nMinimizing Negative Log Likelihood\r\n\\[\r\n\\theta^*=argmin(l(x,y; \\theta) : \\: \\theta \\in \\Re^{d+1} )\r\n\\] ## The Data\r\nWe will start with some libraries and the simple data set we will be\r\nworking with for binary classification.\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nX,y = np.concatenate((np.random.randn(5000, 2), np.ones(5000).reshape(-1,1)), 1), np.random.binomial(1, 0.25, size=5000)\r\nN, D = X.shape\r\nX.shape, y.shape\r\n((5000, 3), (5000,))\r\n\r\nSigmoid\r\nNext, we will look at the sigmoid curve.\r\n\r\ndef sigmoid(z):\r\n    return 1/(1 + np.exp(-z))\r\ntheta_init = np.random.randn(3)\r\ntheta_init\r\narray([-0.43382384,  0.00477304, -0.42501194])\r\nlogits = sigmoid(X.dot(theta_init))\r\nlogits\r\narray([0.40473019, 0.51544891, 0.53547371, ..., 0.33712169, 0.40979105,\r\n       0.28786013])\r\ndef plot_logits(logits, title = \"logits=sigmoid(X.dot(theta))\"):\r\n    plt.scatter(range(len(logits)), sorted(logits), label = \"logits\")\r\n    plt.legend()\r\n    plt.title(title)\r\n    plt.show()\r\n    \r\nplt.clf()\r\nplot_logits(logits)\r\n\r\n\r\nNegative Log Likelihood\r\nNext, the log likelihood.\r\n\r\ndef negative_log_likelihood(logits, y):\r\n    errors = y * np.log(logits) + (1-y)*np.log(1-logits)\r\n    N = len(errors)\r\n    return -(1/N) * np.sum(errors) \r\n\r\nloss = negative_log_likelihood(logits, y)\r\nloss\r\n0.634010228209429\r\n\r\nFinally, the gradient of the log likelihood. Notice that the gradient\r\nis always a vector.\r\nGradient Log Likelihood\r\n\r\ndef gradient_log_likelihood(X, logits, y):\r\n    N, D = X.shape\r\n    grad_vec = (1/N)*X.T.dot(logits-y)\r\n    return grad_vec\r\n\r\ngradient_log_likelihood(X, logits, y)\r\narray([-0.09878357,  0.00720683,  0.14126207])\r\n\r\nGradient Descent\r\nNow we want to optimize. so we will build a gradient descent function\r\nto loop through our training set and converge on a solution. We will\r\nalso take a moment to visualize the logits.\r\n\r\n\r\ndef gradient_descent(f, grad_f, eps=0.01, eta=0.1, max_iter = 1000, **kwargs):\r\n  theta_init = np.random.randn(D)\r\n  thetas = [theta_init]\r\n  losses = []\r\n  for t in range(1,max_iter):\r\n      logits = kwargs[\"h\"](kwargs[\"X\"].dot(thetas[-1]))\r\n      \r\n      loss = f(logits, kwargs[\"y\"])\r\n      losses.append(loss)\r\n      \r\n      \r\n      #if t % 50 == 0:\r\n      #    print(\"Loss {}\".format(losses[-1]))\r\n          #plot_logits(logits)\r\n          #input(\"...\")\r\n      \r\n      grad_vec = grad_f(kwargs[\"X\"], logits, kwargs[\"y\"])\r\n      theta_t = thetas[t-1] - eta * grad_vec\r\n      thetas.append(theta_t)\r\n      \r\n      if np.sqrt(np.sum(np.square(thetas[-2] - thetas[-1]))) <= eps:\r\n        return thetas[-1], losses\r\n        break\r\n\r\n  return None\r\n  \r\n        \r\nfinal_theta, losses = gradient_descent(negative_log_likelihood, \r\n                                       gradient_log_likelihood, \r\n                                       eps = 0.001, \r\n                                       eta=0.01, \r\n                                       max_iter = 100000, \r\n                                       X=X, \r\n                                       h=sigmoid, \r\n                                       y=y)\r\nlogits = sigmoid(X.dot(final_theta))\r\nplt.clf()\r\nplot_logits(logits)\r\n\r\nplt.clf()\r\nplt.plot(losses)\r\nplt.title(\"Training losses\")\r\nplt.show()\r\n\r\n\r\nValidation\r\nThe initial training accuracy with null parameters is below. 49% with\r\na random guess on theta at the start.\r\n\r\ndef predict(logits, threshold = 0.5):\r\n    return (logits > threshold).astype(int)\r\n\r\nlogits = sigmoid(X.dot(theta_init))\r\ny_pred = predict(logits, threshold = 0.5)\r\n\r\nprint(\"Accuracy: {}\".format(np.mean(y_pred == y)))\r\nAccuracy: 0.666\r\n\r\nAfter some training, our final theta parameters now get 73%. Not too\r\nshabby!\r\n\r\ndef predict(logits, threshold = 0.5):\r\n    return (logits > threshold).astype(int)\r\n\r\nlogits = sigmoid(X.dot(final_theta))\r\ny_pred = predict(logits, threshold = 0.5)\r\n\r\nprint(\"Accuracy: {}\".format(np.mean(y_pred == y)))\r\nAccuracy: 0.742\r\n\r\nTest Data\r\nLet’s try this on some test data. We will just generate some more and\r\nsee how we do! It looks like we get about the same percentage on new\r\ndata, so that is a good indicator. We would expect this though, since\r\nthey are sampled directly from the signal population. One good study\r\nmight be to examine which data points are incorrect and where they fall\r\non the logistic curve. Were they really low values or really high?\r\nPerhaps they were right on the decision boundary and just couldn’t\r\ndecide. All these and others are good questions for a deep dive into\r\nmodel interpretation, which we will not get into now.\r\n\r\nXtest,ytest = np.concatenate((np.random.randn(5000, 2), np.ones(5000).reshape(-1,1)), 1), np.random.binomial(1, 0.25, size=5000)\r\n\r\nlogits = sigmoid(Xtest.dot(final_theta))\r\ny_pred = predict(logits, threshold = 0.5)\r\n\r\nprint(\"Test Accuracy: {}\".format(np.mean(y_pred == ytest)))\r\nTest Accuracy: 0.7614\r\n\r\nOptimal Decision Boundary\r\nRight now the decision boundary is 0.5 which is\r\ncustomary in the machine learning and statistical community. We will\r\nexplore what cutoff might be optimal by doing a\r\nrandom search through a bunch of possibilities from\r\n0 to 1. The decision boundary that eeked out some extra\r\nperformance for us is at around 0.68, or for the other\r\nengineers in the room, about 0.7. This tells us that there\r\nis about a 70% likelihood of predicting a 0\r\nclass label and about 30% likelihood of predicting a\r\n1. This is very interesting because our original data was\r\nsampled from a binomial distribution with a probability of success as\r\n25%. So the model, without being told, trained on a random\r\nparameter vector, after applying gradient descent, and through a bit of\r\nsearch optimization, was able to arrive at an estimate on the\r\npopulation’s yes prediction likelihood. Optimization is\r\npretty amazing!\r\n\r\ndef optimize_decision_boundary(logits, y, plot = False):\r\n    results = []\r\n    thresholds = []\r\n    for thresh in np.linspace(0, 1, 20):\r\n        y_pred = predict(logits, threshold = thresh)\r\n        results.append(np.mean(y_pred == y))\r\n        thresholds.append(thresh)\r\n    thresh_star = np.argmax(results)\r\n    \r\n    if plot:\r\n        plt.plot(results)\r\n        plt.suptitle(\"Decision Boundary Threshold Discovery\")\r\n        plt.title(\"Best Accuracy {} at index {} with value {}\".format(results[thresh_star], thresh_star, np.round(thresholds[thresh_star], 4)))\r\n        plt.axvline(x = thresh_star, c='r')\r\n        plt.axhline(y = results[thresh_star], c='r')\r\n        plt.show()\r\n        \r\n    return thresh_star, thresholds[thresh_star]\r\n\r\nplt.clf()\r\nthreshold = optimize_decision_boundary(logits, y, plot=True)\r\n\r\nthreshold\r\n(9, 0.47368421052631576)\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-13-logistic-regression-and-gradient-descent-in-python/logistic-regression-and-gradient-descent-in-python_files/figure-html5/unnamed-chunk-5-3.png",
    "last_modified": "2022-07-13T07:10:48-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-11-transportation-problem-in-r/",
    "title": "Transportation Problem in R",
    "description": "In this post we will learn how to solve supply and demand problems with the transportation problem and linear programming.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-12",
    "categories": [
      "network flows",
      "linear programming",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nMetropolitan Power Plant\r\nSupply\r\nThe Problem\r\nDescription\r\nMathematical\r\nFormulation\r\nData\r\nA Quick Visual\r\n\r\n\r\nLP Model Formulation\r\nObjective\r\nConstraints, Direction, and\r\nRHS\r\nSolution\r\nVisualization\r\n\r\n\r\nAn Easier Way\r\n\r\nMetropolitan Power Plant\r\nSupply\r\nThe Problem\r\nDescription\r\nOne of the most common daily tasks is that of transportation. From\r\nour bed to the dining room table, or from home to work and back, all of\r\nthese examples have limited supply some pre-determined\r\ndemand and an implicit cost in order\r\nto carry it out. The question is, what is the best way to do it? It\r\nwould be silly to travel to go home before you pick up dinner, then go\r\nback out again to do it; we seek to minimize our cost\r\nwhen transporting goods. This can be time, money, risk, or anything else\r\nthat matters.\r\nIn our example we will consider a power plant that needs to supply\r\nelectricity (or any resource for that matter) to a city. All of the\r\ncosts incurred to transport the electricity are known, these will be in\r\nUSD. Supply from each power plant is known. Demand\r\nto each city is also known. The problem is\r\nMathematical Formulation\r\nThe classical linear programming formulation goes as follows:\r\nObjective\r\n\\[\r\nMinimize. \\sum_{i,j}{x_{ij}c_{ij}}\r\n\\]\r\n\\[\r\ns.t.\r\n\\]\r\nSupply Constraints\r\n\\[\r\n\\sum_{j}{x_{ij}} \\le s_i \\: \\forall i \\in S\r\n\\]\r\nDemand Constraints\r\n\\[\r\n\\sum_{i}{x_{ij}} \\ge d_j \\: \\forall j \\in D\r\n\\]\r\nNon-negativity Constraints\r\n\\[\r\nx_{ij} \\ge 0 \\: \\forall (i,j)\\in A\r\n\\]\r\nData\r\nFirst we need to get our data. This will come in the form of an\r\nadjacency matrix which we will readily convert into an arc\r\nmatrix for reasons that will become clear soon (building constraints for\r\nthe LP).\r\n\r\n\r\n#Number of plants and cities\r\nNUM_POWER_PLANTS = 3\r\nNUM_CITIES = 4\r\n\r\n# Adjacency matrix\r\nadj.matrix = matrix(c(8, 6, 10, 9, 9, 12, 13, 7, 14, 9, 16, 5), \r\n                    nrow = NUM_POWER_PLANTS, \r\n                    ncol = NUM_CITIES)\r\nadj.matrix\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]    8    9   13    9\r\n[2,]    6    9    7   16\r\n[3,]   10   12   14    5\r\n\r\nNext we need to take a look at our supply and demand.\r\n\r\n\r\n# Supply and demand vectors\r\nsupply <- c(35, 50, 40)\r\ndemand <- c(45, 20, 30, 30)\r\nsupply\r\n\r\n[1] 35 50 40\r\n\r\ndemand\r\n\r\n[1] 45 20 30 30\r\n\r\nNext define the Power Plants and Cities.\r\n\r\n\r\n# Supply node id lookup\r\nPOWER_PLANTS = 1:3\r\nPOWER_PLANT_LABELS <- sprintf(\"Plant %s\", 1:3)\r\n\r\n# Demand node id lookup\r\nCITIES = 1:4\r\nCITIES_LABELS <- sprintf(\"City %s\", 1:4)\r\n\r\nPOWER_PLANT_LABELS\r\n\r\n[1] \"Plant 1\" \"Plant 2\" \"Plant 3\"\r\n\r\nCITIES_LABELS\r\n\r\n[1] \"City 1\" \"City 2\" \"City 3\" \"City 4\"\r\n\r\nFor the nodes we will supply them with some metatdata into the\r\nDiagrammeR package for slick plotting capability.\r\n\r\n\r\n# Create the powerplant nodes\r\npowerplant_nodes <- DiagrammeR::create_node_df(nodes = POWER_PLANTS,\r\n                                  label = POWER_PLANT_LABELS,\r\n                                  style = \"filled\",\r\n                                  type = \"upper\",\r\n                                  color = rep(\"green\", length(POWER_PLANTS)),\r\n                                  shape = rep(\"circle\", length(POWER_PLANTS)),\r\n                                  data = supply,\r\n                                  n = length(POWER_PLANTS)\r\n                                  )\r\n\r\n# Create the city nodes\r\ncity_nodes <- DiagrammeR::create_node_df(nodes = CITIES,\r\n                                  label = CITIES_LABELS,\r\n                                  style = \"filled\",\r\n                                  type = \"lower\",\r\n                                  color = rep(\"red\", length(CITIES)),\r\n                                  shape = rep(\"square\", length(CITIES)),\r\n                                  data = demand,\r\n                                  n = length(CITIES)\r\n                                  )\r\n\r\n# Create the DiagrammeR dataframe\r\nnodes <- DiagrammeR::combine_ndfs(powerplant_nodes, city_nodes)\r\nnodes\r\n\r\n  id  type   label nodes  style color  shape data\r\n1  1 upper Plant 1     1 filled green circle   35\r\n2  2 upper Plant 2     2 filled green circle   50\r\n3  3 upper Plant 3     3 filled green circle   40\r\n4  4 lower  City 1     1 filled   red square   45\r\n5  5 lower  City 2     2 filled   red square   20\r\n6  6 lower  City 3     3 filled   red square   30\r\n7  7 lower  City 4     4 filled   red square   30\r\n\r\nOne way to convert the adjacency matrix into a\r\nfriendlier edge format is to simply gather it with some\r\nbasic naming of the rows and columns. Once we have the\r\ntidy_edges we can acquire the node_id for each\r\nby doing a little joining with the nodes table formed\r\nabove. This will supply the id's required for the\r\nDiagrammeR package.\r\n\r\n\r\n# Create the \"from\" \"to\" representation, from the raw matrix form\r\ntmp <- adj.matrix %>% as.data.frame\r\nnames(tmp) <- CITIES_LABELS\r\ntmp$from <- POWER_PLANT_LABELS\r\ntidy_edges <- tmp %>% \r\n                tidyr::gather(., key = \"to\", value = \"data\", -c(from)) %>%\r\n                dplyr::mutate(color = \"black\", rel = \"requires\")\r\n\r\n# Go find the from_id\r\nfrom_id = tidy_edges %>% \r\n            dplyr::inner_join(nodes, by = c(\"from\"=\"label\")) %>% \r\n            dplyr::transmute(from_id = id) %>%\r\n            dplyr::pull(from_id)\r\n\r\n# Go find the to_id\r\nto_id = tidy_edges %>% \r\n            dplyr::inner_join(nodes, by = c(\"to\"=\"label\")) %>% \r\n            dplyr::transmute(to_id = id) %>% \r\n            dplyr::pull(to_id)\r\n\r\n\r\n# Get the from_id\r\ntidy_edges$from_id = from_id\r\n\r\n# Get the to_id\r\ntidy_edges$to_id = to_id\r\n\r\nedges <- DiagrammeR::create_edge_df(from = from_id,\r\n                                    to = to_id,\r\n                                    rel = tidy_edges$rel,\r\n                                    color = tidy_edges$color,\r\n                                    data = tidy_edges$data,\r\n                                    label = tidy_edges$data)\r\n\r\n\r\n# Always a good idea to short your edges logically\r\nedges <- edges %>% \r\n            dplyr::arrange(-desc(from), -desc(to)) %>% \r\n            dplyr::mutate(id = row_number())\r\nedges\r\n\r\n   id from to      rel color data label\r\n1   1    1  4 requires black    8     8\r\n2   2    1  5 requires black    9     9\r\n3   3    1  6 requires black   13    13\r\n4   4    1  7 requires black    9     9\r\n5   5    2  4 requires black    6     6\r\n6   6    2  5 requires black    9     9\r\n7   7    2  6 requires black    7     7\r\n8   8    2  7 requires black   16    16\r\n9   9    3  4 requires black   10    10\r\n10 10    3  5 requires black   12    12\r\n11 11    3  6 requires black   14    14\r\n12 12    3  7 requires black    5     5\r\n\r\nA Quick Visual\r\n\r\n\r\n# Create the graph\r\ng = DiagrammeR::create_graph(nodes_df = nodes,\r\n                             edges_df = edges, \r\n                             directed = T, \r\n                             graph_name = \"transportation\")\r\n\r\n# Render the graph\r\nDiagrammeR::render_graph(g, title = \"Power Plants Electricity Transportation to Cities\")\r\n\r\n\r\n\r\nAnother way to visualize this is by converting it to an igraph, which\r\nlooks pretty horrible.\r\n\r\n\r\nig <- DiagrammeR::to_igraph(g)\r\nplot(ig, vertex.size=30)\r\n\r\n\r\n\r\nLP Model Formulation\r\nObjective\r\nFirst things first, we know the costs on each arc already, because\r\nthey are supplied from our adjacency matrix initially. So\r\nwe can just go recover those.\r\n\r\n\r\n# Get objective value\r\nf.obj = edges %>% dplyr::pull(data)\r\nf.obj\r\n\r\n [1]  8  9 13  9  6  9  7 16 10 12 14  5\r\n\r\nConstraints, Direction, and\r\nRHS\r\nFirst some convenient numbering systems will be helpful to preserve\r\nknowledge of the nodes and which id is which.\r\n\r\n\r\n# Align ids up for matrix building\r\nPOWER_PLANT_NODE_IDS <- POWER_PLANTS\r\nCITY_NODE_IDS <- POWER_PLANTS[length(POWER_PLANTS)] + CITIES\r\n\r\nPOWER_PLANT_NODE_IDS\r\n\r\n[1] 1 2 3\r\n\r\nCITY_NODE_IDS\r\n\r\n[1] 4 5 6 7\r\n\r\nNext we need to build the supply constraints. So for each unique\r\npowerplant arc, then sum of it’s outbound arcs must be less than the\r\nsupply constraint, for all supply.\r\n\r\n\r\n# One constraint row for each supply node\r\nSmat = matrix(0, nrow = length(POWER_PLANTS), ncol = nrow(edges))\r\nSmat_rhs = supply\r\nSmat_dir = rep(\"<=\", length(supply))\r\n\r\nfor(i in 1:nrow(Smat)){\r\n  for(j in 1:ncol(Smat)){\r\n    from = edges$from[j]\r\n    to = edges$to[j]\r\n    \r\n    if(from == POWER_PLANT_NODE_IDS[i]){\r\n      Smat[i, j] = 1\r\n    }\r\n  }\r\n}\r\n\r\nSmat\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\r\n[1,]    1    1    1    1    0    0    0    0    0     0     0     0\r\n[2,]    0    0    0    0    1    1    1    1    0     0     0     0\r\n[3,]    0    0    0    0    0    0    0    0    1     1     1     1\r\n\r\nSmat_rhs\r\n\r\n[1] 35 50 40\r\n\r\nSmat_dir\r\n\r\n[1] \"<=\" \"<=\" \"<=\"\r\n\r\nWith the same logic we need to construct the demand matrix.\r\n\r\n\r\n# One constraint node for each demand node\r\nDmat = matrix(0, nrow=length(CITIES), ncol = nrow(edges))\r\nDmat_rhs = demand\r\nDmat_dir = rep(\">=\", length(demand))\r\n\r\nfor(i in 1:nrow(Dmat)){\r\n  for(j in 1:ncol(Dmat)){\r\n    from = edges$from[j]\r\n    to = edges$to[j]\r\n    \r\n    if(to == CITY_NODE_IDS[i]){\r\n      Dmat[i, j] = 1\r\n    }\r\n  }\r\n}\r\n\r\nDmat\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\r\n[1,]    1    0    0    0    1    0    0    0    1     0     0     0\r\n[2,]    0    1    0    0    0    1    0    0    0     1     0     0\r\n[3,]    0    0    1    0    0    0    1    0    0     0     1     0\r\n[4,]    0    0    0    1    0    0    0    1    0     0     0     1\r\n\r\nDmat_rhs\r\n\r\n[1] 45 20 30 30\r\n\r\nDmat_dir\r\n\r\n[1] \">=\" \">=\" \">=\" \">=\"\r\n\r\nNow that we have our supply and demand data matrices put together,\r\nlet’s unify them and take a look at all of our constraints.\r\n\r\n\r\n# All `lpSolve` data\r\nf.obj <- f.obj\r\nf.cons <- rbind(Smat, Dmat)\r\nf.rhs <- c(Smat_rhs, Dmat_rhs)\r\nf.dir <- c(Smat_dir, Dmat_dir)\r\n\r\nf.obj\r\n\r\n [1]  8  9 13  9  6  9  7 16 10 12 14  5\r\n\r\nf.cons\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\r\n[1,]    1    1    1    1    0    0    0    0    0     0     0     0\r\n[2,]    0    0    0    0    1    1    1    1    0     0     0     0\r\n[3,]    0    0    0    0    0    0    0    0    1     1     1     1\r\n[4,]    1    0    0    0    1    0    0    0    1     0     0     0\r\n[5,]    0    1    0    0    0    1    0    0    0     1     0     0\r\n[6,]    0    0    1    0    0    0    1    0    0     0     1     0\r\n[7,]    0    0    0    1    0    0    0    1    0     0     0     1\r\n\r\nf.rhs\r\n\r\n[1] 35 50 40 45 20 30 30\r\n\r\nf.dir\r\n\r\n[1] \"<=\" \"<=\" \"<=\" \">=\" \">=\" \">=\" \">=\"\r\n\r\nSolution\r\nNow we unite everything together and drop it into the solver.\r\nRemember, we are trying to minimize cost in our objective, the\r\nconstraints will maximize the flow. So specify min.\r\n\r\n\r\n# Get the results\r\nresults <- lp(direction = \"min\",  \r\n              objective.in = f.obj, \r\n              const.mat = f.cons, \r\n              const.dir = f.dir, \r\n              const.rhs = f.rhs)\r\n\r\nresults$objval\r\n\r\n[1] 880\r\n\r\nmatrix(results$solution, nrow = length(supply), ncol = length(demand))\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]   15    0   30    0\r\n[2,]   20   20    0    0\r\n[3,]    0    0   10   30\r\n\r\nVisualization\r\nCool! We have our solution. It looks like:\r\nPlant 1 should supply City 1 and\r\nCity 2.\r\nPlant 2 should supply City 1 and\r\nCity 3, and\r\nPlant 3 should supply City 1 and\r\nCity 4.\r\nOne interesting insight is the shared responsibility all plants have\r\nin getting resources to City 1. Another insight is the\r\npartnerships to make City 1 supplied as cheaply as\r\npossible, may require some additional coordination costs to\r\ncarefully handle any error in costs between ill-shipments. The last\r\ninsights are the siloe’s, City 2 is entirely handled by\r\nPlant 1, City 3 by Plant 2, and\r\nCity 4 by Plant 3. This shows us the reliance\r\nwe have on those energy transfers going through, otherwise it’s not only\r\nlights out, it is also a more expensive transfer to get them back\r\non!\r\n\r\n\r\n# Add in the flow\r\nedges$flow <- results$solution\r\n\r\n# Color things if there exists flow\r\nedges <- edges %>% dplyr::mutate(weight = flow,\r\n                                 label = flow,\r\n                                 color = if_else(condition = flow > 0, true = \"black\",false = \"grey\"))\r\n\r\n# Create the graph\r\ng = DiagrammeR::create_graph(nodes_df = nodes,\r\n                             edges_df = edges, \r\n                             directed = T, \r\n                             graph_name = \"transportation\")\r\n\r\n# Draw the graph\r\nDiagrammeR::render_graph(g, title = \"Power Plants Electricity Transportation to Cities\")\r\n\r\n\r\n\r\n\r\n\r\n# ig <- DiagrammeR::to_igraph(g)\r\n\r\nnodes\r\n\r\n  id  type   label nodes  style color  shape data\r\n1  1 upper Plant 1     1 filled green circle   35\r\n2  2 upper Plant 2     2 filled green circle   50\r\n3  3 upper Plant 3     3 filled green circle   40\r\n4  4 lower  City 1     1 filled   red square   45\r\n5  5 lower  City 2     2 filled   red square   20\r\n6  6 lower  City 3     3 filled   red square   30\r\n7  7 lower  City 4     4 filled   red square   30\r\n\r\nvisNet_nodes <- nodes %>% \r\n                dplyr::mutate(shape = \"circle\",\r\n                              font = 12,\r\n                              size = data,)\r\nedges\r\n\r\n   id from to      rel color data label flow weight\r\n1   1    1  4 requires black    8    15   15     15\r\n2   2    1  5 requires black    9    20   20     20\r\n3   3    1  6 requires  grey   13     0    0      0\r\n4   4    1  7 requires  grey    9     0    0      0\r\n5   5    2  4 requires black    6    20   20     20\r\n6   6    2  5 requires  grey    9     0    0      0\r\n7   7    2  6 requires black    7    30   30     30\r\n8   8    2  7 requires  grey   16     0    0      0\r\n9   9    3  4 requires black   10    10   10     10\r\n10 10    3  5 requires  grey   12     0    0      0\r\n11 11    3  6 requires  grey   14     0    0      0\r\n12 12    3  7 requires black    5    30   30     30\r\n\r\nvisNet_edges <- edges %>% \r\n                dplyr::mutate(label = paste(\"(\",flow,\",\",data,\")\",sep=\"\"),\r\n                              length = 250,\r\n                              width = flow/5,\r\n                              arrows = \"to\") \r\n  \r\nvisNetwork(visNet_nodes, \r\n           visNet_edges,\r\n           width = \"100%\",\r\n           main = \"Power Distribution | (flow, cost)\")\r\n\r\n\r\n\r\nAn Easier Way\r\nIt’s almost spooky how easy it is with the lp.transport\r\nAPI. Verify that we got the same solution, 880 and the\r\ndecision vector is also the same. Since the objective value is the same,\r\nthe solutions might differ, because they are equally optimal. Pretty\r\ninteresting to see how easy it is with the convenience of\r\nlpSolve though!\r\n\r\n\r\n# Get the results\r\nresults <- lpSolve::lp.transport(cost.mat = adj.matrix, \r\n                                  direction = \"min\", \r\n                                  row.signs = rep(\"<=\", nrow(adj.matrix)), \r\n                                  col.signs = rep(\">=\", ncol(adj.matrix)),\r\n                                  row.rhs = supply, \r\n                                  col.rhs = demand)\r\n\r\n# Verify `objval` is the same as above\r\nresults$objval\r\n\r\n[1] 880\r\n\r\nresults$solution\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]   15   20    0    0\r\n[2,]   20    0   30    0\r\n[3,]   10    0    0   30\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-11-transportation-problem-in-r/transportation-problem-in-r_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-07-13T06:07:51-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-10-minimum-cost-network-flow-problem-in-r/",
    "title": "Minimum Cost Network Flow Problem (MCNFP) in R",
    "description": "In this post we will walk through how to make least cost maximum flow decisions using linear programming.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-10",
    "categories": [
      "network flows",
      "linear programming",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nTraffic Minimum Cost\r\nNetwork Flow\r\nThe Problem\r\nThe Data\r\nNetwork Visualization\r\n\r\nModel Data\r\nAverage In-Flow\r\nDemand\r\n\r\n\r\nLinear Programming (LP)\r\nLP Structure\r\nArc Constraints\r\nPutting It All Together\r\n\r\nSolve the LP\r\nVisualize Solution\r\nCleaning Up The Visual\r\nFlow vs. Capacity\r\nvs. Time\r\nShortest Path\r\nShortest Path\r\nVisualization\r\n\r\n\r\nReusable Functionality\r\nSourcing\r\n\r\nTraffic Minimum Cost Network\r\nFlow\r\nOne common extension to Maximum Flow problems is to do\r\nit as cheaply as possible. In this article we will extend the maximum\r\nflow example we wrote on in the last\r\npost and include a minimum cost component. The generic problem\r\nformation is below:\r\nObjective \\[\r\nMinimize. \\sum_{(i,j)\\in A}{c_{ij}x_{ij}}   \r\n\\] \\[\r\ns.t.\r\n\\] Node Flow Constraints \\[\r\n\\sum_{j}{x_{ij}} - \\sum_{i}{x_{ji}} = b_i \\: \\forall i \\in N\r\n\\] Arc Flow Constraints \\[\r\nl_{ij} \\le x_{ij} \\le u_{ij} \\: \\forall (i,j) \\in A\r\n\\]\r\nThe Problem\r\nRoad networks are everywhere in our society. In any given\r\nintersection there is a flow of cars, intersections with stop lights,\r\nand connections between each. Every road has a feasible limit it can\r\nsupport. In fact, this is often the cause of most congestion. Our goal\r\nis to minimize the total time required for all cars to travel from node\r\n1 to node 6 in a fictitious road network.\r\nThe Data\r\n\r\n\r\nnodes <- data.frame(id = c(1:6), color = c(\"green\", rep(\"grey\", 4), \"red\"))\r\nedges <- data.frame(from = c(1, 1, 2, 2, 5, 4, 4, 3, 3), \r\n                    to = c(2, 3, 5, 4, 6, 5, 6, 5, 4),\r\n                    lower_bound = 0,\r\n                    upper_bound = c(800, 600, 100, 600, 600, 600, 400, 400, 300),\r\n                    time = c(10, 50, 70, 30, 30, 30, 60, 60, 10), # in minutes\r\n                    flow = 0, #TBD\r\n                    color = \"grey\") %>% dplyr::arrange(from, to)\r\nnodes\r\n\r\n  id color\r\n1  1 green\r\n2  2  grey\r\n3  3  grey\r\n4  4  grey\r\n5  5  grey\r\n6  6   red\r\n\r\nedges\r\n\r\n  from to lower_bound upper_bound time flow color\r\n1    1  2           0         800   10    0  grey\r\n2    1  3           0         600   50    0  grey\r\n3    2  4           0         600   30    0  grey\r\n4    2  5           0         100   70    0  grey\r\n5    3  4           0         300   10    0  grey\r\n6    3  5           0         400   60    0  grey\r\n7    4  5           0         600   30    0  grey\r\n8    4  6           0         400   60    0  grey\r\n9    5  6           0         600   30    0  grey\r\n\r\nNetwork Visualization\r\nWe can see the upper bounds plotted on the edges of this\r\ntransportation network below. The width indicates more capacity for\r\nflow. Examine the trade-off between time and space for travel between\r\narc (1,2).\r\n\r\n\r\ng <- igraph::graph_from_data_frame(d = edges,\r\n                                   directed = TRUE,\r\n                                   vertices = nodes)\r\n\r\n# Capacity plot\r\nplot(g, \r\n     main = \"Vehicle Transportion Network Capacities\", \r\n     edge.label = E(g)$upper_bound, \r\n     edge.width = E(g)$upper_bound/150,\r\n     vertex.color = V(g)$color,\r\n     edge.color = E(g)$color)\r\n\r\n\r\n# Time plot\r\nplot(g, \r\n     main = \"Vehicle Transportion Network Travel Time\", \r\n     edge.label = E(g)$time, \r\n     edge.width = E(g)$time/10,\r\n     vertex.color = V(g)$color,\r\n     edge.color = E(g)$color)\r\n\r\n\r\n\r\nModel Data\r\nAverage In-Flow\r\nFirst we assume the average number of cars that flow through this\r\nnetwork. This is often recovered from past databases that record the\r\nflows through the network.\r\n\r\n\r\nAVERAGE_FLOW <- 900 # per hour\r\nAVERAGE_FLOW\r\n\r\n[1] 900\r\n\r\nDemand\r\nNext we set up the demand that will be flowing through the network.\r\nThis is indicated as the vector b in our model formation\r\nabove. This means the initial node has a supply of 900 vehicles, while\r\nthe final node has a demand of 900 nodes. The objective is to flow as\r\nmany vehicles through the network, in the shortest amount of time.\r\n\r\n\r\ndemand <- c(AVERAGE_FLOW, rep(0, nrow(nodes)-2), -AVERAGE_FLOW)\r\ndemand\r\n\r\n[1]  900    0    0    0    0 -900\r\n\r\nLinear Programming (LP)\r\nLP Structure\r\nThe next step is to set up the optimization. Let’s do that now. There\r\nare 3 key ingredients.\r\nObjective\r\nConstraints\r\nDirections\r\nArc Constraints\r\nWe want to build the constraint matrix, which has 2 parts.\r\nArc Capacity Constraints\r\nNode Flow Constraints\r\nThe Arc Capacity Constraints are the first we\r\naddress.\r\nThe Amat, or A matrix, is the arc matrix which contains\r\nthe upper bounds. Since linear programming relies on the resource\r\nmatrix, we need one row for each arc, our dimensions for variable flow\r\nselection are the number of arcs also. So this means we need an\r\nidentity matrix for the rows of arcs and columns of\r\narcs.\r\n\r\n\r\ncreate_upper_arc_constraints <- function(edges){\r\n  Amat <- matrix(0, nrow = nrow(edges), ncol = nrow(edges))\r\n  Amat_dir <- rep(\"<=\", nrow(Amat))\r\n  Amat_rhs <- c()\r\n\r\n  for(i in 1:ncol(Amat)){\r\n    Amat[i,i] <- 1\r\n    Amat_rhs <- c(Amat_rhs, edges$upper_bound[i])\r\n  }\r\n  \r\n  list(Amat_upper = Amat,\r\n       Amat_upper_dir = Amat_dir,\r\n       Amat_upper_rhs = Amat_rhs)\r\n}\r\n\r\n# This could be higher than zero, but for standard LP this is the default configuration, so not needed.\r\n# create_lower_arc_constraints <- function(edges){\r\n#   Amat <- matrix(0, nrow = nrow(edges), ncol = nrow(edges))\r\n#   Amat_dir <- rep(\">=\", nrow(Amat))\r\n#   Amat_rhs <- c()\r\n# \r\n#   for(i in 1:ncol(Amat)){\r\n#     Amat[i,i] <- 1\r\n#     Amat_rhs <- c(Amat_rhs, edges$lower_bound[i])\r\n#   }\r\n#   \r\n#   list(Amat_lower = Amat,\r\n#        Amat_lower_dir = Amat_dir,\r\n#        Amat_lower_rhs = Amat_rhs)\r\n# }\r\n\r\nupper_results <- create_upper_arc_constraints(edges)\r\n# lower_results <- create_lower_arc_constraints(edges)\r\n# \r\n# Amat <- rbind(upper_results$Amat_upper, lower_results$Amat_lower)\r\n# Amat_dir <- c(upper_results$Amat_upper_dir, lower_results$Amat_lower_dir)\r\n# Amat_rhs <- c(upper_results$Amat_upper_rhs, lower_results$Amat_lower_rhs)\r\n\r\nAmat <- upper_results$Amat_upper\r\nAmat_dir <- upper_results$Amat_upper_dir\r\nAmat_rhs <- upper_results$Amat_upper_rhs\r\n\r\nAmat\r\n\r\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\r\n [1,]    1    0    0    0    0    0    0    0    0\r\n [2,]    0    1    0    0    0    0    0    0    0\r\n [3,]    0    0    1    0    0    0    0    0    0\r\n [4,]    0    0    0    1    0    0    0    0    0\r\n [5,]    0    0    0    0    1    0    0    0    0\r\n [6,]    0    0    0    0    0    1    0    0    0\r\n [7,]    0    0    0    0    0    0    1    0    0\r\n [8,]    0    0    0    0    0    0    0    1    0\r\n [9,]    0    0    0    0    0    0    0    0    1\r\n\r\nAmat_dir\r\n\r\n[1] \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\"\r\n\r\nAmat_rhs\r\n\r\n[1] 800 600 600 100 300 400 600 400 600\r\n\r\nThe Node Flow Constraints are the next to take care\r\nof.\r\nThe Bmat, or B matrix, is a matrix which contains the\r\nnode balance equations codified by flows.\r\nFor each node, we need to match it’s from node and\r\nto node with the appropriate inflow and outflow. If it\r\nmatches an inflow arc, this is increase, so 1\r\nis in the arc column. If it matches an outflow arch, this\r\nis decrease, so -1 is in the arc column. Otherwise\r\n0 remains as the placeholder. The sign here is\r\n== because they must match (i.e., supply = demand). If we\r\nrequire excess at certain points we can set this demand to be higher\r\nthan zero, but we will not do that here.\r\n\r\n\r\ncreate_node_constraints <- function(nodes, edges){\r\n    Bmat <- matrix(0, nrow = nrow(nodes), ncol = nrow(edges))\r\n    Bmat_dir <- rep(\"==\", nrow(Bmat))\r\n    Bmat_rhs <- rep(0, nrow(Bmat))\r\n    \r\n    for(i in 1:nrow(Bmat)){\r\n      node_id <- nodes[i, \"id\"]\r\n      for(j in 1:ncol(Bmat)){\r\n        edge_from <- edges[j,\"from\"]\r\n        edge_to <- edges[j, \"to\"]\r\n        \r\n        if(node_id == edge_from){\r\n          # print(paste(\"Outbound for \", node_id, \"From: \",edge_from, \"to: \", edge_to))\r\n          Bmat[i,j] = 1\r\n        }\r\n        else if(node_id == edge_to){\r\n          # print(paste(\"Inbound for \", node_id, \"From: \",edge_from, \"to: \", edge_to))\r\n          Bmat[i,j] = -1\r\n        }\r\n        else{\r\n          Bmat[i,j] = 0\r\n        }\r\n      }\r\n      Bmat_rhs[i] <- demand[i]\r\n    }\r\n    \r\n    list(Bmat = Bmat,\r\n         Bmat_dir = Bmat_dir,\r\n         Bmat_rhs = Bmat_rhs\r\n    )\r\n}\r\n\r\nresults <- create_node_constraints(nodes, edges)\r\nBmat <- results$Bmat\r\nBmat_dir <- results$Bmat_dir\r\nBmat_rhs <- results$Bmat_rhs\r\n\r\nBmat\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\r\n[1,]    1    1    0    0    0    0    0    0    0\r\n[2,]   -1    0    1    1    0    0    0    0    0\r\n[3,]    0   -1    0    0    1    1    0    0    0\r\n[4,]    0    0   -1    0   -1    0    1    1    0\r\n[5,]    0    0    0   -1    0   -1   -1    0    1\r\n[6,]    0    0    0    0    0    0    0   -1   -1\r\n\r\nBmat_dir\r\n\r\n[1] \"==\" \"==\" \"==\" \"==\" \"==\" \"==\"\r\n\r\nBmat_rhs\r\n\r\n[1]  900    0    0    0    0 -900\r\n\r\nPutting It All Together\r\nNext, the objective is going to be the time. This is the cost we have\r\nto pay for assigning flow to an arc. Let’s take a look at everything all\r\ntogether.\r\n\r\n\r\nf.obj <- edges %>% dplyr::pull(time)\r\nf.cons <- rbind(Amat, Bmat)\r\nf.rhs <- c(Amat_rhs, Bmat_rhs)\r\nf.dir <- c(Amat_dir, Bmat_dir)\r\n\r\nf.obj\r\n\r\n[1] 10 50 30 70 10 60 30 60 30\r\n\r\nf.cons\r\n\r\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\r\n [1,]    1    0    0    0    0    0    0    0    0\r\n [2,]    0    1    0    0    0    0    0    0    0\r\n [3,]    0    0    1    0    0    0    0    0    0\r\n [4,]    0    0    0    1    0    0    0    0    0\r\n [5,]    0    0    0    0    1    0    0    0    0\r\n [6,]    0    0    0    0    0    1    0    0    0\r\n [7,]    0    0    0    0    0    0    1    0    0\r\n [8,]    0    0    0    0    0    0    0    1    0\r\n [9,]    0    0    0    0    0    0    0    0    1\r\n[10,]    1    1    0    0    0    0    0    0    0\r\n[11,]   -1    0    1    1    0    0    0    0    0\r\n[12,]    0   -1    0    0    1    1    0    0    0\r\n[13,]    0    0   -1    0   -1    0    1    1    0\r\n[14,]    0    0    0   -1    0   -1   -1    0    1\r\n[15,]    0    0    0    0    0    0    0   -1   -1\r\n\r\nf.rhs\r\n\r\n [1]  800  600  600  100  300  400  600  400  600  900    0    0    0\r\n[14]    0 -900\r\n\r\nf.dir\r\n\r\n [1] \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"==\" \"==\" \"==\" \"==\"\r\n[14] \"==\" \"==\"\r\n\r\nSolve the LP\r\nNow we unite everything together and drop it into the solver.\r\nRemember, we are trying to minimize cost in our objective, the\r\nconstraints will maximize the flow. So specify min.\r\n\r\n\r\nresults <- lp(direction = \"min\",  \r\n              objective.in = f.obj, \r\n              const.mat = f.cons, \r\n              const.dir = f.dir, \r\n              const.rhs = f.rhs)\r\n\r\nedges$flow <- results$solution\r\nedges\r\n\r\n  from to lower_bound upper_bound time flow color\r\n1    1  2           0         800   10  700  grey\r\n2    1  3           0         600   50  200  grey\r\n3    2  4           0         600   30  600  grey\r\n4    2  5           0         100   70  100  grey\r\n5    3  4           0         300   10  200  grey\r\n6    3  5           0         400   60    0  grey\r\n7    4  5           0         600   30  500  grey\r\n8    4  6           0         400   60  300  grey\r\n9    5  6           0         600   30  600  grey\r\n\r\nVisualize Solution\r\nNow that we have our flow we can do some visualizing and analysis.\r\nThere are two key graphics to examine; the\r\nflow vs. capacity and the flow vs. time.\r\nFirst, the flow vs. capacity will give us insights into\r\nstress on the network. This is because of their implicit advantage they\r\nsupply to the optimizer, maximum flows, so naturally, these get flooded\r\nwith traffic. Second, the flow vs. time will give us\r\ninsights into shortest distance paths (i.e., assuming time is\r\nproportional to distance, which is not always the case). This is because\r\npaths with shorter times will enable more to flow through in any given\r\ntime delta. Between these two visuals, a good assessment of the model\r\noutput is feasible.\r\n\r\n\r\n# Set the flow\r\nedges$flow <- results$solution\r\n\r\n# If the arc is flowing oil, change to black\r\nedges$color[which(edges$flow > 0)] <- \"black\"\r\n\r\n# If the arc is at capacity change it to red\r\nedges$color[which(edges$flow == edges$upper_bound)] <- \"red\"\r\n\r\ng <- igraph::graph_from_data_frame(d = edges,\r\n                                   directed = TRUE,\r\n                                   vertices = nodes)\r\n\r\n# Plot the flow and capacity\r\nplot(g,\r\n     edge.label = paste(\"(\",E(g)$flow, \r\n                        \",\",\r\n                         E(g)$upper_bound, \r\n                         \")\", \r\n                        sep=\"\"), \r\n     main = \"Traffic Flow | (flow, capacity)\", \r\n     vertex.color = V(g)$color,\r\n     vertex.size = 25,\r\n     vertex.label.cex = 1.6,\r\n     edge.color = E(g)$color,\r\n     edge.width = E(g)$flow/200)\r\n\r\n\r\n# Plot the time\r\nplot(g,\r\n     edge.label = paste(\"(\",E(g)$flow, \r\n                        \",\",\r\n                         E(g)$time, \r\n                         \")\", \r\n                        sep=\"\"), \r\n     main = \"Traffic Flow | (flow, time)\", \r\n     vertex.color = V(g)$color,\r\n     vertex.size = 25,\r\n     vertex.label.cex = 1.6,\r\n     edge.color = E(g)$color,\r\n     edge.width = E(g)$flow/200)\r\n\r\n\r\n\r\nCleaning Up The Visual\r\n\r\n\r\nnodes\r\n\r\n  id color\r\n1  1 green\r\n2  2  grey\r\n3  3  grey\r\n4  4  grey\r\n5  5  grey\r\n6  6   red\r\n\r\nvisNet_nodes <- nodes %>% \r\n                dplyr::mutate(label = paste(\"Location \", id),\r\n                              font.size =10,\r\n                              type = \"black\",\r\n                              shape = \"circle\",\r\n                              font = 12)\r\n\r\nvisNet_edges <- edges %>% \r\n                dplyr::mutate(label = paste(\"(\",flow,\",\",upper_bound,\")\",sep=\"\"),\r\n                              length = 250,\r\n                              width = flow/100,\r\n                              arrows = \"to\") \r\n  \r\nvisNetwork(visNet_nodes, \r\n           visNet_edges,\r\n           width = \"100%\",\r\n           main = \"Least Time Maximum Vehicle Flow | (flow, capacity)\")\r\n\r\n\r\n\r\nFlow vs. Capacity vs. Time\r\nTwo of the most popular roadways are\r\n2 -> 4 and 5 -> 6. These supply the\r\nleast amount of time and the most amount of capacity. Without a doubt,\r\nthe model has exploited these as much as possible. The only trick is,\r\njust how the flow gets there. We see up front that 1 ships\r\noff a 200 to 700 flow split from the 900\r\nsupply with the heavier allocation toward 2. Why?\r\n2 is connected to a popular\r\nroadway, meaning much more potential to flow (and\r\nquickly).\r\nIn order to get to 4, going through 3\r\nwill cost much more time (60 oppose to 40).\r\n3 has two outlets, but one is one of the worst\r\nroutes on the network due to it’s 60 minute trek, so it doesn’t even get\r\nany flow allocated.\r\nShortest Path\r\nThe shortest path shows one very interesting insight to this model;\r\nsending a maximum flow through the network is not all about time. The\r\nshortest path (least time) is the sequence\r\n1 -> 2 -> 4 -> 6. However, from the above, we see\r\nthat this isn’t the most stressed path. Why? We aren’t only\r\ninterested in short times for the vehicles flowing through the network.\r\nWe are also interested in getting them all through it! We assumed there\r\nwas a 900 average vehicle flow, and having a macro-level view of this\r\nsystem, sending them all down the shortest path would not solve it (that\r\nis, we would not send as much as possible, only as cheaply as possible;\r\nwe could have pushed more). In order to get the most cars sent through\r\nthe network, in the shortest amount of time we also must take advantage\r\nof the popular roadways that the model is straining (or\r\nadd incentive to the not so popular roadways with\r\ngreater capacity or shorter commute times).\r\n\r\n\r\nshortest.paths <- igraph::shortest_paths(graph = g, from = 1, to = 6)\r\ns_path <- shortest.paths$vpath[[1]]\r\ns_path\r\n\r\n+ 4/6 vertices, named, from ac6f2e8:\r\n[1] 1 2 4 6\r\n\r\nshortest_commute_time <- E(g, path = s_path)$time %>% sum\r\nshortest_commute_time\r\n\r\n[1] 100\r\n\r\nShortest Path Visualization\r\n\r\n\r\nE(g)$color <- \"black\"\r\nE(g, path = s_path)$color <- \"blue\"\r\n\r\nplot(g,\r\n     edge.label = paste(\"(\",E(g)$flow, \r\n                        \",\",\r\n                         E(g)$time, \r\n                         \")\", \r\n                        sep=\"\"), \r\n     main = \"Shortest Path | (flow, capacity)\", \r\n     vertex.color = V(g)$color,\r\n     vertex.size = 25,\r\n     vertex.label.cex = 1.6,\r\n     edge.color = E(g)$color,\r\n     edge.width = E(g)$flow/200)\r\n\r\n\r\n\r\nReusable Functionality\r\nWe can also build a function to reuse for next time.\r\n\r\n\r\n#\r\n# @lp.mincost.maxflow: data.frame, integer, integer, integer or list -> list\r\n#   function outputs an edge list with flows.\r\n#\r\n# @edges: data.frame, should have from, to, and capacity columns for each edge in the network. from and to columns should contain unique node ids as integers from 0 to N.\r\n# @source.id: integer, unique node id\r\n# @dest.id: integer, unique node id\r\n# @flow.demand: integer or list, if integer create a demand signal to push that much from source.id to dest.id, if a list expectation is that it sums to 0 and will contain each nodes supply (if positive) and demand (if negative).\r\n\r\n\r\nlp.mincost.maxflow <- function(edges, source.id, dest.id, flow.demand){\r\n  if(!(\"from\" %in% names(edges)) || \r\n     !(\"to\" %in% names(edges)) ||\r\n     !(\"upper_bound\" %in% names(edges)) ||\r\n     !(\"lower_bound\" %in% names(edges))){\r\n    print(\"Need from, to, and capacity columns.\")\r\n  }\r\n  else{\r\n    \r\n    # Infer the nodes\r\n    nodes <- data.frame(id = sort(unique(c(unique(edges$from), unique(edges$to)))))\r\n    \r\n    # Infer the demand to flow through the network\r\n    if(length(flow.demand)>1){\r\n      if(sum(flow.demand) == 0){\r\n        demand <- flow.demand\r\n      }else{\r\n        print(\"Flow demand doesn't add up to 0.\")\r\n      }\r\n    }\r\n    else{\r\n      demand <- c(flow.demand, rep(0, nrow(nodes)-2), -flow.demand)\r\n    }\r\n\r\n    # Get arc capacity constraints\r\n    create_arc_capacity_constraints <- function(edges){\r\n        \r\n      # For upper\r\n      Amat <- matrix(0, nrow = nrow(edges), ncol = nrow(edges))\r\n      Amat_dir <- rep(\"<=\", nrow(Amat))\r\n      Amat_rhs <- c()\r\n    \r\n      for(i in 1:ncol(Amat)){\r\n        Amat[i,i] <- 1\r\n        Amat_rhs <- c(Amat_rhs, edges$upper_bound[i])\r\n      }\r\n      \r\n      # For lower\r\n      Bmat <- matrix(0, nrow = nrow(edges), ncol = nrow(edges))\r\n      Bmat_dir <- rep(\">=\", nrow(Bmat))\r\n      Bmat_rhs <- c()\r\n    \r\n      for(i in 1:ncol(Bmat)){\r\n        Bmat[i,i] <- 1\r\n        Bmat_rhs <- c(Bmat_rhs, edges$lower_bound[i])\r\n      }\r\n      \r\n      list(Amat = rbind(Amat, Bmat),\r\n           Amat_dir = c(Amat_dir, Bmat_dir),\r\n           Amat_rhs = c(Amat_rhs, Bmat_rhs))\r\n    }\r\n    \r\n   results <- create_arc_capacity_constraints(edges)\r\n    Amat <- results$Amat\r\n    Amat_dir <- results$Amat_dir\r\n    Amat_rhs <- results$Amat_rhs\r\n    \r\n    # Create node flow constraints (in = out)\r\n    create_node_constraints <- function(nodes, edges){\r\n        Bmat <- matrix(0, nrow = nrow(nodes), ncol = nrow(edges))\r\n        Bmat_dir <- rep(\"==\", nrow(Bmat))\r\n        Bmat_rhs <- rep(0, nrow(Bmat))\r\n        \r\n        for(i in 1:nrow(Bmat)){\r\n          node_id <- nodes[i, \"id\"]\r\n          for(j in 1:ncol(Bmat)){\r\n            edge_from <- edges[j,\"from\"]\r\n            edge_to <- edges[j, \"to\"]\r\n            \r\n            if(node_id == edge_from){\r\n              # print(paste(\"Outbound for \", node_id, \"From: \",edge_from, \"to: \", edge_to))\r\n              Bmat[i,j] = 1\r\n            }\r\n            else if(node_id == edge_to){\r\n              # print(paste(\"Inbound for \", node_id, \"From: \",edge_from, \"to: \", edge_to))\r\n              Bmat[i,j] = -1\r\n            }\r\n            else{\r\n              Bmat[i,j] = 0\r\n            }\r\n          }\r\n          Bmat_rhs[i] <- demand[i]\r\n        }\r\n        \r\n        list(Bmat = Bmat,\r\n             Bmat_dir = Bmat_dir,\r\n             Bmat_rhs = Bmat_rhs\r\n        )\r\n    }\r\n    \r\n    results <- create_node_constraints(nodes, edges)\r\n    Bmat <- results$Bmat\r\n    Bmat_dir <- results$Bmat_dir\r\n    Bmat_rhs <- results$Bmat_rhs\r\n    \r\n    # Bring together\r\n    f.obj <- edges$cost\r\n    f.cons <- rbind(Amat, Bmat)\r\n    f.rhs <- c(Amat_rhs, Bmat_rhs)\r\n    f.dir <- c(Amat_dir, Bmat_dir)\r\n    results <- lp(direction = \"min\",  \r\n                  objective.in = f.obj, \r\n                  const.mat = f.cons, \r\n                  const.dir = f.dir, \r\n                  const.rhs = f.rhs)\r\n    \r\n    edges$flow <- results$solution\r\n  }\r\n  \r\n  list(edges = edges, results = results)\r\n}\r\n\r\nedges <- data.frame(from = c(1, 1, 2, 2, 5, 4, 4, 3, 3), \r\n                    to = c(2, 3, 5, 4, 6, 5, 6, 5, 4),\r\n                    lower_bound = 0,\r\n                    upper_bound = c(800, 600, 100, 600, 600, 600, 400, 400, 300),\r\n                    cost = c(10, 50, 70, 30, 30, 30, 60, 60, 10))\r\n\r\nlp.mincost.maxflow(edges = edges, source.id = 1, dest.id = 6, flow.demand = 900)\r\n\r\n$edges\r\n  from to lower_bound upper_bound cost flow\r\n1    1  2           0         800   10  700\r\n2    1  3           0         600   50  200\r\n3    2  5           0         100   70  100\r\n4    2  4           0         600   30  600\r\n5    5  6           0         600   30  600\r\n6    4  5           0         600   30  500\r\n7    4  6           0         400   60  300\r\n8    3  5           0         400   60    0\r\n9    3  4           0         300   10  200\r\n\r\n$results\r\nSuccess: the objective function is 95000 \r\n\r\nSourcing\r\nYou can also source the file to make things easier for next time. The\r\ncode can be found here.\r\n\r\n\r\nsource(\"lp.mincost.maxflow.r\")\r\n\r\nedges <- data.frame(from = c(1, 1, 2, 2, 5, 4, 4, 3, 3), \r\n                    to = c(2, 3, 5, 4, 6, 5, 6, 5, 4),\r\n                    lower_bound = 0,\r\n                    upper_bound = c(800, 600, 100, 600, 600, 600, 400, 400, 300),\r\n                    cost = c(10, 50, 70, 30, 30, 30, 60, 60, 10))\r\n\r\nlp.mincost.maxflow(edges = edges, source.id = 1, dest.id = 6, flow.demand = 900)\r\n\r\n$edges\r\n  from to lower_bound upper_bound cost flow\r\n1    1  2           0         800   10  700\r\n2    1  3           0         600   50  200\r\n3    2  5           0         100   70  100\r\n4    2  4           0         600   30  600\r\n5    5  6           0         600   30  600\r\n6    4  5           0         600   30  500\r\n7    4  6           0         400   60  300\r\n8    3  5           0         400   60    0\r\n9    3  4           0         300   10  200\r\n\r\n$results\r\nSuccess: the objective function is 95000 \r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-10-minimum-cost-network-flow-problem-in-r/minimum-cost-network-flow-problem-in-r_files/figure-html5/visual_1-1.png",
    "last_modified": "2022-07-15T04:32:36-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-09-maximum-network-flows-in-r/",
    "title": "Maximum Network Flows in R",
    "description": "In this post we will walk through how to make a maximum flow decision using network flows and linear programming.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-09",
    "categories": [
      "network flows",
      "linear programming",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nOil Flow Maximization\r\nProblem Formation\r\nNetwork Structure\r\nNetwork Visual\r\nBig M\r\n\r\nLinear Programming (LP)\r\nLP Structure\r\nConstraints\r\nObjective\r\n\r\nSolve the LP\r\nVisualize Maximum Flow\r\nSolution\r\nCleaning Up The Visual\r\n\r\n\r\nReusable Functionality\r\nSourcing\r\nReferences\r\n\r\nOil Flow Maximization\r\nOne classic problem in Network Flows and\r\nOptimization is called the Max-Flow Problem.\r\nThis takes any two nodes in a network, s and t\r\nand attempts to send as much of a resource (or multiple) from\r\ns to t. This is called a flow,\r\nand the flow which maximizes the total bandwidth of the network is\r\ncalled the maximum flow.\r\nFirst, the problem starts with an objective: to\r\nmaximize flow. These are denoted as,\r\n\\[x_{ij} = flow \\: on \\: node_i \\: to \\:\r\nnode_j \\:(or \\: on \\: arc \\: (i,j))\\]\r\nSecond, the problem has a set of constraints, these are called the\r\narc capacities. These are denoted as,\r\n\\[u_{ij} = maximum \\:amount \\:of \\:\r\nfeasible \\: flow \\: on \\: node_i \\: to \\: node_j \\:(or \\: on \\: arc \\:\r\n(i,j))\\] Last, the network graph is supplied as a set of\r\nconnections under the traditional network structure:\r\n\\[ G = (N,E) \\]\r\nProblem Formation\r\nFor our problem, the feasible flow is going to be in units of\r\nmillions of barrels of oil per hour that will pass through an arc of\r\npipeline.\r\nNetwork Structure\r\nThe source for our network is indicated in green and sink in red.\r\n\r\n\r\nnodes <- data.frame(id = c(0:4), color = c(\"green\", rep(\"grey\", 3), \"red\"))\r\nedges <- data.frame(from = c(0,0,1,1,3,2), to = c(1,2,2,3,4,4), capacity = c(2,3,3,4,1,2), color = \"grey\")\r\n\r\nedges\r\n\r\n  from to capacity color\r\n1    0  1        2  grey\r\n2    0  2        3  grey\r\n3    1  2        3  grey\r\n4    1  3        4  grey\r\n5    3  4        1  grey\r\n6    2  4        2  grey\r\n\r\nNetwork Visual\r\n\r\n\r\ng <- igraph::graph_from_data_frame(d = edges,\r\n                                   directed = TRUE,\r\n                                   vertices = nodes)\r\nplot(g, edge.label = E(g)$capacity, \r\n     main = \"Oil Pipeline Capacities\", \r\n     vertex.color = V(g)$color,\r\n     edge.color = E(g)$color)\r\n\r\n\r\n\r\nBig M\r\nIn order to model this problem, we need to put a very large capacity\r\nfrom the source node to the sink node, these\r\nare s and t mentioned above. We have given\r\nthem node names 0 and 4 respectively in the\r\ndataframe for nodes.\r\n\r\n\r\nBIG_M <- 1000000\r\n\r\nedges <- rbind(edges, data.frame(from = c(4),\r\n                                 to = c(0), \r\n                                 capacity = c(BIG_M), \r\n                                 color = \"purple\"))\r\n\r\n# Never hurts to add an id\r\nedges$id <- 0:(nrow(edges)-1)\r\n\r\ng <- igraph::graph_from_data_frame(d = edges,\r\n                                   directed = TRUE,\r\n                                   vertices = nodes)\r\nplot(g,\r\n     edge.label = E(g)$capacity, \r\n     main = \"Oil Pipeline Capacities\", \r\n     vertex.color = V(g)$color,\r\n     edge.color = E(g)$color)\r\n\r\n\r\n\r\nLinear Programming (LP)\r\nLP Structure\r\nThe next step is to set up the optimization. Let’s do that now. There\r\nare 3 key ingredients.\r\nObjective\r\nConstraints\r\nDirections\r\nConstraints\r\nWe want to build the constraint matrix, which has 2 parts.\r\nArc Capacity Constraints\r\nNode Flow Constraints\r\nThe Arc Capacity Constraints are the first we\r\naddress.\r\nThe Amat, or A matrix, is the arc matrix which contains\r\nthe upper bounds. Since linear programming relies on the resource\r\nmatrix, we need one row for each arc, our dimensions for variable flow\r\nselection are the number of arcs also. So this means we need an\r\nidentity matrix for the rows of arcs and columns of\r\narcs.\r\n\r\n\r\nAmat <- matrix(0, nrow = nrow(edges), ncol = nrow(edges))\r\nAmat_dir <- rep(\"<=\", nrow(Amat))\r\nAmat_rhs <- c()\r\n\r\nfor(i in 1:ncol(Amat)){\r\n  Amat[i,i] <- 1\r\n  Amat_rhs <- c(Amat_rhs, edges$capacity[i])\r\n}\r\n\r\nAmat\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\r\n[1,]    1    0    0    0    0    0    0\r\n[2,]    0    1    0    0    0    0    0\r\n[3,]    0    0    1    0    0    0    0\r\n[4,]    0    0    0    1    0    0    0\r\n[5,]    0    0    0    0    1    0    0\r\n[6,]    0    0    0    0    0    1    0\r\n[7,]    0    0    0    0    0    0    1\r\n\r\nAmat_rhs\r\n\r\n[1] 2e+00 3e+00 3e+00 4e+00 1e+00 2e+00 1e+06\r\n\r\nAmat_dir\r\n\r\n[1] \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\"\r\n\r\nThe Node Flow Constraints are the next to take care\r\nof.\r\nFor each node, we need to match it’s from node and\r\nto node with the appropriate inflow and outflow. If it\r\nmatches an inflow arc, this is increase, so 1\r\nis in the arc column. If it matches an outflow arch, this\r\nis decrease, so -1 is in the arc column. Otherwise\r\n0 remains as the placeholder. The sign here is\r\n== because they must match (i.e., supply = demand). If we\r\nrequire excess at certain points we can set this demand to be higher\r\nthan zero, but we will not do that here.\r\n\r\n\r\nBmat <- matrix(0, nrow = nrow(nodes), ncol = nrow(edges))\r\nBmat_dir <- rep(\"==\", nrow(Bmat))\r\nBmat_rhs <- rep(0, nrow(Bmat))\r\n\r\nfor(i in 1:nrow(Bmat)){\r\n  node_id <- nodes[i, \"id\"]\r\n  for(j in 1:ncol(Bmat)){\r\n    edge_from <- edges[j,\"from\"]\r\n    edge_to <- edges[j, \"to\"]\r\n    edge_id <- edges[j, \"id\"]\r\n    \r\n    if(node_id == edge_from){\r\n      Bmat[i,j] = -1\r\n    }\r\n    else if(node_id == edge_to){\r\n      Bmat[i,j] = 1\r\n    }\r\n    else{\r\n      Bmat[i,j] = 0\r\n    }\r\n  }\r\n}\r\n\r\nBmat\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\r\n[1,]   -1   -1    0    0    0    0    1\r\n[2,]    1    0   -1   -1    0    0    0\r\n[3,]    0    1    1    0    0   -1    0\r\n[4,]    0    0    0    1   -1    0    0\r\n[5,]    0    0    0    0    1    1   -1\r\n\r\nBmat_dir\r\n\r\n[1] \"==\" \"==\" \"==\" \"==\" \"==\"\r\n\r\nBmat_rhs\r\n\r\n[1] 0 0 0 0 0\r\n\r\nObjective\r\nNext, the objective is going to be 0 for all values,\r\nexcept our final flow. This we want to maximize.\r\n\r\n\r\nf.obj <- c(rep(0, nrow(edges)-1), 1)\r\nf.obj\r\n\r\n[1] 0 0 0 0 0 0 1\r\n\r\nSolve the LP\r\nNow we unite everything together and drop it into the solver.\r\nRemember, we are trying to maximize flow. So specify\r\nmax.\r\n\r\n\r\nf.cons <- rbind(Amat, Bmat)\r\nf.rhs <- c(Amat_rhs, Bmat_rhs)\r\nf.dir <- c(Amat_dir, Bmat_dir)\r\n\r\nresults <- lp(direction = \"max\",  \r\n              objective.in = f.obj, \r\n              const.mat = f.cons, \r\n              const.dir = f.dir, \r\n              const.rhs = f.rhs)\r\nresults$solution\r\n\r\n[1] 1 2 0 1 1 2 3\r\n\r\nVisualize Maximum Flow\r\nSolution\r\nSince the results$solution contain the maximum flow we\r\ncan push through the pipes as a system, we can add this into our flow\r\ncomponent of the edges dataframe.\r\nThe visual indicates that 2/3 of the capacity was\r\nshipped to node 2, and 1/2 to\r\nnode 1. After this the next best transfer was from\r\nnode 2 to node 4 maxed out at\r\n2/2. This is indicated in red. The arcs that have flow are\r\nindicated in black, and no flow is indicated by grey. The maximum flow\r\narc is just an artificial arc that indicates the maximum\r\nflow, this is indicated in purple.\r\nLastly from the node 1 to node 3\r\n1/4 of the capacity was sent. Then from node 3\r\nto node 4 was maxed out at 1/1.\r\nPosterior analysis to this model output tells us that sending\r\n2 million tons of oil from the source to\r\ndestination 2 and 1 million tons of oil from\r\nthe source to destination 1 will push as much\r\nflow to destination 4 as we possibly can.\r\n\r\n\r\n# Set the flow\r\nedges$flow <- results$solution\r\n\r\n# If the arc is flowing oil, change to black\r\nedges$color[which(edges$flow > 0)] <- \"black\"\r\n\r\n# If the arc is at capacity change it to red\r\nedges$color[which(edges$flow == edges$capacity)] <- \"red\"\r\n\r\n# Last flow is purple\r\nedges$color[nrow(edges)] <- \"purple\"\r\n\r\n\r\ng <- igraph::graph_from_data_frame(d = edges,\r\n                                   directed = TRUE,\r\n                                   vertices = nodes)\r\n\r\n# Make it look a little more appealing\r\nL = cbind(1:5, 1:5)\r\nCURVE = c(0,0.15, 0.3, 0.45, 0, -0.15, -0.3, 0, 0.15, 0) * 5\r\n\r\n# Plot\r\nplot(g,\r\n     layout = L,\r\n     edge.curved = CURVE,\r\n     edge.label = paste(\"(\",E(g)$flow, \r\n                        \",\", \r\n                        E(g)$capacity, \")\", \r\n                        sep=\"\"), \r\n     main = \"Oil Pipeline Flow | (flow, capacity)\", \r\n     vertex.color = V(g)$color,\r\n     vertex.size = 25,\r\n     vertex.label.cex = 1.6,\r\n     edge.color = E(g)$color,\r\n     edge.width = E(g)$flow*2)\r\n\r\n\r\n\r\nCleaning Up The Visual\r\n\r\n\r\nvisNet_nodes <- nodes %>% \r\n                dplyr::mutate(label = paste(\"Location \", id),\r\n                              font.size =10,\r\n                              type = \"black\",\r\n                              shape = \"circle\",\r\n                              font = 12)\r\n\r\nvisNet_edges <- edges %>% \r\n                dplyr::mutate(label = paste(\"(\",flow,\",\",capacity,\")\",sep=\"\"),\r\n                              length = 250,\r\n                              width = flow*2,\r\n                              arrows = \"to\") \r\n  \r\nvisNetwork(visNet_nodes, \r\n           visNet_edges,\r\n           width = \"100%\",\r\n           main = \"Maximum Oil Flow | (flow, capacity)\")\r\n\r\n\r\n\r\nReusable Functionality\r\nIf we would like, we can take the code and example above and make a\r\nreusable functional API so we don’t need to do this time and again. The\r\ncode is as follows.\r\n\r\n\r\n#\r\n# @lp.maxflow: data.frame, integer, integer -> list\r\n#   function outputs an edge list with max flows.\r\n#\r\n# @edges: data.frame, should have from, to, and capacity columns for each edge in the network. from and to columns should contain unique node ids as integers from 0 to N.\r\n# @source.id: integer, unique node id\r\n# @dest.id: integer, unique node id\r\n#\r\nlp.maxflow <- function(edges, source.id, dest.id){\r\n  if(!(\"from\" %in% names(edges)) || !(\"to\" %in% names(edges)) || !(\"capacity\" %in% names(edges))){\r\n    print(\"Need from, to, and capacity columns.\")\r\n  }\r\n  else{\r\n    \r\n    # Infer the nodes\r\n    nodes <- data.frame(id = sort(unique(c(unique(edges$from), unique(edges$to)))))\r\n    \r\n    # Connect source.id to dest.id\r\n    BIG_M = max(edges$capacity)*10\r\n    edges <- rbind(edges, data.frame(from = c(dest.id),\r\n                                 to = c(source.id), \r\n                                 capacity = c(BIG_M)))\r\n    \r\n    \r\n    \r\n    # Build up edge constraints \r\n    Amat <- matrix(0, nrow = nrow(edges), ncol = nrow(edges))\r\n    Amat_dir <- rep(\"<=\", nrow(Amat))\r\n    Amat_rhs <- c()\r\n    \r\n    for(i in 1:ncol(Amat)){\r\n      Amat[i,i] <- 1\r\n      Amat_rhs <- c(Amat_rhs, edges$capacity[i])\r\n    }\r\n    \r\n    # Build up node constraints\r\n    Bmat <- matrix(0, nrow = nrow(nodes), ncol = nrow(edges))\r\n    Bmat_dir <- rep(\"==\", nrow(Bmat))\r\n    Bmat_rhs <- rep(0, nrow(Bmat))\r\n    \r\n    for(i in 1:nrow(Bmat)){\r\n      node_id <- nodes[i, \"id\"]\r\n      for(j in 1:ncol(Bmat)){\r\n        edge_from <- edges[j,\"from\"]\r\n        edge_to <- edges[j, \"to\"]\r\n        edge_id <- edges[j, \"id\"]\r\n        \r\n        if(node_id == edge_from){\r\n          Bmat[i,j] = -1\r\n        }\r\n        else if(node_id == edge_to){\r\n          Bmat[i,j] = 1\r\n        }\r\n        else{\r\n          Bmat[i,j] = 0\r\n        }\r\n      }\r\n    }\r\n\r\n    # Join all model parameters\r\n    f.obj <- c(rep(0, nrow(edges)-1), 1)\r\n    f.cons <- rbind(Amat, Bmat)\r\n    f.rhs <- c(Amat_rhs, Bmat_rhs)\r\n    f.dir <- c(Amat_dir, Bmat_dir)\r\n    \r\n    results <- lp(direction = \"max\",  \r\n                  objective.in = f.obj, \r\n                  const.mat = f.cons, \r\n                  const.dir = f.dir, \r\n                  const.rhs = f.rhs)\r\n    edges$flow <- results$solution\r\n  }\r\n  \r\n  list(edges=edges, flow = results$solution, maxflow = results$objval, results = results)\r\n}\r\n\r\nedges <- data.frame(from = c(0,0,1,1,3,2), to = c(1,2,2,3,4,4), capacity = c(2,3,3,4,1,2))\r\nlp.maxflow(edges, source.id = 0, dest.id = 4)\r\n\r\n$edges\r\n  from to capacity flow\r\n1    0  1        2    1\r\n2    0  2        3    2\r\n3    1  2        3    0\r\n4    1  3        4    1\r\n5    3  4        1    1\r\n6    2  4        2    2\r\n7    4  0       40    3\r\n\r\n$flow\r\n[1] 1 2 0 1 1 2 3\r\n\r\n$maxflow\r\n[1] 3\r\n\r\n$results\r\nSuccess: the objective function is 3 \r\n\r\nSourcing\r\nWe can also save that function and source it for later use. The code\r\nis available on github here.\r\n\r\n\r\nsource(\"lp.maxflow.r\")\r\nedges <- data.frame(from = c(0,0,1,1,3,2), to = c(1,2,2,3,4,4), capacity = c(2,3,3,4,1,2))\r\nlp.maxflow(edges, source.id = 0, dest.id = 4)\r\n\r\n$edges\r\n  from to capacity flow\r\n1    0  1        2    1\r\n2    0  2        3    2\r\n3    1  2        3    0\r\n4    1  3        4    1\r\n5    3  4        1    1\r\n6    2  4        2    2\r\n7    4  0       40    3\r\n\r\n$flow\r\n[1] 1 2 0 1 1 2 3\r\n\r\n$maxflow\r\n[1] 3\r\n\r\n$results\r\nSuccess: the objective function is 3 \r\n\r\nReferences\r\nExample taken from the following sources:\r\nWinston., Wayne. Operations Research, Applications and\r\nAlgorithms 4th Edition.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-09-maximum-network-flows-in-r/maximum-network-flows-in-r_files/figure-html5/initial_graph-1.png",
    "last_modified": "2022-07-15T04:01:51-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-09-shortest-path-optimization-in-r/",
    "title": "Shortest Path Optimization in R",
    "description": "In this post we will walk through how to make least cost decisions using network flows shortest path algorithm.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-09",
    "categories": [
      "network flows",
      "linear programming",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nVehicle\r\nReplacement Using Shortest Path\r\nProblem Setup\r\nProblem Data\r\nNew Car Cost\r\nMaintenance Costs\r\nTrade-in Revenues\r\nCost Matrix\r\n\r\nSolving the Problem\r\nVisualizing the network\r\nShortest Path Solution\r\nShortest Path Analysis\r\n\r\n\r\nReferences\r\n\r\nVehicle Replacement\r\nUsing Shortest Path\r\nA common problem we often face in the world is to find the shortest\r\npath between two points. Whether it is on the road, or how to obtain an\r\nobject in in our intellectual trajectory, we are always seeking to\r\noptimize.\r\nIn network flows one common problem is to find the\r\ns-t shortest path. The problem formulation is as\r\nfollows.\r\nGiven some net of nodes, and two particular nodes s and\r\nt that are not the same, find the shortest distance through\r\nthe edges connecting them. The mathematical formulation is below. \\[ N \\in \\{n_1, n_2, .., n_D\\} \\] \\[ E = \\{N \\times N\\} = \\{e_{ij} \\in \\Re^+ |\r\n\\:\\:\\forall i,j \\in N \\} \\] \\[ s,t \\in\r\nN : s\\neq t\\]\r\nProblem Setup\r\nWe all have to drive cars at some point in our adult lives (well,\r\nmost of us). With this comes the question of investment\r\ndecisions if I am allowed to call it that. Which car should we buy?\r\nHow long should we keep it? Is it more prudent to keep paying\r\nmaintenance, repairs, and damages, or just get a new ride?\r\nWe would like to know what is the best decision to make over the next\r\n5 years for our vehicle needs. So we take this problem and model it as\r\nan optimization problem using the famous\r\nshortest paths algorithm.\r\nSo with our problem the decision space is pretty straight\r\nforward:\r\nEach year we can choose to keep our vehicle, or\r\nTrade it in\r\nHowever, every year we choose to keep our vehicle, we must pay\r\nmaintenance costs for it. So every year we keep it, there is a\r\ncumulative maintenance cost. Once we trade-in we offset the cost of the\r\nnew car with the trade in value, and pay much less maintenance on the\r\nnew ride. Let’s take a look at the problem data.\r\nProblem Data\r\nWe know the cost every year of a new vehicle is assumed as\r\n$12,000 for simplicity. Further, we have some records of\r\nwhat maintenance costs and trade-in values will be.\r\nNew Car Cost\r\nThe new car cost is assumed constant every year. An interesting\r\nhomework assignment would be to make this stochastic and\r\nchange over time. This is much more suitable to the real world, but for\r\nthis example will remain constant.\r\n\r\n\r\nNEW_CAR_COST <- 12000\r\nNEW_CAR_COST\r\n\r\n[1] 12000\r\n\r\nMaintenance Costs\r\nThe maintenance costs are for 4 years. Each year you keep the car,\r\nyou will pay more on maintenance.\r\n\r\n\r\nM <- data.frame(Year = c(0:4), Maintenance_cost = c(2000, 4000, 5000, 9000, 12000))\r\nM_vec <- M %>% dplyr::pull(Maintenance_cost)\r\nM\r\n\r\n  Year Maintenance_cost\r\n1    0             2000\r\n2    1             4000\r\n3    2             5000\r\n4    3             9000\r\n5    4            12000\r\n\r\nTrade-in Revenues\r\nThe trade-in price is similar to maintenance. Each year you keep the\r\nvehicle, it will depreciate. So to account for this we have a decreasing\r\ntrade-in value.\r\n\r\n\r\nT <- data.frame(Year = c(1:5), Trade_in_price = c(7000, 6000, 2000, 1000, 0))\r\nT_vec <- T %>% dplyr::pull(Trade_in_price)\r\nT\r\n\r\n  Year Trade_in_price\r\n1    1           7000\r\n2    2           6000\r\n3    3           2000\r\n4    4           1000\r\n5    5              0\r\n\r\nCost Matrix\r\nSince we know the costs will be cumulative, so we know what each\r\nyears will be. The cost matrix will be for the number of years the car\r\nis kept to accumulate costs from maintenance. In mathematical language,\r\nthis is represented below:\r\n\\[ c_{ij} = \\sum_{t=1}^{j-1}{M_{t-1}}\\:\\:\r\nif\\: j > i \\:\\: otherwise \\: \\infty \\]\r\nWhere M is the maintenance matrix defined above.\r\nWe also know our objective is to minimize the total cost, which\r\nequates to maintanence cost +\r\ncost to purchase a new car -\r\ntrade in value.\r\n\r\n\r\n# Nodes dataframe\r\nnodes = data.frame(Year = c(sprintf(\"Year %s\", seq(1:6))),\r\n                   Color = c(\"green\", \"gold\", \"gold\", \"gold\", \"gold\", \"red\"))\r\nn = nrow(nodes)\r\n\r\n# Edges list\r\nedges = list(from=c(), to=c(), cost=c(), color=c())\r\n\r\n# Cost matrix\r\nC <- matrix(0, n, n)\r\nBIG_M <- 1000000\r\nfor(i in 1:n){\r\n  for (j in 1:n){\r\n    if(j > i){\r\n      \r\n      # Cost of maintenance\r\n      maintenance_cost <- M_vec[1:(j-i)]\r\n      maintenance_cost_total <- sum(maintenance_cost)\r\n\r\n      # Cost of new car\r\n      new_car_cost = NEW_CAR_COST\r\n      \r\n      # Trade-in value\r\n      trade_in_revenue <- T_vec[j-i]\r\n\r\n      # Total cost for decision to buy car on year i and sell it on year j\r\n      total_cost_for_decision_i_to_j <- new_car_cost + maintenance_cost_total - trade_in_revenue\r\n\r\n      # Save the value into cost matrix\r\n      C[i,j] <- total_cost_for_decision_i_to_j\r\n      edges$from <- append(edges$from, paste(\"Year\", i))\r\n      edges$to <- append(edges$to, paste(\"Year\", j))\r\n      edges$cost <- append(edges$cost, total_cost_for_decision_i_to_j)\r\n      edges$color <- append(edges$color, \"grey\")\r\n    }\r\n    else{\r\n      \r\n      # Big M otherwise to make edge infeasible\r\n      C[i,j] <- BIG_M\r\n    }\r\n  }\r\n}\r\n\r\n# Edges dataframe\r\nedges <- edges %>% as.data.frame\r\n\r\nnodes\r\n\r\n    Year Color\r\n1 Year 1 green\r\n2 Year 2  gold\r\n3 Year 3  gold\r\n4 Year 4  gold\r\n5 Year 5  gold\r\n6 Year 6   red\r\n\r\nedges\r\n\r\n     from     to  cost color\r\n1  Year 1 Year 2  7000  grey\r\n2  Year 1 Year 3 12000  grey\r\n3  Year 1 Year 4 21000  grey\r\n4  Year 1 Year 5 31000  grey\r\n5  Year 1 Year 6 44000  grey\r\n6  Year 2 Year 3  7000  grey\r\n7  Year 2 Year 4 12000  grey\r\n8  Year 2 Year 5 21000  grey\r\n9  Year 2 Year 6 31000  grey\r\n10 Year 3 Year 4  7000  grey\r\n11 Year 3 Year 5 12000  grey\r\n12 Year 3 Year 6 21000  grey\r\n13 Year 4 Year 5  7000  grey\r\n14 Year 4 Year 6 12000  grey\r\n15 Year 5 Year 6  7000  grey\r\n\r\nC\r\n\r\n      [,1]  [,2]    [,3]    [,4]    [,5]    [,6]\r\n[1,] 1e+06 7e+03   12000   21000   31000   44000\r\n[2,] 1e+06 1e+06    7000   12000   21000   31000\r\n[3,] 1e+06 1e+06 1000000    7000   12000   21000\r\n[4,] 1e+06 1e+06 1000000 1000000    7000   12000\r\n[5,] 1e+06 1e+06 1000000 1000000 1000000    7000\r\n[6,] 1e+06 1e+06 1000000 1000000 1000000 1000000\r\n\r\nSolving the Problem\r\nNow that we have our cost matrix, the last ingredient is to solve the\r\nproblem. That means to solve the s-t shortest path from\r\nYear 0 to Year 6, so we can determine what is\r\nthe cheapest investment strategy for us. For this, we will be using the\r\nigraph package.\r\nVisualizing the network\r\n\r\n\r\ng <- igraph::graph_from_data_frame(d = edges,\r\n                                   directed = TRUE,\r\n                                   vertices = nodes)\r\n# Make it a little cleaner\r\nplot(g,\r\n     main = \"Cost of Annual Vehicle Trade-in\",\r\n     edge.arrow.size=.5,\r\n     vertex.color=V(g)$Color,\r\n     edge.color=E(g)$color,\r\n     edge.label = E(g)$cost,\r\n     edge.width = E(g)$cost/10000,\r\n     vertex.size=20,\r\n     vertex.frame.color=\"gray\",\r\n     vertex.label.color=\"black\",\r\n     vertex.label.cex=0.8,\r\n     vertex.label.dist=2, layout = layout_as_star,\r\n     edge.curved=0.2)\r\n\r\n\r\n\r\nShortest Path Solution\r\nSo once we perform the Dijkstra's Shortest Path\r\nalgorithm on the network we obtain a solution in matrix form. This\r\nsolution tells us the best possible cost for our car decision from\r\nYear 1 to Year 6 will be at\r\n$31,000.\r\nWhat this does not show us is what path was chosen to obtain that\r\nvalue.\r\n\r\n\r\n# Get the shortest paht cost matrix\r\ns_paths_cost <- igraph::shortest.paths(graph = g, v = V(g), weights = E(g)$cost, algorithm = \"dijkstra\")\r\ns_paths_cost\r\n\r\n       Year 1 Year 2 Year 3 Year 4 Year 5 Year 6\r\nYear 1      0   7000  12000  19000  24000  31000\r\nYear 2   7000      0   7000  12000  19000  24000\r\nYear 3  12000   7000      0   7000  12000  19000\r\nYear 4  19000  12000   7000      0   7000  12000\r\nYear 5  24000  19000  12000   7000      0   7000\r\nYear 6  31000  24000  19000  12000   7000      0\r\n\r\nShortest Path Analysis\r\nThe optimal selection is the following sequence:\r\nBuy a new car in Year 1 keep the car for a year, then\r\nsell on Year 2.\r\nBuy another car in Year 2, but keep for two years and\r\nsell on Year 4.\r\nFinally, buy a car on Year 4, keep for two years and\r\nsell on Year 6.\r\nDo these numbers add up? Let’s check. 7000 + 12000 + 12000 == 31000\r\nis TRUE.\r\nSo our least cost strategy can be no less than $31000\r\nover the next 6 years. With the current cost structure, means to keep\r\nthe car for a year or two, then pitch it because the trade-off between\r\nmaintenance accumulation and depreciation start to mutually deter from a\r\nleast cost decision.\r\n\r\n\r\n# Get all path distances solution vertex path\r\ns.paths <- igraph::shortest_paths(graph = g,\r\n                                  from = \"Year 1\",\r\n                                  output = \"vpath\",\r\n                                  weights = E(g)$cost, \r\n                                  to = \"Year 6\")\r\n                                  # v = V(g), \r\n                                  # to = V(g), \r\n                                  # weights = E(g)$cost)\r\n\r\n# Update colors from vertex path found\r\ns.paths$vpath\r\n\r\n[[1]]\r\n+ 4/6 vertices, named, from 81b1a31:\r\n[1] Year 1 Year 2 Year 4 Year 6\r\n\r\nE(g, path = s.paths$vpath[[1]])$color <- \"red\"\r\n\r\n\r\n\r\n\r\nplot(g,\r\n     main = \"Least Cost Vehicle Trade-in Policy\",\r\n     edge.arrow.size=.5,\r\n     vertex.color=V(g)$Color,\r\n     edge.color=E(g)$color,\r\n     edge.label = E(g)$cost,\r\n     edge.width = E(g)$cost/10000,\r\n     vertex.size=20,\r\n     vertex.frame.color=\"gray\",\r\n     vertex.label.color=\"black\",\r\n     vertex.label.cex=0.8,\r\n     vertex.label.dist=2, layout = layout_as_star,\r\n     edge.curved=0.2)\r\n\r\n\r\n\r\nThis plot doesn’t look too great! Let’s try to spruce it up a bit\r\nusing visNetwork, a common package in R that leverages the\r\nvis.js framework, which can be found here.\r\n\r\n\r\nV(g)$label = nodes$Year\r\nV(g)$shape = \"circle\"\r\nE(g)$width = edges$cost/10000\r\nE(g)$weight = edges$cost/10000\r\nE(g)$label = edges$cost %>% as.character\r\n\r\nvisIgraph(igraph = g)\r\n\r\n\r\n\r\nReferences\r\nExample taken from the following sources:\r\nWinston., Wayne. Operations Research, Applications and\r\nAlgorithms 4th Edition.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-09-shortest-path-optimization-in-r/shortest-path-optimization-in-r_files/figure-html5/viz_network-1.png",
    "last_modified": "2022-07-13T06:06:56-04:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Optimization Daily",
    "description": "Welcome to Optimization Daily! Grab a coffee, take a read, and enjoy your stay.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-09",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-07-10T20:34:19-04:00",
    "input_file": {}
  }
]
