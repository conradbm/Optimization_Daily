[
  {
    "path": "posts/2022-07-14-q-learning-in-r/",
    "title": "Q-learning in R",
    "description": "In this post we will walk through the basics of Q-learning and source supporting functions.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-14",
    "categories": [
      "reinforcement learning",
      "markov decision process",
      "probability",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nQ-Learning House\r\nNavigation\r\nValue Iteration\r\n\r\nThe Problem\r\nReward Matrix\r\nEpisilon-greedy\r\n\r\nReferences\r\n\r\nQ-Learning House Navigation\r\nReinforcement learning is a relatively new field in machine learning,\r\nwith ties in control systems, decision theory, probability, and\r\nsimulation. A Markov Decision Process (MDP) is a process\r\nthat relies on previous information to make new decisions with maximum\r\nprobability of a meritorious outcome. One of the simplest algorithms to\r\ncompute this is called Value Iteration. We will be working\r\nthrough this with a simple housing example where we hope to\r\nlearn the optimal policy that will navigate an agent\r\nthrough the house. In general, these policies are often lodged in our\r\nown mental models, but once put on paper (or in a matrix), they are a\r\nlot more interesting, and sometimes calibrate even our own thinking.\r\nAfter all, the field of Deep Reinfrocement Learning has\r\nbeaten renouned championed in chess, alpha-go, and even star craft. So\r\nwhatever it is learning in those deep models can certainly augment our\r\nintelligence.\r\nThe simplicity of MCPs make them very interpretable, but at the cost\r\nof more power. So let’s take a look and see if we can cook up a function\r\nto solve the problem.\r\n\r\n\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\n\r\n\r\nValue Iteration\r\nThe idea of value iteration is that it is a recursive function with a\r\nfew well known things.\r\nS = State-space\r\nA = Action-space\r\nR = Reward function\r\nThe problem is set up as an objective. The real goal is to acquire a\r\npolicy that acts optimally. By acting optimally, this is\r\nscoped to maximizing a univariate reward function. How is this done?\r\nLet’s take a look at the math.\r\nValue Iteration \\[\r\nQ(s_1,a_1)=R(s_1,a_1) + \\gamma\\sum_{s_2 \\neq s_1 \\in S}{P(s_2 | s_1,\r\na_1) \\: max(Q(s_2, a_2) \\: \\forall a_2 \\in A)}\r\n\\]\r\nAt each iteration, a reward is known, and the future reward is also\r\ncalculated for every other state.\r\nBellman Equation \\[\r\nQ_{now}(s_1,a_1) = Q_{old}(s_1,a_1) +\\alpha \\: (r + \\gamma \\:\r\nmax_{a_2}(s_2, a_2)-Q_{old}(s_1, a_1))\r\n\\]\r\nThe Bellman Equation is also well known for solving\r\nproblems like this, and it doesn’t require the transition probability\r\nabove.\r\nThe Problem\r\nWe hope to mimic a house navigation agent. The goal is to be able to\r\nreview the optimal policy at the end that teaches the agent to travel\r\nfrom any room to any other room.\r\nHouse floorplanReward Matrix\r\nThere are three criteria,\r\n-1 if “you can’t get there from here”\r\n0 if the destination is not the target state\r\n100 if the destination is the target state\r\n\r\n\r\nR <- matrix(c(-1, -1, -1, -1, 0, 1,\r\n       -1, -1, -1, 0, -1, 0,\r\n       -1, -1, -1, 0, -1, -1, \r\n       -1, 0, 0, -1, 0, -1,\r\n        0, -1, -1, 0, -1, 0,\r\n       -1, 100, -1, -1, 100, 100), nrow=6, ncol=6, byrow=TRUE) %>% t\r\n\r\nR\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6]\r\n[1,]   -1   -1   -1   -1    0   -1\r\n[2,]   -1   -1   -1    0   -1  100\r\n[3,]   -1   -1   -1    0   -1   -1\r\n[4,]   -1    0    0   -1    0   -1\r\n[5,]    0   -1   -1    0   -1  100\r\n[6,]    1    0   -1   -1    0  100\r\n\r\n\r\n\r\ntmp <- R %>% as.data.frame()\r\ntmp <- tmp %>% dplyr::mutate(from = names(tmp))\r\ntmp <- tmp %>% tidyr::gather(., key = \"to\", \"reward\", -from)\r\ntmp <- tmp %>% dplyr::arrange(desc(reward))\r\n\r\nggplot(data = tmp) + \r\n  geom_tile(aes(x=from, y=to, fill=as.factor(reward))) + \r\n  labs(fill = \"Reward\") + theme_classic() + \r\n  ggtitle(\"Reward Matrix\")\r\n\r\n\r\n\r\n\r\n\r\nsource(\"https://raw.githubusercontent.com/NicoleRadziwill/R-Functions/master/qlearn.R\")\r\n\r\nresults <- q.learn(R,10000,alpha=0.1,gamma=0.8,tgt.state=6) \r\nround(results)\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6]\r\n[1,]    0    0    0    0   80    0\r\n[2,]    0    0    0   64    0  100\r\n[3,]    0    0    0   64    0    0\r\n[4,]    0   80   51    0   80    0\r\n[5,]   64    0    0   64    0  100\r\n[6,]   64   80    0    0   80  100\r\n\r\nThe table produced tells us the average value to obtain policies. A\r\npolicy is a path through the states. One can quickly observe going from\r\nroom 0 to room 5 which is the solution. You\r\ncan walk through these and choose the argmax which along\r\nthe columns that provides the decision for your next row of choice.\r\n\r\n\r\ntmp <- results %>% as.data.frame()\r\ntmp <- tmp %>% dplyr::mutate(from = names(tmp))\r\ntmp <- tmp %>% tidyr::gather(., key = \"to\", \"reward\", -from)\r\ntmp <- tmp %>% dplyr::arrange(desc(reward))\r\n\r\nggplot(data = tmp) + \r\n  geom_tile(aes(x=to, y=from, fill=reward)) + \r\n  labs(fill = \"Average Value (%)\") + theme_classic() + \r\n  ggtitle(\"Policy Decision Matrix\") +\r\n  scale_fill_gradient(low = \"red\", high = \"green\") +\r\n  ylab(\"State From\") + \r\n  xlab(\"Action To\")\r\n\r\n\r\n\r\nPretty cool!\r\nEpisilon-greedy\r\nOne common way of sampling an action is the\r\nepislon-greedy approach. This tells us to exploit with\r\nepsilon probability and randomly explore with 1-epsilon. Let’s take a\r\nlook.\r\n\r\n\r\nq.learn.epsilon.greedy <- function(R, epochs = 10000, alpha = 0.1,gamma = 0.8, epsilon = 0.75,tgt.state = 6, testme = T){\r\n  \r\n  Q = matrix(0, nrow = nrow(R), ncol=ncol(R))\r\n  \r\n  epsilon.greedy <- function(state, Q, epsilon = 0.75, test = F){\r\n    random_number <- runif(n = 1, min = 0, max = 1)\r\n    if(random_number < epsilon){\r\n      \r\n      if(test)\r\n        paste(\"exploit\")\r\n      else\r\n        which.max(Q[state,])\r\n    }\r\n    else{\r\n      \r\n      if(test)\r\n        paste(\"explore\")\r\n      else\r\n        sample(which(Q[state, ] != state), 1)\r\n      \r\n    }\r\n  }\r\n  \r\n  test.epsilon.greedy <- function(epsilon = 0.75){\r\n    memory <- c()\r\n    tmp <- c()\r\n    for(j in 1:100){\r\n      for(i in 1:100){\r\n      # Start at a random state\r\n      cs = sample(1:nrow(Q), 1)\r\n      ns = epsilon.greedy(cs, Q, epsilon = epsilon, test = T) \r\n      tmp <- c(tmp, ns) # we expect 75/100 to be exploits\r\n      } \r\n      memory <- c(memory, mean(tmp == \"exploit\"))\r\n    }\r\n    \r\n    # looks good.\r\n    memory %>% mean\r\n  }\r\n  \r\n  # Test it\r\n  if(testme){\r\n    \r\n    t1<-test.epsilon.greedy(epsilon=0.75)\r\n    t2<-test.epsilon.greedy(epsilon=0.5)\r\n    t3<-test.epsilon.greedy(epsilon=0.25)\r\n    print(paste(round(t1,2),\" == 0.75\"))\r\n    print(paste(round(t2,2),\" == 0.50\"))\r\n    print(paste(round(t3,2),\" == 0.25\"))\r\n  }\r\n  \r\n  \r\n  learning_rate = alpha\r\n  discount_rate = gamma\r\n  dest.state = tgt.state\r\n  \r\n  for(i in 1:epochs){\r\n    \r\n    # Start at a random state\r\n    cs = sample(1:nrow(Q), 1)\r\n    cs    \r\n  \r\n    while(TRUE){\r\n        \r\n        \r\n        # Take an action\r\n        a = epsilon.greedy(cs, Q, epsilon = 0.8, test = F)\r\n        \r\n        \r\n        # Assess the reward\r\n        r = R[cs, a]\r\n        \r\n        # Update policy matrix\r\n        Q[cs, a] = learning_rate * (Q[cs, a]) + (1 - learning_rate) * (r + discount_rate * max(Q[a, ]) )\r\n         # Q[cs,a] <- Q[cs,a] + learning_rate*(R[cs,a] + discount_rate*max(Q[a, ]) - Q[cs,a])\r\n        \r\n         # Update the current state\r\n        cs = a\r\n        \r\n        \r\n        if(a == dest.state){\r\n          break\r\n        }\r\n    }\r\n  }\r\n  Q*100/max(Q)\r\n}\r\n\r\nQ = q.learn.epsilon.greedy(R, epochs = 10000, alpha = 0.1,gamma = 0.8, epsilon = 0.75, tgt.state = 6, testme=T)\r\n\r\n[1] \"0.75  == 0.75\"\r\n[1] \"0.51  == 0.50\"\r\n[1] \"0.26  == 0.25\"\r\n\r\nidx = apply(Q, 1, which.max)\r\nI <- matrix(0, nrow=nrow(Q), ncol=ncol(Q))\r\nfor(i in 1:nrow(I)){\r\n  I[i,idx[i]] = 1\r\n}\r\nI\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6]\r\n[1,]    0    0    0    0    1    0\r\n[2,]    0    0    0    0    0    1\r\n[3,]    0    1    0    0    0    0\r\n[4,]    0    1    0    0    0    0\r\n[5,]    0    0    0    0    0    1\r\n[6,]    0    0    0    0    0    1\r\n\r\nOur resulting matrix from an epsilon greedy algorithm\r\ndoes something similar to the 100% exploit algorithm sourced in from the\r\ngithub above. The policy is as follows:\r\nIf in room 0 go to room 4, then to room 5 for the victory. (2\r\nsteps)\r\nIf in room 4 go to room 5 for the victory. (1 step)\r\nIf in room 1 go to room 5 for the victory. (1 step)\r\nIf in room 2 go to room 1, then go to room 5 for the victory. (2\r\nsteps)\r\nIf in room 3 go to room 1, then go to room 5 for the victory. (2\r\nsteps)\r\nIf in room 4, go to room 5 for the victory. (1 step)\r\nIf in room 5 .. victory! (0 steps)\r\nFor convenience, an indicator matrix is also built to illustrate the\r\nbest possibile choice to and from any state with zero ambiguity. At a\r\nglance, outside of room 5, room 1 is a favorable place to be. It seems\r\nlike it discovered a good outlet to arrive at the optimal, that is not\r\noptimal itself. Very interesting!\r\nReferences\r\nhttps://www.r-bloggers.com/2017/12/a-simple-intro-to-q-learning-in-r-floor-plan-navigation/#google_vignette\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-14-q-learning-in-r/q-learning-in-r_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-07-15T13:29:13-04:00",
    "input_file": "q-learning-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-13-logistic-regression-and-gradient-descent-in-python/",
    "title": "Logistic Regression and Gradient Descent in Python",
    "description": "In this post we will walk through how to train a Logistic Regression model from scratch using Gradient Descent in Python.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-13",
    "categories": [
      "machine learning",
      "non-linear programming",
      "python"
    ],
    "contents": "\r\n\r\nContents\r\nOverview\r\nSigmoid\r\nNegative Log Likelihood\r\nGradient Log Likelihood\r\n\r\nGradient Descent\r\nValidation\r\nTest Data\r\nOptimal Decision\r\nBoundary\r\n\r\n\r\nOverview\r\nOne of the most simple machine learning problems is that of\r\nbinary classification. Further, one of the most simple\r\nnon-linear models is logistic regression. In short, this\r\nmodel takes a set of parameters and seeks a linear combination mapped to\r\na non-linear sigmoid function which maximizes the\r\nlikelihood of fitting the data. The math for this is\r\nbelow,\r\nSigmoid Function\r\n\\[\r\n\\sigma(z)=\\frac{1}{1+exp(-z)}\r\n\\] Maximum Likelihood Estimation\r\n\\[\r\nL(x,y; \\theta)=\\frac{1}{n} \\prod_{i=1}^n{\\sigma(\\theta^Tx)^{y_i} +\r\n(1-\\sigma(\\theta^Tx))^{1-y_i}}\r\n\\] Negative Log Likelihood or Cross Entropy\r\n\\[\r\nl(x,y; \\theta)=-\\frac{1}{n}\\sum_{i=1}^n{{y_i}log(\\sigma(\\theta^Tx)) +\r\n(1-y_i)log(1-\\sigma(\\theta^Tx))}\r\n\\] Gradient of Negative Log Likelihood\r\n\\[\r\n\\nabla_\\theta l(x,y) =\r\n\\frac{1}{n}\\sum_{i=1}^n{x_i(\\sigma(\\theta^Tx)-y_i)}\r\n\\]\r\nMinimizing Negative Log Likelihood\r\n\\[\r\n\\theta^*=argmin(l(x,y; \\theta) : \\: \\theta \\in \\Re^{d+1} )\r\n\\] ## The Data\r\nWe will start with some libraries and the simple data set we will be\r\nworking with for binary classification.\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nX,y = np.concatenate((np.random.randn(5000, 2), np.ones(5000).reshape(-1,1)), 1), np.random.binomial(1, 0.25, size=5000)\r\nN, D = X.shape\r\nX.shape, y.shape\r\n((5000, 3), (5000,))\r\n\r\nSigmoid\r\nNext, we will look at the sigmoid curve.\r\n\r\ndef sigmoid(z):\r\n    return 1/(1 + np.exp(-z))\r\ntheta_init = np.random.randn(3)\r\ntheta_init\r\narray([-0.43382384,  0.00477304, -0.42501194])\r\nlogits = sigmoid(X.dot(theta_init))\r\nlogits\r\narray([0.40473019, 0.51544891, 0.53547371, ..., 0.33712169, 0.40979105,\r\n       0.28786013])\r\ndef plot_logits(logits, title = \"logits=sigmoid(X.dot(theta))\"):\r\n    plt.scatter(range(len(logits)), sorted(logits), label = \"logits\")\r\n    plt.legend()\r\n    plt.title(title)\r\n    plt.show()\r\n    \r\nplt.clf()\r\nplot_logits(logits)\r\n\r\n\r\nNegative Log Likelihood\r\nNext, the log likelihood.\r\n\r\ndef negative_log_likelihood(logits, y):\r\n    errors = y * np.log(logits) + (1-y)*np.log(1-logits)\r\n    N = len(errors)\r\n    return -(1/N) * np.sum(errors) \r\n\r\nloss = negative_log_likelihood(logits, y)\r\nloss\r\n0.634010228209429\r\n\r\nFinally, the gradient of the log likelihood. Notice that the gradient\r\nis always a vector.\r\nGradient Log Likelihood\r\n\r\ndef gradient_log_likelihood(X, logits, y):\r\n    N, D = X.shape\r\n    grad_vec = (1/N)*X.T.dot(logits-y)\r\n    return grad_vec\r\n\r\ngradient_log_likelihood(X, logits, y)\r\narray([-0.09878357,  0.00720683,  0.14126207])\r\n\r\nGradient Descent\r\nNow we want to optimize. so we will build a gradient descent function\r\nto loop through our training set and converge on a solution. We will\r\nalso take a moment to visualize the logits.\r\n\r\n\r\ndef gradient_descent(f, grad_f, eps=0.01, eta=0.1, max_iter = 1000, **kwargs):\r\n  theta_init = np.random.randn(D)\r\n  thetas = [theta_init]\r\n  losses = []\r\n  for t in range(1,max_iter):\r\n      logits = kwargs[\"h\"](kwargs[\"X\"].dot(thetas[-1]))\r\n      \r\n      loss = f(logits, kwargs[\"y\"])\r\n      losses.append(loss)\r\n      \r\n      \r\n      #if t % 50 == 0:\r\n      #    print(\"Loss {}\".format(losses[-1]))\r\n          #plot_logits(logits)\r\n          #input(\"...\")\r\n      \r\n      grad_vec = grad_f(kwargs[\"X\"], logits, kwargs[\"y\"])\r\n      theta_t = thetas[t-1] - eta * grad_vec\r\n      thetas.append(theta_t)\r\n      \r\n      if np.sqrt(np.sum(np.square(thetas[-2] - thetas[-1]))) <= eps:\r\n        return thetas[-1], losses\r\n        break\r\n\r\n  return None\r\n  \r\n        \r\nfinal_theta, losses = gradient_descent(negative_log_likelihood, \r\n                                       gradient_log_likelihood, \r\n                                       eps = 0.001, \r\n                                       eta=0.01, \r\n                                       max_iter = 100000, \r\n                                       X=X, \r\n                                       h=sigmoid, \r\n                                       y=y)\r\nlogits = sigmoid(X.dot(final_theta))\r\nplt.clf()\r\nplot_logits(logits)\r\n\r\nplt.clf()\r\nplt.plot(losses)\r\nplt.title(\"Training losses\")\r\nplt.show()\r\n\r\n\r\nValidation\r\nThe initial training accuracy with null parameters is below. 49% with\r\na random guess on theta at the start.\r\n\r\ndef predict(logits, threshold = 0.5):\r\n    return (logits > threshold).astype(int)\r\n\r\nlogits = sigmoid(X.dot(theta_init))\r\ny_pred = predict(logits, threshold = 0.5)\r\n\r\nprint(\"Accuracy: {}\".format(np.mean(y_pred == y)))\r\nAccuracy: 0.666\r\n\r\nAfter some training, our final theta parameters now get 73%. Not too\r\nshabby!\r\n\r\ndef predict(logits, threshold = 0.5):\r\n    return (logits > threshold).astype(int)\r\n\r\nlogits = sigmoid(X.dot(final_theta))\r\ny_pred = predict(logits, threshold = 0.5)\r\n\r\nprint(\"Accuracy: {}\".format(np.mean(y_pred == y)))\r\nAccuracy: 0.742\r\n\r\nTest Data\r\nLet’s try this on some test data. We will just generate some more and\r\nsee how we do! It looks like we get about the same percentage on new\r\ndata, so that is a good indicator. We would expect this though, since\r\nthey are sampled directly from the signal population. One good study\r\nmight be to examine which data points are incorrect and where they fall\r\non the logistic curve. Were they really low values or really high?\r\nPerhaps they were right on the decision boundary and just couldn’t\r\ndecide. All these and others are good questions for a deep dive into\r\nmodel interpretation, which we will not get into now.\r\n\r\nXtest,ytest = np.concatenate((np.random.randn(5000, 2), np.ones(5000).reshape(-1,1)), 1), np.random.binomial(1, 0.25, size=5000)\r\n\r\nlogits = sigmoid(Xtest.dot(final_theta))\r\ny_pred = predict(logits, threshold = 0.5)\r\n\r\nprint(\"Test Accuracy: {}\".format(np.mean(y_pred == ytest)))\r\nTest Accuracy: 0.7614\r\n\r\nOptimal Decision Boundary\r\nRight now the decision boundary is 0.5 which is\r\ncustomary in the machine learning and statistical community. We will\r\nexplore what cutoff might be optimal by doing a\r\nrandom search through a bunch of possibilities from\r\n0 to 1. The decision boundary that eeked out some extra\r\nperformance for us is at around 0.68, or for the other\r\nengineers in the room, about 0.7. This tells us that there\r\nis about a 70% likelihood of predicting a 0\r\nclass label and about 30% likelihood of predicting a\r\n1. This is very interesting because our original data was\r\nsampled from a binomial distribution with a probability of success as\r\n25%. So the model, without being told, trained on a random\r\nparameter vector, after applying gradient descent, and through a bit of\r\nsearch optimization, was able to arrive at an estimate on the\r\npopulation’s yes prediction likelihood. Optimization is\r\npretty amazing!\r\n\r\ndef optimize_decision_boundary(logits, y, plot = False):\r\n    results = []\r\n    thresholds = []\r\n    for thresh in np.linspace(0, 1, 20):\r\n        y_pred = predict(logits, threshold = thresh)\r\n        results.append(np.mean(y_pred == y))\r\n        thresholds.append(thresh)\r\n    thresh_star = np.argmax(results)\r\n    \r\n    if plot:\r\n        plt.plot(results)\r\n        plt.suptitle(\"Decision Boundary Threshold Discovery\")\r\n        plt.title(\"Best Accuracy {} at index {} with value {}\".format(results[thresh_star], thresh_star, np.round(thresholds[thresh_star], 4)))\r\n        plt.axvline(x = thresh_star, c='r')\r\n        plt.axhline(y = results[thresh_star], c='r')\r\n        plt.show()\r\n        \r\n    return thresh_star, thresholds[thresh_star]\r\n\r\nplt.clf()\r\nthreshold = optimize_decision_boundary(logits, y, plot=True)\r\n\r\nthreshold\r\n(9, 0.47368421052631576)\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-13-logistic-regression-and-gradient-descent-in-python/logistic-regression-and-gradient-descent-in-python_files/figure-html5/unnamed-chunk-5-3.png",
    "last_modified": "2022-07-13T07:10:48-04:00",
    "input_file": "logistic-regression-and-gradient-descent-in-python.knit.md"
  },
  {
    "path": "posts/2022-07-13-image-recognition-in-r/",
    "title": "Image Recognition in R",
    "description": "In this post we will explore image classification in keras for several datasets and how transfer learning can be readily applied to improve well known models.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-13",
    "categories": [
      "image recognition",
      "deep learning",
      "transfer learning",
      "keras",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nRecognize Handwritten\r\nDigits\r\nLibraries\r\nData Retrieval\r\nData Shapes\r\nVisualize Images\r\n\r\nModel\r\nSimple Dense Model\r\nSummary\r\nCompile\r\nFit\r\nVisualize\r\n\r\nAnother way\r\nSummary\r\nCompile\r\nFit\r\nVisualize\r\nPredictions\r\nEvaluate\r\nSave Model\r\nReload Model\r\n\r\n\r\nRecognize Fashion\r\nLoad the data\r\nPreprocess the data\r\nBuild the model\r\nEvaluate Accuracy\r\nMake predictions\r\nHow well did we do?\r\n\r\nRecognize Animals and\r\nObjects\r\nConvolutional Neural\r\nNetwork\r\nTransfer Learning\r\nVisualize performance\r\nSave the model\r\nEvaluate the model\r\n\r\nReferences\r\n\r\nRecognize Handwritten Digits\r\nComputer vision as a sub-field of deep learning has exploaded over\r\nthe last decade. The advent of better computers, readily available data\r\nsources, and explosively intelligent models with very little code has\r\nmade the unthinkable doable, and quickly.\r\nLibraries\r\n\r\n\r\nlibrary(keras)\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\nFirst we will grab the MNIST dataset. This consists of an array of\r\n28x28 images with 10 classification labels.\r\nData Retrieval\r\n\r\n\r\n\r\n\r\n\r\nmnist %>% names\r\n\r\n[1] \"train\" \"test\" \r\n\r\nWe can save the shapes and number of classes for later.\r\nData Shapes\r\n\r\n\r\n# Get the width and height\r\nWIDTH = dim(mnist$train$x)[[2]]\r\nHEIGHT = dim(mnist$train$x)[[3]]\r\n\r\n\r\n# Get unique number of classes\r\nCLASSES = length(unique(mnist$train$y))\r\n\r\nmnist$train$x %>% dim\r\n\r\n[1] 60000    28    28\r\n\r\nmnist$train$y %>% dim\r\n\r\n[1] 60000\r\n\r\nmnist$test$x %>% dim\r\n\r\n[1] 10000    28    28\r\n\r\nmnist$test$y %>% dim\r\n\r\n[1] 10000\r\n\r\nVisualize Images\r\nNext we can visualize a few images using the plot\r\nfunction in r. This was a little weird at first, because the images\r\nsometimes need standardized for rgb values depending on the function and\r\ndata shape.\r\n\r\n\r\nlibrary(raster)\r\nplot_a_few <- function(x,y, a_few = 3, rgb_dim=FALSE){\r\n    # Render a few images\r\n    rand_image_index = sample(1:dim(x)[[1]], size = a_few)\r\n    par(mar=c(0, 0, 0, 0))\r\n    for(i in rand_image_index){\r\n      if(rgb_dim){\r\n        img = x[i,,,]\r\n      }\r\n      else{\r\n        img = x[i,,]\r\n        # image(img, useRaster=TRUE, axes=FALSE)\r\n      }\r\n      \r\n      plot(as.raster(img))\r\n      label = y[i]\r\n      print(label)\r\n    }\r\n}\r\n\r\nplot_a_few(mnist$train$x, mnist$train$y, a_few=3)\r\n\r\n\r\n[1] 3\r\n\r\n[1] 2\r\n\r\n[1] 9\r\n\r\nModel\r\nSimple Dense Model\r\nThe simplest model will take the image tensor and flatten it into the\r\nstandard feed forward format. The prediction is over our\r\nCLASSES which is 10.\r\n\r\n\r\n# Simple model\r\nmodel <- keras::keras_model_sequential() %>% \r\n            keras::layer_flatten(input_shape = c(WIDTH, HEIGHT), \r\n                                 name = \"mnist_flatten_input\") %>% \r\n            keras::layer_dense(units = 128, activation = \"relu\", \r\n                               name = \"mnist_dense\") %>% \r\n            keras::layer_dropout(0.2, name = \"mnist_dropout\") %>% \r\n            keras::layer_dense(CLASSES, activation = \"softmax\", \r\n                               name = \"mnist_dense_output\")\r\nmodel\r\n\r\nModel: \"sequential\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nmnist_flatten_input (Flatten)  (None, 784)                 0          \r\n______________________________________________________________________\r\nmnist_dense (Dense)            (None, 128)                 100480     \r\n______________________________________________________________________\r\nmnist_dropout (Dropout)        (None, 128)                 0          \r\n______________________________________________________________________\r\nmnist_dense_output (Dense)     (None, 10)                  1290       \r\n======================================================================\r\nTotal params: 101,770\r\nTrainable params: 101,770\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nSummary\r\n\r\n\r\n# Some summary statistics\r\nbase::summary(model)\r\n\r\nModel: \"sequential\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nmnist_flatten_input (Flatten)  (None, 784)                 0          \r\n______________________________________________________________________\r\nmnist_dense (Dense)            (None, 128)                 100480     \r\n______________________________________________________________________\r\nmnist_dropout (Dropout)        (None, 128)                 0          \r\n______________________________________________________________________\r\nmnist_dense_output (Dense)     (None, 10)                  1290       \r\n======================================================================\r\nTotal params: 101,770\r\nTrainable params: 101,770\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nCompile\r\nReminder that sparse_categorical_crossentropy is for\r\nnon-matrix like y values. This will do it for you.\r\nOtherwise, you need to use the to_categorical function to\r\ntransform the y vector into a matrix.\r\n\r\n\r\n# Compile the model\r\nmodel %>% \r\n  keras::compile(\r\n    loss = \"sparse_categorical_crossentropy\",\r\n    optimizer = \"adam\",\r\n    metrics = \"accuracy\"\r\n  )\r\n\r\n\r\nFit\r\n\r\n\r\n# Fit the model\r\nhistory = model %>% \r\n            keras::fit(\r\n              x = mnist$train$x, y = mnist$train$y,\r\n              epochs = 5,\r\n              validation_split = 0.3,\r\n              verbose = 2\r\n            )\r\n\r\n\r\nVisualize\r\n\r\n\r\nplot_history_metrics = function(history){\r\n    # Plot fit results - loss and accuracy for this model\r\n    tmp = data.frame(history$metrics) %>% dplyr::mutate(epoch = row_number())\r\n    plt1 = ggplot(data=tmp) +\r\n            geom_line(aes(x=epoch, y = loss, color=\"training loss\")) +\r\n            geom_line(aes(x=epoch, y = val_loss, color=\"validation loss\")) +\r\n            theme_bw() +\r\n            labs(color=\"Legend\") + \r\n            ggtitle(\"Model Loss\")\r\n    plt1\r\n    \r\n    \r\n    plt2 = ggplot(data=tmp) +\r\n            geom_line(aes(x=epoch, y = accuracy, color=\"training accuracy\")) +\r\n            geom_line(aes(x=epoch, y = val_accuracy, color=\"validation accuracy\")) +\r\n            theme_bw() +\r\n            labs(color=\"Legend\") + \r\n            ggtitle(\"Model Accuracy\")\r\n    plt2\r\n    \r\n    list(loss_plot = plt1, acc_plot = plt2)\r\n}\r\n\r\nplot_history_metrics(history)\r\n\r\n$loss_plot\r\n\r\n\r\n$acc_plot\r\n\r\n\r\nAnother way\r\nAnother equally valid way as oppose to flattening the input as an\r\narray is to do it explicitely on the outside. This can be done use the\r\narray_reshape function. We can also make our y values into\r\na categorical matrix using the to_Categorical function.\r\nThis will change our sparse_categorical_crossentropy into\r\ncategorical_crossentropy. A tricky distinction, but one\r\ndoesn’t expect a matrix, one does.\r\n\r\n\r\nx_train <- keras::array_reshape(mnist$train$x, c(nrow(mnist$train$x), WIDTH*HEIGHT))\r\nx_test <- keras::array_reshape(mnist$test$x, c(nrow(mnist$test$x), WIDTH*HEIGHT))\r\ny_train <- keras::to_categorical(mnist$train$y, 10)\r\ny_test <- keras::to_categorical(mnist$test$y, 10)\r\n\r\nx_test %>% dim\r\n\r\n[1] 10000   784\r\n\r\ny_test %>% head\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\r\n[1,]    0    0    0    0    0    0    0    1    0     0\r\n[2,]    0    0    1    0    0    0    0    0    0     0\r\n[3,]    0    1    0    0    0    0    0    0    0     0\r\n[4,]    1    0    0    0    0    0    0    0    0     0\r\n[5,]    0    0    0    0    1    0    0    0    0     0\r\n[6,]    0    1    0    0    0    0    0    0    0     0\r\n\r\n\r\n\r\n# Model pre-flattened for shape and made categorically long in y\r\nmodel <- keras::keras_model_sequential() %>%  \r\n            keras::layer_dense(input_shape = c(WIDTH*HEIGHT), \r\n                               units = 128, \r\n                               activation = \"relu\", \r\n                               name = \"mnist_dense\") %>% \r\n            keras::layer_dropout(0.2, \r\n                                 name = \"mnist_dropout\") %>% \r\n            keras::layer_dense(CLASSES, \r\n                               activation = \"softmax\", \r\n                               name = \"mnist_dense_output\")\r\nmodel\r\n\r\nModel: \"sequential_1\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nmnist_dense (Dense)            (None, 128)                 100480     \r\n______________________________________________________________________\r\nmnist_dropout (Dropout)        (None, 128)                 0          \r\n______________________________________________________________________\r\nmnist_dense_output (Dense)     (None, 10)                  1290       \r\n======================================================================\r\nTotal params: 101,770\r\nTrainable params: 101,770\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nSummary\r\n\r\n\r\n# Model architectures\r\nbase::summary(model)\r\n\r\nModel: \"sequential_1\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nmnist_dense (Dense)            (None, 128)                 100480     \r\n______________________________________________________________________\r\nmnist_dropout (Dropout)        (None, 128)                 0          \r\n______________________________________________________________________\r\nmnist_dense_output (Dense)     (None, 10)                  1290       \r\n======================================================================\r\nTotal params: 101,770\r\nTrainable params: 101,770\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nCompile\r\n\r\n\r\n# Compile the model\r\nmodel %>% \r\n  keras::compile(\r\n    # loss = \"sparse_categorical_crossentropy\",\r\n    loss = \"categorical_crossentropy\",\r\n    optimizer = \"adam\",\r\n    metrics = \"accuracy\"\r\n  )\r\n\r\n\r\nFit\r\nOnce we configure our model, we can compile it,\r\nfit, then plot to see the performance. Turns\r\nout, you can just do plot(history) and the function to plot\r\nthese metrics is entirely superfluous.\r\n\r\n\r\nhistory = model %>% keras::fit(\r\n  x = x_train, y = y_train,\r\n  validation_split = 0.3,\r\n  epochs = 5,\r\n  verbose = 2\r\n)\r\n\r\n\r\nVisualize\r\n\r\n\r\nplot_history_metrics = function(history){\r\n    # Plot fit results - loss and accuracy for this model\r\n    tmp = data.frame(history$metrics) %>% dplyr::mutate(epoch = row_number())\r\n    plt1 = ggplot(data=tmp) +\r\n            geom_line(aes(x=epoch, y = loss, color=\"training loss\")) +\r\n            geom_line(aes(x=epoch, y = val_loss, color=\"validation loss\")) +\r\n            theme_bw() +\r\n            labs(color=\"Legend\") + \r\n            ggtitle(\"Model Loss\")\r\n    plt1\r\n    \r\n    \r\n    plt2 = ggplot(data=tmp) +\r\n            geom_line(aes(x=epoch, y = accuracy, color=\"training accuracy\")) +\r\n            geom_line(aes(x=epoch, y = val_accuracy, color=\"validation accuracy\")) +\r\n            theme_bw() +\r\n            labs(color=\"Legend\") + \r\n            ggtitle(\"Model Accuracy\")\r\n    plt2\r\n    \r\n    list(loss_plot = plt1, acc_plot = plt2)\r\n}\r\n\r\nplot_history_metrics(history)\r\n\r\n$loss_plot\r\n\r\n\r\n$acc_plot\r\n\r\n\r\nPredictions\r\n\r\n\r\n# Generate some predictions on the unseen data\r\npredictions = stats::predict(model, x_test)\r\npredictions %>% head()\r\n\r\n             [,1]         [,2]         [,3]         [,4]         [,5]\r\n[1,] 2.227834e-07 1.146102e-07 3.779165e-06 8.758837e-04 3.697259e-09\r\n[2,] 8.023340e-08 2.998142e-05 9.999614e-01 6.299148e-06 1.639815e-13\r\n[3,] 2.070014e-07 9.994919e-01 3.830419e-05 3.406002e-06 4.921339e-05\r\n[4,] 9.996891e-01 2.887514e-08 1.701280e-05 6.713914e-08 3.676894e-08\r\n[5,] 7.651256e-07 1.150332e-11 3.691493e-07 4.230928e-09 9.981548e-01\r\n[6,] 2.478353e-09 9.999220e-01 4.330399e-07 1.326177e-07 7.487398e-07\r\n             [,6]         [,7]         [,8]         [,9]        [,10]\r\n[1,] 2.292366e-07 3.003187e-12 9.991176e-01 1.755072e-07 2.135613e-06\r\n[2,] 1.473648e-07 5.765563e-08 5.105388e-11 2.201943e-06 1.194979e-12\r\n[3,] 6.619522e-06 3.123074e-06 3.831029e-04 2.146841e-05 2.597842e-06\r\n[4,] 3.171424e-07 3.722662e-07 2.930266e-04 6.135127e-09 4.965008e-08\r\n[5,] 2.048299e-08 1.496129e-06 1.140888e-04 7.702196e-07 1.727599e-03\r\n[6,] 3.147493e-08 4.829987e-09 7.620423e-05 3.231777e-07 1.556567e-07\r\n\r\nEvaluate\r\n\r\n\r\n# Evaluate performance\r\n# test_results = model %>% \r\n#                   evaluate(mnist$test$x, mnist$test$y, verbose = 0)\r\n# test_results\r\n\r\n\r\nSave Model\r\nOne thing keras makes incredibly easy is the ability to save your\r\nmodel. This will create a folder and allow for easy access to and from\r\nyour model if you need it for predictions in another environment or\r\nAPI.\r\n\r\n\r\n# Serialize the model (it becomes a folder)\r\nkeras::save_model_tf(object = model, filepath = \"mnist_model\")\r\n\r\n\r\nReload Model\r\n\r\n\r\n# Reload the model\r\nreloaded_model = keras::load_model_tf(\"mnist_model\")\r\nreloaded_model %>% summary\r\n\r\nModel: \"sequential_1\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nmnist_dense (Dense)            (None, 128)                 100480     \r\n______________________________________________________________________\r\nmnist_dropout (Dropout)        (None, 128)                 0          \r\n______________________________________________________________________\r\nmnist_dense_output (Dense)     (None, 10)                  1290       \r\n======================================================================\r\nTotal params: 101,770\r\nTrainable params: 101,770\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nbase::all.equal(stats::predict(model, x_test), \r\n                stats::predict(reloaded_model, x_test))\r\n\r\n[1] TRUE\r\n\r\nRecognize Fashion\r\nRecognizing other types of objects is just as easy as before. Let’s\r\nrepeat our steps for a new dataset, because practice makes\r\nperfect!\r\nLoad the data\r\n\r\n\r\nfashion_mnist <- dataset_fashion_mnist()\r\n\r\nc(train_images, train_labels) %<-% fashion_mnist$train\r\nc(test_images, test_labels) %<-% fashion_mnist$test\r\n\r\n\r\n\r\n\r\nclass_names = c('T-shirt/top',\r\n                'Trouser',\r\n                'Pullover',\r\n                'Dress',\r\n                'Coat', \r\n                'Sandal',\r\n                'Shirt',\r\n                'Sneaker',\r\n                'Bag',\r\n                'Ankle boot')\r\n\r\n\r\n\r\n\r\ndim(train_images)\r\n\r\n[1] 60000    28    28\r\n\r\ndim(train_labels)\r\n\r\n[1] 60000\r\n\r\ntrain_labels[1:20]\r\n\r\n [1] 9 0 0 3 0 2 7 2 5 5 0 9 5 5 7 9 1 0 6 4\r\n\r\ndim(test_images)\r\n\r\n[1] 10000    28    28\r\n\r\ndim(test_labels)\r\n\r\n[1] 10000\r\n\r\nPreprocess the data\r\n\r\n\r\nlibrary(tidyr)\r\nlibrary(ggplot2)\r\n\r\n\r\nimage_1 <- as.data.frame(train_images[1,,])\r\ncolnames(image_1) <- seq_len(ncol(image_1))\r\nimage_1$y <- seq_len(nrow(image_1))\r\nimage_1 <- tidyr::gather(image_1, key = \"x\", value = \"value\", -y)\r\nimage_1$x <- as.integer(image_1$x)\r\n\r\nggplot(image_1, aes(x=x,y=y,fill=value)) +\r\n  geom_tile() +\r\n  scale_fill_gradient(low = \"white\", high = \"black\", na.value = NA) +\r\n  scale_y_reverse() +\r\n  theme_minimal() +\r\n  theme(panel.grid = element_blank()) +\r\n  theme(aspect.ratio = 1) +\r\n  xlab(\"\") +\r\n  ylab(\"\") +\r\n  ggtitle(paste(class_names[train_labels[1]+1]))\r\n\r\n\r\n\r\n\r\n\r\ntrain_images <- train_images / 255\r\ntest_images <- test_images / 255\r\n\r\npar(mfcol = c(5,5))\r\npar(mar=c(0,0,1.5,0), axs='i', yaxs='i')\r\nfor(i in 1:25){\r\n  img <- train_images[i,,]\r\n  # img <- t(apply(img, 2, rev))\r\n  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n', main = paste(class_names[train_labels[i]+1]))\r\n}\r\n\r\n\r\n\r\nBuild the model\r\n\r\n\r\nmodel <- keras_model_sequential() %>%\r\n  layer_flatten(input_shape = c(28, 28)) %>% \r\n  layer_dense(units = 128, activation = 'relu') %>% \r\n  layer_dense(units = 10, activation = 'softmax')\r\n\r\nmodel %>% compile(\r\n  optimizer = 'adam', \r\n  loss = 'sparse_categorical_crossentropy',\r\n  metrics = c('accuracy')\r\n)\r\n\r\nmodel %>% \r\n  fit(x=train_images, y=train_labels, \r\n      epochs = 5, verbose = 2, validation_split=0.3)\r\n\r\n\r\nEvaluate Accuracy\r\n\r\n\r\nscore <- model %>% evaluate(test_images, test_labels, verbose = 0)\r\n\r\ncat('Test loss:', score[1], \"\\n\")\r\n\r\nTest loss: 0.3666847 \r\n\r\ncat('Test accuracy:', score[2], \"\\n\")\r\n\r\nTest accuracy: 0.8697 \r\n\r\nMake predictions\r\n\r\n\r\npredictions <- model %>% predict(test_images)\r\npredictions %>% head\r\n\r\n             [,1]         [,2]         [,3]         [,4]         [,5]\r\n[1,] 1.604551e-06 1.876069e-08 9.970527e-07 8.895261e-07 1.813439e-06\r\n[2,] 2.315823e-05 5.689982e-11 9.929472e-01 2.342679e-07 8.066850e-04\r\n[3,] 8.755771e-06 9.999907e-01 2.348761e-07 2.407484e-07 9.683061e-08\r\n[4,] 9.201439e-06 9.999119e-01 9.318416e-07 7.628213e-05 1.573179e-06\r\n[5,] 9.377546e-02 3.622059e-06 7.261117e-02 1.071351e-03 5.222877e-03\r\n[6,] 8.985694e-04 9.989796e-01 5.242249e-05 1.027616e-05 2.311750e-05\r\n             [,6]         [,7]         [,8]         [,9]        [,10]\r\n[1,] 6.241337e-04 2.464743e-05 2.187973e-02 1.042031e-05 9.774557e-01\r\n[2,] 4.651124e-13 6.222541e-03 8.701920e-12 8.015744e-08 2.261251e-12\r\n[3,] 4.624537e-11 2.501358e-08 2.490507e-13 3.212504e-09 6.252807e-12\r\n[4,] 2.287331e-09 1.668286e-07 1.444669e-10 1.290671e-08 1.131530e-09\r\n[5,] 5.195885e-07 8.264931e-01 1.605505e-06 8.186871e-04 1.657291e-06\r\n[6,] 8.518493e-09 3.598326e-05 5.994213e-10 1.746233e-07 6.766231e-09\r\n\r\npreds = apply(predictions, 1, which.max)\r\npreds %>% head\r\n\r\n[1] 10  3  2  2  7  2\r\n\r\n#or\r\n\r\npreds = model %>% predict_classes(x = test_images)\r\npreds %>% unique\r\n\r\n [1] 9 2 1 6 4 5 7 3 8 0\r\n\r\nHow well did we do?\r\n\r\n\r\npar(mfcol=c(5,5))\r\npar(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')\r\nfor (i in 1:25) { \r\n  img <- test_images[i, , ]\r\n  img <- t(apply(img, 2, rev)) \r\n  # subtract 1 as labels go from 0 to 9\r\n  predicted_label <- which.max(predictions[i, ]) - 1\r\n  true_label <- test_labels[i]\r\n  if (predicted_label == true_label) {\r\n    color <- '#008800' \r\n  } else {\r\n    color <- '#bb0000'\r\n  }\r\n  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',\r\n        main = paste0(class_names[predicted_label + 1], \" (\",\r\n                      class_names[true_label + 1], \")\"),\r\n        col.main = color)\r\n}\r\n\r\n\r\n\r\n\r\n\r\ntrain_images %>% dim\r\n\r\n[1] 60000    28    28\r\n\r\nRecognize Animals and\r\nObjects\r\n\r\n\r\nlibrary(tensorflow)\r\nlibrary(keras)\r\n\r\ncifar <- dataset_cifar10()\r\n\r\nclass_names <- c('airplane', 'automobile', 'bird', 'cat', 'deer',\r\n               'dog', 'frog', 'horse', 'ship', 'truck')\r\n\r\nindex <- 1:30\r\n\r\npar(mfcol = c(5,6), mar = rep(1,4), oma=rep(0.2, 4))\r\ncifar$train$x[index,,,] %>% \r\n  purrr::array_tree(margin=1) %>% \r\n  purrr::set_names(class_names[cifar$train$y[index] + 1]) %>% \r\n  purrr::map(as.raster, max = 255) %>% \r\n  purrr::iwalk(~{plot(.x); title(.y)})\r\n\r\n\r\n\r\nConvolutional Neural Network\r\n\r\n\r\nmodel <- keras_model_sequential() %>% \r\n  layer_conv_2d(input_shape = c(32, 32, 3),  filters = 32, kernel_size = c(3,3), activation = \"relu\") %>% \r\n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \r\n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = \"relu\") %>% \r\n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \r\n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = \"relu\") %>% \r\n  layer_flatten() %>% \r\n  layer_dense(units = 64, activation = \"relu\") %>% \r\n  layer_dense(units = 10, activation = \"softmax\")\r\n\r\nsummary(model)\r\n\r\nModel: \"sequential_3\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nconv2d_2 (Conv2D)              (None, 30, 30, 32)          896        \r\n______________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2D) (None, 15, 15, 32)          0          \r\n______________________________________________________________________\r\nconv2d_1 (Conv2D)              (None, 13, 13, 64)          18496      \r\n______________________________________________________________________\r\nmax_pooling2d (MaxPooling2D)   (None, 6, 6, 64)            0          \r\n______________________________________________________________________\r\nconv2d (Conv2D)                (None, 4, 4, 64)            36928      \r\n______________________________________________________________________\r\nflatten_1 (Flatten)            (None, 1024)                0          \r\n______________________________________________________________________\r\ndense_3 (Dense)                (None, 64)                  65600      \r\n______________________________________________________________________\r\ndense_2 (Dense)                (None, 10)                  650        \r\n======================================================================\r\nTotal params: 122,570\r\nTrainable params: 122,570\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nmodel %>% compile(\r\n  optimizer = \"adam\",\r\n  loss = \"sparse_categorical_crossentropy\",\r\n  metrics = \"accuracy\"\r\n)\r\n\r\n\r\nhistory <- model %>% \r\n  fit(\r\n    x = cifar$train$x, y = cifar$train$y,\r\n    epochs = 10,\r\n    validation_data = unname(cifar$test),\r\n    verbose = 2\r\n  )\r\n\r\n\r\n\r\n\r\nevaluate(model, cifar$test$x, cifar$test$y, verbose = 0)\r\n\r\n    loss accuracy \r\n1.079032 0.660300 \r\n\r\nTransfer Learning\r\nWe will not actually train here, because it takes about 45 minutes.\r\nBut we will just unload the model previously trained. But the idea is to\r\ntake a layer or many layers from a previouos model and then stack our\r\nmodel on top of it. You don’t have to stack it “on top”, but I chose to\r\nhere for simplicity. By taking what the previous model learned, we then\r\nput our custom output layers there so it can learn to classify new\r\nthings, with old feature vectors it learned from the\r\nimagenet data set.\r\n\r\n\r\nlibrary(devtools)\r\nlibrary(tfhub)\r\nlibrary(keras)\r\nlibrary(reticulate)\r\n\r\nc(train_images, train_labels) %<-% cifar$train\r\nc(test_images, test_labels) %<-% cifar$test\r\n\r\ntrain_images %>% dim\r\n\r\n[1] 50000    32    32     3\r\n\r\ntrain_labels %>% dim\r\n\r\n[1] 50000     1\r\n\r\ntest_images %>% dim\r\n\r\n[1] 10000    32    32     3\r\n\r\ntest_labels %>% dim  \r\n\r\n[1] 10000     1\r\n\r\nimage_shape <- c(32,32,3)\r\n\r\nconv_base <- keras::application_resnet101(weights = \"imagenet\",\r\n                                          include_top = FALSE, \r\n                                          input_shape = c(32,32,3))\r\n\r\n\r\nfreeze_weights(conv_base)\r\n\r\nmodel <- keras_model_sequential() %>%\r\n  conv_base %>%\r\n  layer_flatten() %>%\r\n  # layer_reshape(c(1,2048)) %>% \r\n  layer_dense(units = 256, activation = \"relu\") %>%\r\n  layer_dense(units = 10, activation = \"softmax\")\r\n\r\n\r\nmodel %>% compile(\r\n  optimizer = \"adam\",\r\n  loss = \"sparse_categorical_crossentropy\",\r\n  metrics = \"accuracy\"\r\n)\r\n\r\n\r\n# Uncommented training (fit) section. Just load previous model. Takes 45 mins to train/update from the base model.\r\n\r\n# unfreeze_weights(conv_base, from = \"block5_conv1\")\r\n\r\n# history <- model %>% fit(\r\n#   x=train_images, y=train_labels, \r\n#   validation_split = 0.3,\r\n#   epochs=10, \r\n#   verbose = 2\r\n# )\r\n\r\nmodel = keras::load_model_tf(\"cifar10_tl_model\")\r\nmodel %>% summary\r\n\r\nModel: \"sequential_10\"\r\n___________________________________________________________\r\nLayer (type)              Output Shape            Param #  \r\n===========================================================\r\nresnet101 (Functional)    (None, 1, 1, 2048)      42658176 \r\n___________________________________________________________\r\nflatten_5 (Flatten)       (None, 2048)            0        \r\n___________________________________________________________\r\ndense_24 (Dense)          (None, 256)             524544   \r\n___________________________________________________________\r\ndense_23 (Dense)          (None, 10)              2570     \r\n===========================================================\r\nTotal params: 43,185,290\r\nTrainable params: 43,079,946\r\nNon-trainable params: 105,344\r\n___________________________________________________________\r\n\r\n# summary(model)\r\n# train_images[1,,,] %>% dim\r\n# train_labels[1]\r\n\r\n\r\nVisualize performance\r\n\r\n\r\n# plot(history)\r\n\r\n\r\nSave the model\r\nThe following code is used to serialize the model, since this is\r\nalready done and the process is fairly intensive, we will not be\r\nrepeating it here.\r\n\r\n\r\n# # Serialize the model (it becomes a folder)\r\n# keras::save_model_tf(object = model, filepath = \"cifar10_tl_model\")\r\n# \r\n# # Reload the model\r\n# reloaded_model = keras::load_model_tf(\"cifar10_tl_model\")\r\n# reloaded_model %>% summary\r\n\r\n\r\nEvaluate the model\r\n\r\n\r\nevaluate(model, x = test_images, y = test_labels)\r\n\r\n    loss accuracy \r\n1.747058 0.578400 \r\n\r\nReferences\r\nhttps://tensorflow.rstudio.com/tutorials/beginners/\r\nhttps://tensorflow.rstudio.com/tutorials/advanced/images/cnn/\r\nhttps://tensorflow.rstudio.com/tutorials/advanced/images/transfer-learning-hub/\r\nhttps://keras.rstudio.com/reference/freeze_layers.html\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-13-image-recognition-in-r/image-recognition-in-r_files/figure-html5/visualize-1.png",
    "last_modified": "2022-07-14T12:07:30-04:00",
    "input_file": "image-recognition-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-11-transportation-problem-in-r/",
    "title": "Transportation Problem in R",
    "description": "In this post we will learn how to solve supply and demand problems with the transportation problem and linear programming.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-12",
    "categories": [
      "network flows",
      "linear programming",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nMetropolitan Power Plant\r\nSupply\r\nThe Problem\r\nDescription\r\nMathematical\r\nFormulation\r\nData\r\nA Quick Visual\r\n\r\n\r\nLP Model Formulation\r\nObjective\r\nConstraints, Direction, and\r\nRHS\r\nSolution\r\nVisualization\r\n\r\n\r\nAn Easier Way\r\n\r\nMetropolitan Power Plant\r\nSupply\r\nThe Problem\r\nDescription\r\nOne of the most common daily tasks is that of transportation. From\r\nour bed to the dining room table, or from home to work and back, all of\r\nthese examples have limited supply some pre-determined\r\ndemand and an implicit cost in order\r\nto carry it out. The question is, what is the best way to do it? It\r\nwould be silly to travel to go home before you pick up dinner, then go\r\nback out again to do it; we seek to minimize our cost\r\nwhen transporting goods. This can be time, money, risk, or anything else\r\nthat matters.\r\nIn our example we will consider a power plant that needs to supply\r\nelectricity (or any resource for that matter) to a city. All of the\r\ncosts incurred to transport the electricity are known, these will be in\r\nUSD. Supply from each power plant is known. Demand\r\nto each city is also known. The problem is\r\nMathematical Formulation\r\nThe classical linear programming formulation goes as follows:\r\nObjective\r\n\\[\r\nMinimize. \\sum_{i,j}{x_{ij}c_{ij}}\r\n\\]\r\n\\[\r\ns.t.\r\n\\]\r\nSupply Constraints\r\n\\[\r\n\\sum_{j}{x_{ij}} \\le s_i \\: \\forall i \\in S\r\n\\]\r\nDemand Constraints\r\n\\[\r\n\\sum_{i}{x_{ij}} \\ge d_j \\: \\forall j \\in D\r\n\\]\r\nNon-negativity Constraints\r\n\\[\r\nx_{ij} \\ge 0 \\: \\forall (i,j)\\in A\r\n\\]\r\nData\r\nFirst we need to get our data. This will come in the form of an\r\nadjacency matrix which we will readily convert into an arc\r\nmatrix for reasons that will become clear soon (building constraints for\r\nthe LP).\r\n\r\n\r\n#Number of plants and cities\r\nNUM_POWER_PLANTS = 3\r\nNUM_CITIES = 4\r\n\r\n# Adjacency matrix\r\nadj.matrix = matrix(c(8, 6, 10, 9, 9, 12, 13, 7, 14, 9, 16, 5), \r\n                    nrow = NUM_POWER_PLANTS, \r\n                    ncol = NUM_CITIES)\r\nadj.matrix\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]    8    9   13    9\r\n[2,]    6    9    7   16\r\n[3,]   10   12   14    5\r\n\r\nNext we need to take a look at our supply and demand.\r\n\r\n\r\n# Supply and demand vectors\r\nsupply <- c(35, 50, 40)\r\ndemand <- c(45, 20, 30, 30)\r\nsupply\r\n\r\n[1] 35 50 40\r\n\r\ndemand\r\n\r\n[1] 45 20 30 30\r\n\r\nNext define the Power Plants and Cities.\r\n\r\n\r\n# Supply node id lookup\r\nPOWER_PLANTS = 1:3\r\nPOWER_PLANT_LABELS <- sprintf(\"Plant %s\", 1:3)\r\n\r\n# Demand node id lookup\r\nCITIES = 1:4\r\nCITIES_LABELS <- sprintf(\"City %s\", 1:4)\r\n\r\nPOWER_PLANT_LABELS\r\n\r\n[1] \"Plant 1\" \"Plant 2\" \"Plant 3\"\r\n\r\nCITIES_LABELS\r\n\r\n[1] \"City 1\" \"City 2\" \"City 3\" \"City 4\"\r\n\r\nFor the nodes we will supply them with some metatdata into the\r\nDiagrammeR package for slick plotting capability.\r\n\r\n\r\n# Create the powerplant nodes\r\npowerplant_nodes <- DiagrammeR::create_node_df(nodes = POWER_PLANTS,\r\n                                  label = POWER_PLANT_LABELS,\r\n                                  style = \"filled\",\r\n                                  type = \"upper\",\r\n                                  color = rep(\"green\", length(POWER_PLANTS)),\r\n                                  shape = rep(\"circle\", length(POWER_PLANTS)),\r\n                                  data = supply,\r\n                                  n = length(POWER_PLANTS)\r\n                                  )\r\n\r\n# Create the city nodes\r\ncity_nodes <- DiagrammeR::create_node_df(nodes = CITIES,\r\n                                  label = CITIES_LABELS,\r\n                                  style = \"filled\",\r\n                                  type = \"lower\",\r\n                                  color = rep(\"red\", length(CITIES)),\r\n                                  shape = rep(\"square\", length(CITIES)),\r\n                                  data = demand,\r\n                                  n = length(CITIES)\r\n                                  )\r\n\r\n# Create the DiagrammeR dataframe\r\nnodes <- DiagrammeR::combine_ndfs(powerplant_nodes, city_nodes)\r\nnodes\r\n\r\n  id  type   label nodes  style color  shape data\r\n1  1 upper Plant 1     1 filled green circle   35\r\n2  2 upper Plant 2     2 filled green circle   50\r\n3  3 upper Plant 3     3 filled green circle   40\r\n4  4 lower  City 1     1 filled   red square   45\r\n5  5 lower  City 2     2 filled   red square   20\r\n6  6 lower  City 3     3 filled   red square   30\r\n7  7 lower  City 4     4 filled   red square   30\r\n\r\nOne way to convert the adjacency matrix into a\r\nfriendlier edge format is to simply gather it with some\r\nbasic naming of the rows and columns. Once we have the\r\ntidy_edges we can acquire the node_id for each\r\nby doing a little joining with the nodes table formed\r\nabove. This will supply the id's required for the\r\nDiagrammeR package.\r\n\r\n\r\n# Create the \"from\" \"to\" representation, from the raw matrix form\r\ntmp <- adj.matrix %>% as.data.frame\r\nnames(tmp) <- CITIES_LABELS\r\ntmp$from <- POWER_PLANT_LABELS\r\ntidy_edges <- tmp %>% \r\n                tidyr::gather(., key = \"to\", value = \"data\", -c(from)) %>%\r\n                dplyr::mutate(color = \"black\", rel = \"requires\")\r\n\r\n# Go find the from_id\r\nfrom_id = tidy_edges %>% \r\n            dplyr::inner_join(nodes, by = c(\"from\"=\"label\")) %>% \r\n            dplyr::transmute(from_id = id) %>%\r\n            dplyr::pull(from_id)\r\n\r\n# Go find the to_id\r\nto_id = tidy_edges %>% \r\n            dplyr::inner_join(nodes, by = c(\"to\"=\"label\")) %>% \r\n            dplyr::transmute(to_id = id) %>% \r\n            dplyr::pull(to_id)\r\n\r\n\r\n# Get the from_id\r\ntidy_edges$from_id = from_id\r\n\r\n# Get the to_id\r\ntidy_edges$to_id = to_id\r\n\r\nedges <- DiagrammeR::create_edge_df(from = from_id,\r\n                                    to = to_id,\r\n                                    rel = tidy_edges$rel,\r\n                                    color = tidy_edges$color,\r\n                                    data = tidy_edges$data,\r\n                                    label = tidy_edges$data)\r\n\r\n\r\n# Always a good idea to short your edges logically\r\nedges <- edges %>% \r\n            dplyr::arrange(-desc(from), -desc(to)) %>% \r\n            dplyr::mutate(id = row_number())\r\nedges\r\n\r\n   id from to      rel color data label\r\n1   1    1  4 requires black    8     8\r\n2   2    1  5 requires black    9     9\r\n3   3    1  6 requires black   13    13\r\n4   4    1  7 requires black    9     9\r\n5   5    2  4 requires black    6     6\r\n6   6    2  5 requires black    9     9\r\n7   7    2  6 requires black    7     7\r\n8   8    2  7 requires black   16    16\r\n9   9    3  4 requires black   10    10\r\n10 10    3  5 requires black   12    12\r\n11 11    3  6 requires black   14    14\r\n12 12    3  7 requires black    5     5\r\n\r\nA Quick Visual\r\n\r\n\r\n# Create the graph\r\ng = DiagrammeR::create_graph(nodes_df = nodes,\r\n                             edges_df = edges, \r\n                             directed = T, \r\n                             graph_name = \"transportation\")\r\n\r\n# Render the graph\r\nDiagrammeR::render_graph(g, title = \"Power Plants Electricity Transportation to Cities\")\r\n\r\n\r\n\r\nAnother way to visualize this is by converting it to an igraph, which\r\nlooks pretty horrible.\r\n\r\n\r\nig <- DiagrammeR::to_igraph(g)\r\nplot(ig, vertex.size=30)\r\n\r\n\r\n\r\nLP Model Formulation\r\nObjective\r\nFirst things first, we know the costs on each arc already, because\r\nthey are supplied from our adjacency matrix initially. So\r\nwe can just go recover those.\r\n\r\n\r\n# Get objective value\r\nf.obj = edges %>% dplyr::pull(data)\r\nf.obj\r\n\r\n [1]  8  9 13  9  6  9  7 16 10 12 14  5\r\n\r\nConstraints, Direction, and\r\nRHS\r\nFirst some convenient numbering systems will be helpful to preserve\r\nknowledge of the nodes and which id is which.\r\n\r\n\r\n# Align ids up for matrix building\r\nPOWER_PLANT_NODE_IDS <- POWER_PLANTS\r\nCITY_NODE_IDS <- POWER_PLANTS[length(POWER_PLANTS)] + CITIES\r\n\r\nPOWER_PLANT_NODE_IDS\r\n\r\n[1] 1 2 3\r\n\r\nCITY_NODE_IDS\r\n\r\n[1] 4 5 6 7\r\n\r\nNext we need to build the supply constraints. So for each unique\r\npowerplant arc, then sum of it’s outbound arcs must be less than the\r\nsupply constraint, for all supply.\r\n\r\n\r\n# One constraint row for each supply node\r\nSmat = matrix(0, nrow = length(POWER_PLANTS), ncol = nrow(edges))\r\nSmat_rhs = supply\r\nSmat_dir = rep(\"<=\", length(supply))\r\n\r\nfor(i in 1:nrow(Smat)){\r\n  for(j in 1:ncol(Smat)){\r\n    from = edges$from[j]\r\n    to = edges$to[j]\r\n    \r\n    if(from == POWER_PLANT_NODE_IDS[i]){\r\n      Smat[i, j] = 1\r\n    }\r\n  }\r\n}\r\n\r\nSmat\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\r\n[1,]    1    1    1    1    0    0    0    0    0     0     0     0\r\n[2,]    0    0    0    0    1    1    1    1    0     0     0     0\r\n[3,]    0    0    0    0    0    0    0    0    1     1     1     1\r\n\r\nSmat_rhs\r\n\r\n[1] 35 50 40\r\n\r\nSmat_dir\r\n\r\n[1] \"<=\" \"<=\" \"<=\"\r\n\r\nWith the same logic we need to construct the demand matrix.\r\n\r\n\r\n# One constraint node for each demand node\r\nDmat = matrix(0, nrow=length(CITIES), ncol = nrow(edges))\r\nDmat_rhs = demand\r\nDmat_dir = rep(\">=\", length(demand))\r\n\r\nfor(i in 1:nrow(Dmat)){\r\n  for(j in 1:ncol(Dmat)){\r\n    from = edges$from[j]\r\n    to = edges$to[j]\r\n    \r\n    if(to == CITY_NODE_IDS[i]){\r\n      Dmat[i, j] = 1\r\n    }\r\n  }\r\n}\r\n\r\nDmat\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\r\n[1,]    1    0    0    0    1    0    0    0    1     0     0     0\r\n[2,]    0    1    0    0    0    1    0    0    0     1     0     0\r\n[3,]    0    0    1    0    0    0    1    0    0     0     1     0\r\n[4,]    0    0    0    1    0    0    0    1    0     0     0     1\r\n\r\nDmat_rhs\r\n\r\n[1] 45 20 30 30\r\n\r\nDmat_dir\r\n\r\n[1] \">=\" \">=\" \">=\" \">=\"\r\n\r\nNow that we have our supply and demand data matrices put together,\r\nlet’s unify them and take a look at all of our constraints.\r\n\r\n\r\n# All `lpSolve` data\r\nf.obj <- f.obj\r\nf.cons <- rbind(Smat, Dmat)\r\nf.rhs <- c(Smat_rhs, Dmat_rhs)\r\nf.dir <- c(Smat_dir, Dmat_dir)\r\n\r\nf.obj\r\n\r\n [1]  8  9 13  9  6  9  7 16 10 12 14  5\r\n\r\nf.cons\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\r\n[1,]    1    1    1    1    0    0    0    0    0     0     0     0\r\n[2,]    0    0    0    0    1    1    1    1    0     0     0     0\r\n[3,]    0    0    0    0    0    0    0    0    1     1     1     1\r\n[4,]    1    0    0    0    1    0    0    0    1     0     0     0\r\n[5,]    0    1    0    0    0    1    0    0    0     1     0     0\r\n[6,]    0    0    1    0    0    0    1    0    0     0     1     0\r\n[7,]    0    0    0    1    0    0    0    1    0     0     0     1\r\n\r\nf.rhs\r\n\r\n[1] 35 50 40 45 20 30 30\r\n\r\nf.dir\r\n\r\n[1] \"<=\" \"<=\" \"<=\" \">=\" \">=\" \">=\" \">=\"\r\n\r\nSolution\r\nNow we unite everything together and drop it into the solver.\r\nRemember, we are trying to minimize cost in our objective, the\r\nconstraints will maximize the flow. So specify min.\r\n\r\n\r\n# Get the results\r\nresults <- lp(direction = \"min\",  \r\n              objective.in = f.obj, \r\n              const.mat = f.cons, \r\n              const.dir = f.dir, \r\n              const.rhs = f.rhs)\r\n\r\nresults$objval\r\n\r\n[1] 880\r\n\r\nmatrix(results$solution, nrow = length(supply), ncol = length(demand))\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]   15    0   30    0\r\n[2,]   20   20    0    0\r\n[3,]    0    0   10   30\r\n\r\nVisualization\r\nCool! We have our solution. It looks like:\r\nPlant 1 should supply City 1 and\r\nCity 2.\r\nPlant 2 should supply City 1 and\r\nCity 3, and\r\nPlant 3 should supply City 1 and\r\nCity 4.\r\nOne interesting insight is the shared responsibility all plants have\r\nin getting resources to City 1. Another insight is the\r\npartnerships to make City 1 supplied as cheaply as\r\npossible, may require some additional coordination costs to\r\ncarefully handle any error in costs between ill-shipments. The last\r\ninsights are the siloe’s, City 2 is entirely handled by\r\nPlant 1, City 3 by Plant 2, and\r\nCity 4 by Plant 3. This shows us the reliance\r\nwe have on those energy transfers going through, otherwise it’s not only\r\nlights out, it is also a more expensive transfer to get them back\r\non!\r\n\r\n\r\n# Add in the flow\r\nedges$flow <- results$solution\r\n\r\n# Color things if there exists flow\r\nedges <- edges %>% dplyr::mutate(weight = flow,\r\n                                 label = flow,\r\n                                 color = if_else(condition = flow > 0, true = \"black\",false = \"grey\"))\r\n\r\n# Create the graph\r\ng = DiagrammeR::create_graph(nodes_df = nodes,\r\n                             edges_df = edges, \r\n                             directed = T, \r\n                             graph_name = \"transportation\")\r\n\r\n# Draw the graph\r\nDiagrammeR::render_graph(g, title = \"Power Plants Electricity Transportation to Cities\")\r\n\r\n\r\n\r\n\r\n\r\n# ig <- DiagrammeR::to_igraph(g)\r\n\r\nnodes\r\n\r\n  id  type   label nodes  style color  shape data\r\n1  1 upper Plant 1     1 filled green circle   35\r\n2  2 upper Plant 2     2 filled green circle   50\r\n3  3 upper Plant 3     3 filled green circle   40\r\n4  4 lower  City 1     1 filled   red square   45\r\n5  5 lower  City 2     2 filled   red square   20\r\n6  6 lower  City 3     3 filled   red square   30\r\n7  7 lower  City 4     4 filled   red square   30\r\n\r\nvisNet_nodes <- nodes %>% \r\n                dplyr::mutate(shape = \"circle\",\r\n                              font = 12,\r\n                              size = data,)\r\nedges\r\n\r\n   id from to      rel color data label flow weight\r\n1   1    1  4 requires black    8    15   15     15\r\n2   2    1  5 requires black    9    20   20     20\r\n3   3    1  6 requires  grey   13     0    0      0\r\n4   4    1  7 requires  grey    9     0    0      0\r\n5   5    2  4 requires black    6    20   20     20\r\n6   6    2  5 requires  grey    9     0    0      0\r\n7   7    2  6 requires black    7    30   30     30\r\n8   8    2  7 requires  grey   16     0    0      0\r\n9   9    3  4 requires black   10    10   10     10\r\n10 10    3  5 requires  grey   12     0    0      0\r\n11 11    3  6 requires  grey   14     0    0      0\r\n12 12    3  7 requires black    5    30   30     30\r\n\r\nvisNet_edges <- edges %>% \r\n                dplyr::mutate(label = paste(\"(\",flow,\",\",data,\")\",sep=\"\"),\r\n                              length = 250,\r\n                              width = flow/5,\r\n                              arrows = \"to\") \r\n  \r\nvisNetwork(visNet_nodes, \r\n           visNet_edges,\r\n           width = \"100%\",\r\n           main = \"Power Distribution | (flow, cost)\")\r\n\r\n\r\n\r\nAn Easier Way\r\nIt’s almost spooky how easy it is with the lp.transport\r\nAPI. Verify that we got the same solution, 880 and the\r\ndecision vector is also the same. Since the objective value is the same,\r\nthe solutions might differ, because they are equally optimal. Pretty\r\ninteresting to see how easy it is with the convenience of\r\nlpSolve though!\r\n\r\n\r\n# Get the results\r\nresults <- lpSolve::lp.transport(cost.mat = adj.matrix, \r\n                                  direction = \"min\", \r\n                                  row.signs = rep(\"<=\", nrow(adj.matrix)), \r\n                                  col.signs = rep(\">=\", ncol(adj.matrix)),\r\n                                  row.rhs = supply, \r\n                                  col.rhs = demand)\r\n\r\n# Verify `objval` is the same as above\r\nresults$objval\r\n\r\n[1] 880\r\n\r\nresults$solution\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]   15   20    0    0\r\n[2,]   20    0   30    0\r\n[3,]   10    0    0   30\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-11-transportation-problem-in-r/transportation-problem-in-r_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-07-13T06:07:50-04:00",
    "input_file": "transportation-problem-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-10-minimum-cost-network-flow-problem-in-r/",
    "title": "Minimum Cost Network Flow Problem (MCNFP) in R",
    "description": "In this post we will walk through how to make least cost maximum flow decisions using linear programming.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-10",
    "categories": [
      "network flows",
      "linear programming",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nTraffic Minimum Cost\r\nNetwork Flow\r\nThe Problem\r\nThe Data\r\nNetwork Visualization\r\n\r\nModel Data\r\nAverage In-Flow\r\nDemand\r\n\r\n\r\nLinear Programming (LP)\r\nLP Structure\r\nArc Constraints\r\nPutting It All Together\r\n\r\nSolve the LP\r\nVisualize Solution\r\nCleaning Up The Visual\r\nFlow vs. Capacity\r\nvs. Time\r\nShortest Path\r\nShortest Path\r\nVisualization\r\n\r\n\r\nReusable Functionality\r\nSourcing\r\n\r\nTraffic Minimum Cost Network\r\nFlow\r\nOne common extension to Maximum Flow problems is to do\r\nit as cheaply as possible. In this article we will extend the maximum\r\nflow example we wrote on in the last\r\npost and include a minimum cost component. The generic problem\r\nformation is below:\r\nObjective \\[\r\nMinimize. \\sum_{(i,j)\\in A}{c_{ij}x_{ij}}   \r\n\\] \\[\r\ns.t.\r\n\\] Node Flow Constraints \\[\r\n\\sum_{j}{x_{ij}} - \\sum_{i}{x_{ji}} = b_i \\: \\forall i \\in N\r\n\\] Arc Flow Constraints \\[\r\nl_{ij} \\le x_{ij} \\le u_{ij} \\: \\forall (i,j) \\in A\r\n\\]\r\nThe Problem\r\nRoad networks are everywhere in our society. In any given\r\nintersection there is a flow of cars, intersections with stop lights,\r\nand connections between each. Every road has a feasible limit it can\r\nsupport. In fact, this is often the cause of most congestion. Our goal\r\nis to minimize the total time required for all cars to travel from node\r\n1 to node 6 in a fictitious road network.\r\nThe Data\r\n\r\n\r\nnodes <- data.frame(id = c(1:6), color = c(\"green\", rep(\"grey\", 4), \"red\"))\r\nedges <- data.frame(from = c(1, 1, 2, 2, 5, 4, 4, 3, 3), \r\n                    to = c(2, 3, 5, 4, 6, 5, 6, 5, 4),\r\n                    lower_bound = 0,\r\n                    upper_bound = c(800, 600, 100, 600, 600, 600, 400, 400, 300),\r\n                    time = c(10, 50, 70, 30, 30, 30, 60, 60, 10), # in minutes\r\n                    flow = 0, #TBD\r\n                    color = \"grey\") %>% dplyr::arrange(from, to)\r\nnodes\r\n\r\n  id color\r\n1  1 green\r\n2  2  grey\r\n3  3  grey\r\n4  4  grey\r\n5  5  grey\r\n6  6   red\r\n\r\nedges\r\n\r\n  from to lower_bound upper_bound time flow color\r\n1    1  2           0         800   10    0  grey\r\n2    1  3           0         600   50    0  grey\r\n3    2  4           0         600   30    0  grey\r\n4    2  5           0         100   70    0  grey\r\n5    3  4           0         300   10    0  grey\r\n6    3  5           0         400   60    0  grey\r\n7    4  5           0         600   30    0  grey\r\n8    4  6           0         400   60    0  grey\r\n9    5  6           0         600   30    0  grey\r\n\r\nNetwork Visualization\r\nWe can see the upper bounds plotted on the edges of this\r\ntransportation network below. The width indicates more capacity for\r\nflow. Examine the trade-off between time and space for travel between\r\narc (1,2).\r\n\r\n\r\ng <- igraph::graph_from_data_frame(d = edges,\r\n                                   directed = TRUE,\r\n                                   vertices = nodes)\r\n\r\n# Capacity plot\r\nplot(g, \r\n     main = \"Vehicle Transportion Network Capacities\", \r\n     edge.label = E(g)$upper_bound, \r\n     edge.width = E(g)$upper_bound/150,\r\n     vertex.color = V(g)$color,\r\n     edge.color = E(g)$color)\r\n\r\n\r\n# Time plot\r\nplot(g, \r\n     main = \"Vehicle Transportion Network Travel Time\", \r\n     edge.label = E(g)$time, \r\n     edge.width = E(g)$time/10,\r\n     vertex.color = V(g)$color,\r\n     edge.color = E(g)$color)\r\n\r\n\r\n\r\nModel Data\r\nAverage In-Flow\r\nFirst we assume the average number of cars that flow through this\r\nnetwork. This is often recovered from past databases that record the\r\nflows through the network.\r\n\r\n\r\nAVERAGE_FLOW <- 900 # per hour\r\nAVERAGE_FLOW\r\n\r\n[1] 900\r\n\r\nDemand\r\nNext we set up the demand that will be flowing through the network.\r\nThis is indicated as the vector b in our model formation\r\nabove. This means the initial node has a supply of 900 vehicles, while\r\nthe final node has a demand of 900 nodes. The objective is to flow as\r\nmany vehicles through the network, in the shortest amount of time.\r\n\r\n\r\ndemand <- c(AVERAGE_FLOW, rep(0, nrow(nodes)-2), -AVERAGE_FLOW)\r\ndemand\r\n\r\n[1]  900    0    0    0    0 -900\r\n\r\nLinear Programming (LP)\r\nLP Structure\r\nThe next step is to set up the optimization. Let’s do that now. There\r\nare 3 key ingredients.\r\nObjective\r\nConstraints\r\nDirections\r\nArc Constraints\r\nWe want to build the constraint matrix, which has 2 parts.\r\nArc Capacity Constraints\r\nNode Flow Constraints\r\nThe Arc Capacity Constraints are the first we\r\naddress.\r\nThe Amat, or A matrix, is the arc matrix which contains\r\nthe upper bounds. Since linear programming relies on the resource\r\nmatrix, we need one row for each arc, our dimensions for variable flow\r\nselection are the number of arcs also. So this means we need an\r\nidentity matrix for the rows of arcs and columns of\r\narcs.\r\n\r\n\r\ncreate_upper_arc_constraints <- function(edges){\r\n  Amat <- matrix(0, nrow = nrow(edges), ncol = nrow(edges))\r\n  Amat_dir <- rep(\"<=\", nrow(Amat))\r\n  Amat_rhs <- c()\r\n\r\n  for(i in 1:ncol(Amat)){\r\n    Amat[i,i] <- 1\r\n    Amat_rhs <- c(Amat_rhs, edges$upper_bound[i])\r\n  }\r\n  \r\n  list(Amat_upper = Amat,\r\n       Amat_upper_dir = Amat_dir,\r\n       Amat_upper_rhs = Amat_rhs)\r\n}\r\n\r\n# This could be higher than zero, but for standard LP this is the default configuration, so not needed.\r\n# create_lower_arc_constraints <- function(edges){\r\n#   Amat <- matrix(0, nrow = nrow(edges), ncol = nrow(edges))\r\n#   Amat_dir <- rep(\">=\", nrow(Amat))\r\n#   Amat_rhs <- c()\r\n# \r\n#   for(i in 1:ncol(Amat)){\r\n#     Amat[i,i] <- 1\r\n#     Amat_rhs <- c(Amat_rhs, edges$lower_bound[i])\r\n#   }\r\n#   \r\n#   list(Amat_lower = Amat,\r\n#        Amat_lower_dir = Amat_dir,\r\n#        Amat_lower_rhs = Amat_rhs)\r\n# }\r\n\r\nupper_results <- create_upper_arc_constraints(edges)\r\n# lower_results <- create_lower_arc_constraints(edges)\r\n# \r\n# Amat <- rbind(upper_results$Amat_upper, lower_results$Amat_lower)\r\n# Amat_dir <- c(upper_results$Amat_upper_dir, lower_results$Amat_lower_dir)\r\n# Amat_rhs <- c(upper_results$Amat_upper_rhs, lower_results$Amat_lower_rhs)\r\n\r\nAmat <- upper_results$Amat_upper\r\nAmat_dir <- upper_results$Amat_upper_dir\r\nAmat_rhs <- upper_results$Amat_upper_rhs\r\n\r\nAmat\r\n\r\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\r\n [1,]    1    0    0    0    0    0    0    0    0\r\n [2,]    0    1    0    0    0    0    0    0    0\r\n [3,]    0    0    1    0    0    0    0    0    0\r\n [4,]    0    0    0    1    0    0    0    0    0\r\n [5,]    0    0    0    0    1    0    0    0    0\r\n [6,]    0    0    0    0    0    1    0    0    0\r\n [7,]    0    0    0    0    0    0    1    0    0\r\n [8,]    0    0    0    0    0    0    0    1    0\r\n [9,]    0    0    0    0    0    0    0    0    1\r\n\r\nAmat_dir\r\n\r\n[1] \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\"\r\n\r\nAmat_rhs\r\n\r\n[1] 800 600 600 100 300 400 600 400 600\r\n\r\nThe Node Flow Constraints are the next to take care\r\nof.\r\nThe Bmat, or B matrix, is a matrix which contains the\r\nnode balance equations codified by flows.\r\nFor each node, we need to match it’s from node and\r\nto node with the appropriate inflow and outflow. If it\r\nmatches an inflow arc, this is increase, so 1\r\nis in the arc column. If it matches an outflow arch, this\r\nis decrease, so -1 is in the arc column. Otherwise\r\n0 remains as the placeholder. The sign here is\r\n== because they must match (i.e., supply = demand). If we\r\nrequire excess at certain points we can set this demand to be higher\r\nthan zero, but we will not do that here.\r\n\r\n\r\ncreate_node_constraints <- function(nodes, edges){\r\n    Bmat <- matrix(0, nrow = nrow(nodes), ncol = nrow(edges))\r\n    Bmat_dir <- rep(\"==\", nrow(Bmat))\r\n    Bmat_rhs <- rep(0, nrow(Bmat))\r\n    \r\n    for(i in 1:nrow(Bmat)){\r\n      node_id <- nodes[i, \"id\"]\r\n      for(j in 1:ncol(Bmat)){\r\n        edge_from <- edges[j,\"from\"]\r\n        edge_to <- edges[j, \"to\"]\r\n        \r\n        if(node_id == edge_from){\r\n          # print(paste(\"Outbound for \", node_id, \"From: \",edge_from, \"to: \", edge_to))\r\n          Bmat[i,j] = 1\r\n        }\r\n        else if(node_id == edge_to){\r\n          # print(paste(\"Inbound for \", node_id, \"From: \",edge_from, \"to: \", edge_to))\r\n          Bmat[i,j] = -1\r\n        }\r\n        else{\r\n          Bmat[i,j] = 0\r\n        }\r\n      }\r\n      Bmat_rhs[i] <- demand[i]\r\n    }\r\n    \r\n    list(Bmat = Bmat,\r\n         Bmat_dir = Bmat_dir,\r\n         Bmat_rhs = Bmat_rhs\r\n    )\r\n}\r\n\r\nresults <- create_node_constraints(nodes, edges)\r\nBmat <- results$Bmat\r\nBmat_dir <- results$Bmat_dir\r\nBmat_rhs <- results$Bmat_rhs\r\n\r\nBmat\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\r\n[1,]    1    1    0    0    0    0    0    0    0\r\n[2,]   -1    0    1    1    0    0    0    0    0\r\n[3,]    0   -1    0    0    1    1    0    0    0\r\n[4,]    0    0   -1    0   -1    0    1    1    0\r\n[5,]    0    0    0   -1    0   -1   -1    0    1\r\n[6,]    0    0    0    0    0    0    0   -1   -1\r\n\r\nBmat_dir\r\n\r\n[1] \"==\" \"==\" \"==\" \"==\" \"==\" \"==\"\r\n\r\nBmat_rhs\r\n\r\n[1]  900    0    0    0    0 -900\r\n\r\nPutting It All Together\r\nNext, the objective is going to be the time. This is the cost we have\r\nto pay for assigning flow to an arc. Let’s take a look at everything all\r\ntogether.\r\n\r\n\r\nf.obj <- edges %>% dplyr::pull(time)\r\nf.cons <- rbind(Amat, Bmat)\r\nf.rhs <- c(Amat_rhs, Bmat_rhs)\r\nf.dir <- c(Amat_dir, Bmat_dir)\r\n\r\nf.obj\r\n\r\n[1] 10 50 30 70 10 60 30 60 30\r\n\r\nf.cons\r\n\r\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\r\n [1,]    1    0    0    0    0    0    0    0    0\r\n [2,]    0    1    0    0    0    0    0    0    0\r\n [3,]    0    0    1    0    0    0    0    0    0\r\n [4,]    0    0    0    1    0    0    0    0    0\r\n [5,]    0    0    0    0    1    0    0    0    0\r\n [6,]    0    0    0    0    0    1    0    0    0\r\n [7,]    0    0    0    0    0    0    1    0    0\r\n [8,]    0    0    0    0    0    0    0    1    0\r\n [9,]    0    0    0    0    0    0    0    0    1\r\n[10,]    1    1    0    0    0    0    0    0    0\r\n[11,]   -1    0    1    1    0    0    0    0    0\r\n[12,]    0   -1    0    0    1    1    0    0    0\r\n[13,]    0    0   -1    0   -1    0    1    1    0\r\n[14,]    0    0    0   -1    0   -1   -1    0    1\r\n[15,]    0    0    0    0    0    0    0   -1   -1\r\n\r\nf.rhs\r\n\r\n [1]  800  600  600  100  300  400  600  400  600  900    0    0    0\r\n[14]    0 -900\r\n\r\nf.dir\r\n\r\n [1] \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"==\" \"==\" \"==\" \"==\"\r\n[14] \"==\" \"==\"\r\n\r\nSolve the LP\r\nNow we unite everything together and drop it into the solver.\r\nRemember, we are trying to minimize cost in our objective, the\r\nconstraints will maximize the flow. So specify min.\r\n\r\n\r\nresults <- lp(direction = \"min\",  \r\n              objective.in = f.obj, \r\n              const.mat = f.cons, \r\n              const.dir = f.dir, \r\n              const.rhs = f.rhs)\r\n\r\nedges$flow <- results$solution\r\nedges\r\n\r\n  from to lower_bound upper_bound time flow color\r\n1    1  2           0         800   10  700  grey\r\n2    1  3           0         600   50  200  grey\r\n3    2  4           0         600   30  600  grey\r\n4    2  5           0         100   70  100  grey\r\n5    3  4           0         300   10  200  grey\r\n6    3  5           0         400   60    0  grey\r\n7    4  5           0         600   30  500  grey\r\n8    4  6           0         400   60  300  grey\r\n9    5  6           0         600   30  600  grey\r\n\r\nVisualize Solution\r\nNow that we have our flow we can do some visualizing and analysis.\r\nThere are two key graphics to examine; the\r\nflow vs. capacity and the flow vs. time.\r\nFirst, the flow vs. capacity will give us insights into\r\nstress on the network. This is because of their implicit advantage they\r\nsupply to the optimizer, maximum flows, so naturally, these get flooded\r\nwith traffic. Second, the flow vs. time will give us\r\ninsights into shortest distance paths (i.e., assuming time is\r\nproportional to distance, which is not always the case). This is because\r\npaths with shorter times will enable more to flow through in any given\r\ntime delta. Between these two visuals, a good assessment of the model\r\noutput is feasible.\r\n\r\n\r\n# Set the flow\r\nedges$flow <- results$solution\r\n\r\n# If the arc is flowing oil, change to black\r\nedges$color[which(edges$flow > 0)] <- \"black\"\r\n\r\n# If the arc is at capacity change it to red\r\nedges$color[which(edges$flow == edges$upper_bound)] <- \"red\"\r\n\r\ng <- igraph::graph_from_data_frame(d = edges,\r\n                                   directed = TRUE,\r\n                                   vertices = nodes)\r\n\r\n# Plot the flow and capacity\r\nplot(g,\r\n     edge.label = paste(\"(\",E(g)$flow, \r\n                        \",\",\r\n                         E(g)$upper_bound, \r\n                         \")\", \r\n                        sep=\"\"), \r\n     main = \"Traffic Flow | (flow, capacity)\", \r\n     vertex.color = V(g)$color,\r\n     vertex.size = 25,\r\n     vertex.label.cex = 1.6,\r\n     edge.color = E(g)$color,\r\n     edge.width = E(g)$flow/200)\r\n\r\n\r\n# Plot the time\r\nplot(g,\r\n     edge.label = paste(\"(\",E(g)$flow, \r\n                        \",\",\r\n                         E(g)$time, \r\n                         \")\", \r\n                        sep=\"\"), \r\n     main = \"Traffic Flow | (flow, time)\", \r\n     vertex.color = V(g)$color,\r\n     vertex.size = 25,\r\n     vertex.label.cex = 1.6,\r\n     edge.color = E(g)$color,\r\n     edge.width = E(g)$flow/200)\r\n\r\n\r\n\r\nCleaning Up The Visual\r\n\r\n\r\nnodes\r\n\r\n  id color\r\n1  1 green\r\n2  2  grey\r\n3  3  grey\r\n4  4  grey\r\n5  5  grey\r\n6  6   red\r\n\r\nvisNet_nodes <- nodes %>% \r\n                dplyr::mutate(label = paste(\"Location \", id),\r\n                              font.size =10,\r\n                              type = \"black\",\r\n                              shape = \"circle\",\r\n                              font = 12)\r\n\r\nvisNet_edges <- edges %>% \r\n                dplyr::mutate(label = paste(\"(\",flow,\",\",upper_bound,\")\",sep=\"\"),\r\n                              length = 250,\r\n                              width = flow/100,\r\n                              arrows = \"to\") \r\n  \r\nvisNetwork(visNet_nodes, \r\n           visNet_edges,\r\n           width = \"100%\",\r\n           main = \"Least Time Maximum Vehicle Flow | (flow, capacity)\")\r\n\r\n\r\n\r\nFlow vs. Capacity vs. Time\r\nTwo of the most popular roadways are\r\n2 -> 4 and 5 -> 6. These supply the\r\nleast amount of time and the most amount of capacity. Without a doubt,\r\nthe model has exploited these as much as possible. The only trick is,\r\njust how the flow gets there. We see up front that 1 ships\r\noff a 200 to 700 flow split from the 900\r\nsupply with the heavier allocation toward 2. Why?\r\n2 is connected to a popular\r\nroadway, meaning much more potential to flow (and\r\nquickly).\r\nIn order to get to 4, going through 3\r\nwill cost much more time (60 oppose to 40).\r\n3 has two outlets, but one is one of the worst\r\nroutes on the network due to it’s 60 minute trek, so it doesn’t even get\r\nany flow allocated.\r\nShortest Path\r\nThe shortest path shows one very interesting insight to this model;\r\nsending a maximum flow through the network is not all about time. The\r\nshortest path (least time) is the sequence\r\n1 -> 2 -> 4 -> 6. However, from the above, we see\r\nthat this isn’t the most stressed path. Why? We aren’t only\r\ninterested in short times for the vehicles flowing through the network.\r\nWe are also interested in getting them all through it! We assumed there\r\nwas a 900 average vehicle flow, and having a macro-level view of this\r\nsystem, sending them all down the shortest path would not solve it (that\r\nis, we would not send as much as possible, only as cheaply as possible;\r\nwe could have pushed more). In order to get the most cars sent through\r\nthe network, in the shortest amount of time we also must take advantage\r\nof the popular roadways that the model is straining (or\r\nadd incentive to the not so popular roadways with\r\ngreater capacity or shorter commute times).\r\n\r\n\r\nshortest.paths <- igraph::shortest_paths(graph = g, from = 1, to = 6)\r\ns_path <- shortest.paths$vpath[[1]]\r\ns_path\r\n\r\n+ 4/6 vertices, named, from ac6f2e8:\r\n[1] 1 2 4 6\r\n\r\nshortest_commute_time <- E(g, path = s_path)$time %>% sum\r\nshortest_commute_time\r\n\r\n[1] 100\r\n\r\nShortest Path Visualization\r\n\r\n\r\nE(g)$color <- \"black\"\r\nE(g, path = s_path)$color <- \"blue\"\r\n\r\nplot(g,\r\n     edge.label = paste(\"(\",E(g)$flow, \r\n                        \",\",\r\n                         E(g)$time, \r\n                         \")\", \r\n                        sep=\"\"), \r\n     main = \"Shortest Path | (flow, capacity)\", \r\n     vertex.color = V(g)$color,\r\n     vertex.size = 25,\r\n     vertex.label.cex = 1.6,\r\n     edge.color = E(g)$color,\r\n     edge.width = E(g)$flow/200)\r\n\r\n\r\n\r\nReusable Functionality\r\nWe can also build a function to reuse for next time.\r\n\r\n\r\n#\r\n# @lp.mincost.maxflow: data.frame, integer, integer, integer or list -> list\r\n#   function outputs an edge list with flows.\r\n#\r\n# @edges: data.frame, should have from, to, and capacity columns for each edge in the network. from and to columns should contain unique node ids as integers from 0 to N.\r\n# @source.id: integer, unique node id\r\n# @dest.id: integer, unique node id\r\n# @flow.demand: integer or list, if integer create a demand signal to push that much from source.id to dest.id, if a list expectation is that it sums to 0 and will contain each nodes supply (if positive) and demand (if negative).\r\n\r\n\r\nlp.mincost.maxflow <- function(edges, source.id, dest.id, flow.demand){\r\n  if(!(\"from\" %in% names(edges)) || \r\n     !(\"to\" %in% names(edges)) ||\r\n     !(\"upper_bound\" %in% names(edges)) ||\r\n     !(\"lower_bound\" %in% names(edges))){\r\n    print(\"Need from, to, and capacity columns.\")\r\n  }\r\n  else{\r\n    \r\n    # Infer the nodes\r\n    nodes <- data.frame(id = sort(unique(c(unique(edges$from), unique(edges$to)))))\r\n    \r\n    # Infer the demand to flow through the network\r\n    if(length(flow.demand)>1){\r\n      if(sum(flow.demand) == 0){\r\n        demand <- flow.demand\r\n      }else{\r\n        print(\"Flow demand doesn't add up to 0.\")\r\n      }\r\n    }\r\n    else{\r\n      demand <- c(flow.demand, rep(0, nrow(nodes)-2), -flow.demand)\r\n    }\r\n\r\n    # Get arc capacity constraints\r\n    create_arc_capacity_constraints <- function(edges){\r\n        \r\n      # For upper\r\n      Amat <- matrix(0, nrow = nrow(edges), ncol = nrow(edges))\r\n      Amat_dir <- rep(\"<=\", nrow(Amat))\r\n      Amat_rhs <- c()\r\n    \r\n      for(i in 1:ncol(Amat)){\r\n        Amat[i,i] <- 1\r\n        Amat_rhs <- c(Amat_rhs, edges$upper_bound[i])\r\n      }\r\n      \r\n      # For lower\r\n      Bmat <- matrix(0, nrow = nrow(edges), ncol = nrow(edges))\r\n      Bmat_dir <- rep(\">=\", nrow(Bmat))\r\n      Bmat_rhs <- c()\r\n    \r\n      for(i in 1:ncol(Bmat)){\r\n        Bmat[i,i] <- 1\r\n        Bmat_rhs <- c(Bmat_rhs, edges$lower_bound[i])\r\n      }\r\n      \r\n      list(Amat = rbind(Amat, Bmat),\r\n           Amat_dir = c(Amat_dir, Bmat_dir),\r\n           Amat_rhs = c(Amat_rhs, Bmat_rhs))\r\n    }\r\n    \r\n   results <- create_arc_capacity_constraints(edges)\r\n    Amat <- results$Amat\r\n    Amat_dir <- results$Amat_dir\r\n    Amat_rhs <- results$Amat_rhs\r\n    \r\n    # Create node flow constraints (in = out)\r\n    create_node_constraints <- function(nodes, edges){\r\n        Bmat <- matrix(0, nrow = nrow(nodes), ncol = nrow(edges))\r\n        Bmat_dir <- rep(\"==\", nrow(Bmat))\r\n        Bmat_rhs <- rep(0, nrow(Bmat))\r\n        \r\n        for(i in 1:nrow(Bmat)){\r\n          node_id <- nodes[i, \"id\"]\r\n          for(j in 1:ncol(Bmat)){\r\n            edge_from <- edges[j,\"from\"]\r\n            edge_to <- edges[j, \"to\"]\r\n            \r\n            if(node_id == edge_from){\r\n              # print(paste(\"Outbound for \", node_id, \"From: \",edge_from, \"to: \", edge_to))\r\n              Bmat[i,j] = 1\r\n            }\r\n            else if(node_id == edge_to){\r\n              # print(paste(\"Inbound for \", node_id, \"From: \",edge_from, \"to: \", edge_to))\r\n              Bmat[i,j] = -1\r\n            }\r\n            else{\r\n              Bmat[i,j] = 0\r\n            }\r\n          }\r\n          Bmat_rhs[i] <- demand[i]\r\n        }\r\n        \r\n        list(Bmat = Bmat,\r\n             Bmat_dir = Bmat_dir,\r\n             Bmat_rhs = Bmat_rhs\r\n        )\r\n    }\r\n    \r\n    results <- create_node_constraints(nodes, edges)\r\n    Bmat <- results$Bmat\r\n    Bmat_dir <- results$Bmat_dir\r\n    Bmat_rhs <- results$Bmat_rhs\r\n    \r\n    # Bring together\r\n    f.obj <- edges$cost\r\n    f.cons <- rbind(Amat, Bmat)\r\n    f.rhs <- c(Amat_rhs, Bmat_rhs)\r\n    f.dir <- c(Amat_dir, Bmat_dir)\r\n    results <- lp(direction = \"min\",  \r\n                  objective.in = f.obj, \r\n                  const.mat = f.cons, \r\n                  const.dir = f.dir, \r\n                  const.rhs = f.rhs)\r\n    \r\n    edges$flow <- results$solution\r\n  }\r\n  \r\n  list(edges = edges, results = results)\r\n}\r\n\r\nedges <- data.frame(from = c(1, 1, 2, 2, 5, 4, 4, 3, 3), \r\n                    to = c(2, 3, 5, 4, 6, 5, 6, 5, 4),\r\n                    lower_bound = 0,\r\n                    upper_bound = c(800, 600, 100, 600, 600, 600, 400, 400, 300),\r\n                    cost = c(10, 50, 70, 30, 30, 30, 60, 60, 10))\r\n\r\nlp.mincost.maxflow(edges = edges, source.id = 1, dest.id = 6, flow.demand = 900)\r\n\r\n$edges\r\n  from to lower_bound upper_bound cost flow\r\n1    1  2           0         800   10  700\r\n2    1  3           0         600   50  200\r\n3    2  5           0         100   70  100\r\n4    2  4           0         600   30  600\r\n5    5  6           0         600   30  600\r\n6    4  5           0         600   30  500\r\n7    4  6           0         400   60  300\r\n8    3  5           0         400   60    0\r\n9    3  4           0         300   10  200\r\n\r\n$results\r\nSuccess: the objective function is 95000 \r\n\r\nSourcing\r\nYou can also source the file to make things easier for next time. The\r\ncode can be found here.\r\n\r\n\r\nsource(\"lp.mincost.maxflow.r\")\r\n\r\nedges <- data.frame(from = c(1, 1, 2, 2, 5, 4, 4, 3, 3), \r\n                    to = c(2, 3, 5, 4, 6, 5, 6, 5, 4),\r\n                    lower_bound = 0,\r\n                    upper_bound = c(800, 600, 100, 600, 600, 600, 400, 400, 300),\r\n                    cost = c(10, 50, 70, 30, 30, 30, 60, 60, 10))\r\n\r\nlp.mincost.maxflow(edges = edges, source.id = 1, dest.id = 6, flow.demand = 900)\r\n\r\n$edges\r\n  from to lower_bound upper_bound cost flow\r\n1    1  2           0         800   10  700\r\n2    1  3           0         600   50  200\r\n3    2  5           0         100   70  100\r\n4    2  4           0         600   30  600\r\n5    5  6           0         600   30  600\r\n6    4  5           0         600   30  500\r\n7    4  6           0         400   60  300\r\n8    3  5           0         400   60    0\r\n9    3  4           0         300   10  200\r\n\r\n$results\r\nSuccess: the objective function is 95000 \r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-10-minimum-cost-network-flow-problem-in-r/minimum-cost-network-flow-problem-in-r_files/figure-html5/visual_1-1.png",
    "last_modified": "2022-07-15T04:32:36-04:00",
    "input_file": "minimum-cost-network-flow-problem-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-09-maximum-network-flows-in-r/",
    "title": "Maximum Network Flows in R",
    "description": "In this post we will walk through how to make a maximum flow decision using network flows and linear programming.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-09",
    "categories": [
      "network flows",
      "linear programming",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nOil Flow Maximization\r\nProblem Formation\r\nNetwork Structure\r\nNetwork Visual\r\nBig M\r\n\r\nLinear Programming (LP)\r\nLP Structure\r\nConstraints\r\nObjective\r\n\r\nSolve the LP\r\nVisualize Maximum Flow\r\nSolution\r\nCleaning Up The Visual\r\n\r\n\r\nReusable Functionality\r\nSourcing\r\nReferences\r\n\r\nOil Flow Maximization\r\nOne classic problem in Network Flows and\r\nOptimization is called the Max-Flow Problem.\r\nThis takes any two nodes in a network, s and t\r\nand attempts to send as much of a resource (or multiple) from\r\ns to t. This is called a flow,\r\nand the flow which maximizes the total bandwidth of the network is\r\ncalled the maximum flow.\r\nFirst, the problem starts with an objective: to\r\nmaximize flow. These are denoted as,\r\n\\[x_{ij} = flow \\: on \\: node_i \\: to \\:\r\nnode_j \\:(or \\: on \\: arc \\: (i,j))\\]\r\nSecond, the problem has a set of constraints, these are called the\r\narc capacities. These are denoted as,\r\n\\[u_{ij} = maximum \\:amount \\:of \\:\r\nfeasible \\: flow \\: on \\: node_i \\: to \\: node_j \\:(or \\: on \\: arc \\:\r\n(i,j))\\] Last, the network graph is supplied as a set of\r\nconnections under the traditional network structure:\r\n\\[ G = (N,E) \\]\r\nProblem Formation\r\nFor our problem, the feasible flow is going to be in units of\r\nmillions of barrels of oil per hour that will pass through an arc of\r\npipeline.\r\nNetwork Structure\r\nThe source for our network is indicated in green and sink in red.\r\n\r\n\r\nnodes <- data.frame(id = c(0:4), color = c(\"green\", rep(\"grey\", 3), \"red\"))\r\nedges <- data.frame(from = c(0,0,1,1,3,2), to = c(1,2,2,3,4,4), capacity = c(2,3,3,4,1,2), color = \"grey\")\r\n\r\nedges\r\n\r\n  from to capacity color\r\n1    0  1        2  grey\r\n2    0  2        3  grey\r\n3    1  2        3  grey\r\n4    1  3        4  grey\r\n5    3  4        1  grey\r\n6    2  4        2  grey\r\n\r\nNetwork Visual\r\n\r\n\r\ng <- igraph::graph_from_data_frame(d = edges,\r\n                                   directed = TRUE,\r\n                                   vertices = nodes)\r\nplot(g, edge.label = E(g)$capacity, \r\n     main = \"Oil Pipeline Capacities\", \r\n     vertex.color = V(g)$color,\r\n     edge.color = E(g)$color)\r\n\r\n\r\n\r\nBig M\r\nIn order to model this problem, we need to put a very large capacity\r\nfrom the source node to the sink node, these\r\nare s and t mentioned above. We have given\r\nthem node names 0 and 4 respectively in the\r\ndataframe for nodes.\r\n\r\n\r\nBIG_M <- 1000000\r\n\r\nedges <- rbind(edges, data.frame(from = c(4),\r\n                                 to = c(0), \r\n                                 capacity = c(BIG_M), \r\n                                 color = \"purple\"))\r\n\r\n# Never hurts to add an id\r\nedges$id <- 0:(nrow(edges)-1)\r\n\r\ng <- igraph::graph_from_data_frame(d = edges,\r\n                                   directed = TRUE,\r\n                                   vertices = nodes)\r\nplot(g,\r\n     edge.label = E(g)$capacity, \r\n     main = \"Oil Pipeline Capacities\", \r\n     vertex.color = V(g)$color,\r\n     edge.color = E(g)$color)\r\n\r\n\r\n\r\nLinear Programming (LP)\r\nLP Structure\r\nThe next step is to set up the optimization. Let’s do that now. There\r\nare 3 key ingredients.\r\nObjective\r\nConstraints\r\nDirections\r\nConstraints\r\nWe want to build the constraint matrix, which has 2 parts.\r\nArc Capacity Constraints\r\nNode Flow Constraints\r\nThe Arc Capacity Constraints are the first we\r\naddress.\r\nThe Amat, or A matrix, is the arc matrix which contains\r\nthe upper bounds. Since linear programming relies on the resource\r\nmatrix, we need one row for each arc, our dimensions for variable flow\r\nselection are the number of arcs also. So this means we need an\r\nidentity matrix for the rows of arcs and columns of\r\narcs.\r\n\r\n\r\nAmat <- matrix(0, nrow = nrow(edges), ncol = nrow(edges))\r\nAmat_dir <- rep(\"<=\", nrow(Amat))\r\nAmat_rhs <- c()\r\n\r\nfor(i in 1:ncol(Amat)){\r\n  Amat[i,i] <- 1\r\n  Amat_rhs <- c(Amat_rhs, edges$capacity[i])\r\n}\r\n\r\nAmat\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\r\n[1,]    1    0    0    0    0    0    0\r\n[2,]    0    1    0    0    0    0    0\r\n[3,]    0    0    1    0    0    0    0\r\n[4,]    0    0    0    1    0    0    0\r\n[5,]    0    0    0    0    1    0    0\r\n[6,]    0    0    0    0    0    1    0\r\n[7,]    0    0    0    0    0    0    1\r\n\r\nAmat_rhs\r\n\r\n[1] 2e+00 3e+00 3e+00 4e+00 1e+00 2e+00 1e+06\r\n\r\nAmat_dir\r\n\r\n[1] \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\"\r\n\r\nThe Node Flow Constraints are the next to take care\r\nof.\r\nFor each node, we need to match it’s from node and\r\nto node with the appropriate inflow and outflow. If it\r\nmatches an inflow arc, this is increase, so 1\r\nis in the arc column. If it matches an outflow arch, this\r\nis decrease, so -1 is in the arc column. Otherwise\r\n0 remains as the placeholder. The sign here is\r\n== because they must match (i.e., supply = demand). If we\r\nrequire excess at certain points we can set this demand to be higher\r\nthan zero, but we will not do that here.\r\n\r\n\r\nBmat <- matrix(0, nrow = nrow(nodes), ncol = nrow(edges))\r\nBmat_dir <- rep(\"==\", nrow(Bmat))\r\nBmat_rhs <- rep(0, nrow(Bmat))\r\n\r\nfor(i in 1:nrow(Bmat)){\r\n  node_id <- nodes[i, \"id\"]\r\n  for(j in 1:ncol(Bmat)){\r\n    edge_from <- edges[j,\"from\"]\r\n    edge_to <- edges[j, \"to\"]\r\n    edge_id <- edges[j, \"id\"]\r\n    \r\n    if(node_id == edge_from){\r\n      Bmat[i,j] = -1\r\n    }\r\n    else if(node_id == edge_to){\r\n      Bmat[i,j] = 1\r\n    }\r\n    else{\r\n      Bmat[i,j] = 0\r\n    }\r\n  }\r\n}\r\n\r\nBmat\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\r\n[1,]   -1   -1    0    0    0    0    1\r\n[2,]    1    0   -1   -1    0    0    0\r\n[3,]    0    1    1    0    0   -1    0\r\n[4,]    0    0    0    1   -1    0    0\r\n[5,]    0    0    0    0    1    1   -1\r\n\r\nBmat_dir\r\n\r\n[1] \"==\" \"==\" \"==\" \"==\" \"==\"\r\n\r\nBmat_rhs\r\n\r\n[1] 0 0 0 0 0\r\n\r\nObjective\r\nNext, the objective is going to be 0 for all values,\r\nexcept our final flow. This we want to maximize.\r\n\r\n\r\nf.obj <- c(rep(0, nrow(edges)-1), 1)\r\nf.obj\r\n\r\n[1] 0 0 0 0 0 0 1\r\n\r\nSolve the LP\r\nNow we unite everything together and drop it into the solver.\r\nRemember, we are trying to maximize flow. So specify\r\nmax.\r\n\r\n\r\nf.cons <- rbind(Amat, Bmat)\r\nf.rhs <- c(Amat_rhs, Bmat_rhs)\r\nf.dir <- c(Amat_dir, Bmat_dir)\r\n\r\nresults <- lp(direction = \"max\",  \r\n              objective.in = f.obj, \r\n              const.mat = f.cons, \r\n              const.dir = f.dir, \r\n              const.rhs = f.rhs)\r\nresults$solution\r\n\r\n[1] 1 2 0 1 1 2 3\r\n\r\nVisualize Maximum Flow\r\nSolution\r\nSince the results$solution contain the maximum flow we\r\ncan push through the pipes as a system, we can add this into our flow\r\ncomponent of the edges dataframe.\r\nThe visual indicates that 2/3 of the capacity was\r\nshipped to node 2, and 1/2 to\r\nnode 1. After this the next best transfer was from\r\nnode 2 to node 4 maxed out at\r\n2/2. This is indicated in red. The arcs that have flow are\r\nindicated in black, and no flow is indicated by grey. The maximum flow\r\narc is just an artificial arc that indicates the maximum\r\nflow, this is indicated in purple.\r\nLastly from the node 1 to node 3\r\n1/4 of the capacity was sent. Then from node 3\r\nto node 4 was maxed out at 1/1.\r\nPosterior analysis to this model output tells us that sending\r\n2 million tons of oil from the source to\r\ndestination 2 and 1 million tons of oil from\r\nthe source to destination 1 will push as much\r\nflow to destination 4 as we possibly can.\r\n\r\n\r\n# Set the flow\r\nedges$flow <- results$solution\r\n\r\n# If the arc is flowing oil, change to black\r\nedges$color[which(edges$flow > 0)] <- \"black\"\r\n\r\n# If the arc is at capacity change it to red\r\nedges$color[which(edges$flow == edges$capacity)] <- \"red\"\r\n\r\n# Last flow is purple\r\nedges$color[nrow(edges)] <- \"purple\"\r\n\r\n\r\ng <- igraph::graph_from_data_frame(d = edges,\r\n                                   directed = TRUE,\r\n                                   vertices = nodes)\r\n\r\n# Make it look a little more appealing\r\nL = cbind(1:5, 1:5)\r\nCURVE = c(0,0.15, 0.3, 0.45, 0, -0.15, -0.3, 0, 0.15, 0) * 5\r\n\r\n# Plot\r\nplot(g,\r\n     layout = L,\r\n     edge.curved = CURVE,\r\n     edge.label = paste(\"(\",E(g)$flow, \r\n                        \",\", \r\n                        E(g)$capacity, \")\", \r\n                        sep=\"\"), \r\n     main = \"Oil Pipeline Flow | (flow, capacity)\", \r\n     vertex.color = V(g)$color,\r\n     vertex.size = 25,\r\n     vertex.label.cex = 1.6,\r\n     edge.color = E(g)$color,\r\n     edge.width = E(g)$flow*2)\r\n\r\n\r\n\r\nCleaning Up The Visual\r\n\r\n\r\nvisNet_nodes <- nodes %>% \r\n                dplyr::mutate(label = paste(\"Location \", id),\r\n                              font.size =10,\r\n                              type = \"black\",\r\n                              shape = \"circle\",\r\n                              font = 12)\r\n\r\nvisNet_edges <- edges %>% \r\n                dplyr::mutate(label = paste(\"(\",flow,\",\",capacity,\")\",sep=\"\"),\r\n                              length = 250,\r\n                              width = flow*2,\r\n                              arrows = \"to\") \r\n  \r\nvisNetwork(visNet_nodes, \r\n           visNet_edges,\r\n           width = \"100%\",\r\n           main = \"Maximum Oil Flow | (flow, capacity)\")\r\n\r\n\r\n\r\nReusable Functionality\r\nIf we would like, we can take the code and example above and make a\r\nreusable functional API so we don’t need to do this time and again. The\r\ncode is as follows.\r\n\r\n\r\n#\r\n# @lp.maxflow: data.frame, integer, integer -> list\r\n#   function outputs an edge list with max flows.\r\n#\r\n# @edges: data.frame, should have from, to, and capacity columns for each edge in the network. from and to columns should contain unique node ids as integers from 0 to N.\r\n# @source.id: integer, unique node id\r\n# @dest.id: integer, unique node id\r\n#\r\nlp.maxflow <- function(edges, source.id, dest.id){\r\n  if(!(\"from\" %in% names(edges)) || !(\"to\" %in% names(edges)) || !(\"capacity\" %in% names(edges))){\r\n    print(\"Need from, to, and capacity columns.\")\r\n  }\r\n  else{\r\n    \r\n    # Infer the nodes\r\n    nodes <- data.frame(id = sort(unique(c(unique(edges$from), unique(edges$to)))))\r\n    \r\n    # Connect source.id to dest.id\r\n    BIG_M = max(edges$capacity)*10\r\n    edges <- rbind(edges, data.frame(from = c(dest.id),\r\n                                 to = c(source.id), \r\n                                 capacity = c(BIG_M)))\r\n    \r\n    \r\n    \r\n    # Build up edge constraints \r\n    Amat <- matrix(0, nrow = nrow(edges), ncol = nrow(edges))\r\n    Amat_dir <- rep(\"<=\", nrow(Amat))\r\n    Amat_rhs <- c()\r\n    \r\n    for(i in 1:ncol(Amat)){\r\n      Amat[i,i] <- 1\r\n      Amat_rhs <- c(Amat_rhs, edges$capacity[i])\r\n    }\r\n    \r\n    # Build up node constraints\r\n    Bmat <- matrix(0, nrow = nrow(nodes), ncol = nrow(edges))\r\n    Bmat_dir <- rep(\"==\", nrow(Bmat))\r\n    Bmat_rhs <- rep(0, nrow(Bmat))\r\n    \r\n    for(i in 1:nrow(Bmat)){\r\n      node_id <- nodes[i, \"id\"]\r\n      for(j in 1:ncol(Bmat)){\r\n        edge_from <- edges[j,\"from\"]\r\n        edge_to <- edges[j, \"to\"]\r\n        edge_id <- edges[j, \"id\"]\r\n        \r\n        if(node_id == edge_from){\r\n          Bmat[i,j] = -1\r\n        }\r\n        else if(node_id == edge_to){\r\n          Bmat[i,j] = 1\r\n        }\r\n        else{\r\n          Bmat[i,j] = 0\r\n        }\r\n      }\r\n    }\r\n\r\n    # Join all model parameters\r\n    f.obj <- c(rep(0, nrow(edges)-1), 1)\r\n    f.cons <- rbind(Amat, Bmat)\r\n    f.rhs <- c(Amat_rhs, Bmat_rhs)\r\n    f.dir <- c(Amat_dir, Bmat_dir)\r\n    \r\n    results <- lp(direction = \"max\",  \r\n                  objective.in = f.obj, \r\n                  const.mat = f.cons, \r\n                  const.dir = f.dir, \r\n                  const.rhs = f.rhs)\r\n    edges$flow <- results$solution\r\n  }\r\n  \r\n  list(edges=edges, flow = results$solution, maxflow = results$objval, results = results)\r\n}\r\n\r\nedges <- data.frame(from = c(0,0,1,1,3,2), to = c(1,2,2,3,4,4), capacity = c(2,3,3,4,1,2))\r\nlp.maxflow(edges, source.id = 0, dest.id = 4)\r\n\r\n$edges\r\n  from to capacity flow\r\n1    0  1        2    1\r\n2    0  2        3    2\r\n3    1  2        3    0\r\n4    1  3        4    1\r\n5    3  4        1    1\r\n6    2  4        2    2\r\n7    4  0       40    3\r\n\r\n$flow\r\n[1] 1 2 0 1 1 2 3\r\n\r\n$maxflow\r\n[1] 3\r\n\r\n$results\r\nSuccess: the objective function is 3 \r\n\r\nSourcing\r\nWe can also save that function and source it for later use. The code\r\nis available on github here.\r\n\r\n\r\nsource(\"lp.maxflow.r\")\r\nedges <- data.frame(from = c(0,0,1,1,3,2), to = c(1,2,2,3,4,4), capacity = c(2,3,3,4,1,2))\r\nlp.maxflow(edges, source.id = 0, dest.id = 4)\r\n\r\n$edges\r\n  from to capacity flow\r\n1    0  1        2    1\r\n2    0  2        3    2\r\n3    1  2        3    0\r\n4    1  3        4    1\r\n5    3  4        1    1\r\n6    2  4        2    2\r\n7    4  0       40    3\r\n\r\n$flow\r\n[1] 1 2 0 1 1 2 3\r\n\r\n$maxflow\r\n[1] 3\r\n\r\n$results\r\nSuccess: the objective function is 3 \r\n\r\nReferences\r\nExample taken from the following sources:\r\nWinston., Wayne. Operations Research, Applications and\r\nAlgorithms 4th Edition.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-09-maximum-network-flows-in-r/maximum-network-flows-in-r_files/figure-html5/initial_graph-1.png",
    "last_modified": "2022-07-15T04:01:51-04:00",
    "input_file": "maximum-network-flows-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-09-shortest-path-optimization-in-r/",
    "title": "Shortest Path Optimization in R",
    "description": "In this post we will walk through how to make least cost decisions using network flows shortest path algorithm.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-09",
    "categories": [
      "network flows",
      "linear programming",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nVehicle\r\nReplacement Using Shortest Path\r\nProblem Setup\r\nProblem Data\r\nNew Car Cost\r\nMaintenance Costs\r\nTrade-in Revenues\r\nCost Matrix\r\n\r\nSolving the Problem\r\nVisualizing the network\r\nShortest Path Solution\r\nShortest Path Analysis\r\n\r\n\r\nReferences\r\n\r\nVehicle Replacement\r\nUsing Shortest Path\r\nA common problem we often face in the world is to find the shortest\r\npath between two points. Whether it is on the road, or how to obtain an\r\nobject in in our intellectual trajectory, we are always seeking to\r\noptimize.\r\nIn network flows one common problem is to find the\r\ns-t shortest path. The problem formulation is as\r\nfollows.\r\nGiven some net of nodes, and two particular nodes s and\r\nt that are not the same, find the shortest distance through\r\nthe edges connecting them. The mathematical formulation is below. \\[ N \\in \\{n_1, n_2, .., n_D\\} \\] \\[ E = \\{N \\times N\\} = \\{e_{ij} \\in \\Re^+ |\r\n\\:\\:\\forall i,j \\in N \\} \\] \\[ s,t \\in\r\nN : s\\neq t\\]\r\nProblem Setup\r\nWe all have to drive cars at some point in our adult lives (well,\r\nmost of us). With this comes the question of investment\r\ndecisions if I am allowed to call it that. Which car should we buy?\r\nHow long should we keep it? Is it more prudent to keep paying\r\nmaintenance, repairs, and damages, or just get a new ride?\r\nWe would like to know what is the best decision to make over the next\r\n5 years for our vehicle needs. So we take this problem and model it as\r\nan optimization problem using the famous\r\nshortest paths algorithm.\r\nSo with our problem the decision space is pretty straight\r\nforward:\r\nEach year we can choose to keep our vehicle, or\r\nTrade it in\r\nHowever, every year we choose to keep our vehicle, we must pay\r\nmaintenance costs for it. So every year we keep it, there is a\r\ncumulative maintenance cost. Once we trade-in we offset the cost of the\r\nnew car with the trade in value, and pay much less maintenance on the\r\nnew ride. Let’s take a look at the problem data.\r\nProblem Data\r\nWe know the cost every year of a new vehicle is assumed as\r\n$12,000 for simplicity. Further, we have some records of\r\nwhat maintenance costs and trade-in values will be.\r\nNew Car Cost\r\nThe new car cost is assumed constant every year. An interesting\r\nhomework assignment would be to make this stochastic and\r\nchange over time. This is much more suitable to the real world, but for\r\nthis example will remain constant.\r\n\r\n\r\nNEW_CAR_COST <- 12000\r\nNEW_CAR_COST\r\n\r\n[1] 12000\r\n\r\nMaintenance Costs\r\nThe maintenance costs are for 4 years. Each year you keep the car,\r\nyou will pay more on maintenance.\r\n\r\n\r\nM <- data.frame(Year = c(0:4), Maintenance_cost = c(2000, 4000, 5000, 9000, 12000))\r\nM_vec <- M %>% dplyr::pull(Maintenance_cost)\r\nM\r\n\r\n  Year Maintenance_cost\r\n1    0             2000\r\n2    1             4000\r\n3    2             5000\r\n4    3             9000\r\n5    4            12000\r\n\r\nTrade-in Revenues\r\nThe trade-in price is similar to maintenance. Each year you keep the\r\nvehicle, it will depreciate. So to account for this we have a decreasing\r\ntrade-in value.\r\n\r\n\r\nT <- data.frame(Year = c(1:5), Trade_in_price = c(7000, 6000, 2000, 1000, 0))\r\nT_vec <- T %>% dplyr::pull(Trade_in_price)\r\nT\r\n\r\n  Year Trade_in_price\r\n1    1           7000\r\n2    2           6000\r\n3    3           2000\r\n4    4           1000\r\n5    5              0\r\n\r\nCost Matrix\r\nSince we know the costs will be cumulative, so we know what each\r\nyears will be. The cost matrix will be for the number of years the car\r\nis kept to accumulate costs from maintenance. In mathematical language,\r\nthis is represented below:\r\n\\[ c_{ij} = \\sum_{t=1}^{j-1}{M_{t-1}}\\:\\:\r\nif\\: j > i \\:\\: otherwise \\: \\infty \\]\r\nWhere M is the maintenance matrix defined above.\r\nWe also know our objective is to minimize the total cost, which\r\nequates to maintanence cost +\r\ncost to purchase a new car -\r\ntrade in value.\r\n\r\n\r\n# Nodes dataframe\r\nnodes = data.frame(Year = c(sprintf(\"Year %s\", seq(1:6))),\r\n                   Color = c(\"green\", \"gold\", \"gold\", \"gold\", \"gold\", \"red\"))\r\nn = nrow(nodes)\r\n\r\n# Edges list\r\nedges = list(from=c(), to=c(), cost=c(), color=c())\r\n\r\n# Cost matrix\r\nC <- matrix(0, n, n)\r\nBIG_M <- 1000000\r\nfor(i in 1:n){\r\n  for (j in 1:n){\r\n    if(j > i){\r\n      \r\n      # Cost of maintenance\r\n      maintenance_cost <- M_vec[1:(j-i)]\r\n      maintenance_cost_total <- sum(maintenance_cost)\r\n\r\n      # Cost of new car\r\n      new_car_cost = NEW_CAR_COST\r\n      \r\n      # Trade-in value\r\n      trade_in_revenue <- T_vec[j-i]\r\n\r\n      # Total cost for decision to buy car on year i and sell it on year j\r\n      total_cost_for_decision_i_to_j <- new_car_cost + maintenance_cost_total - trade_in_revenue\r\n\r\n      # Save the value into cost matrix\r\n      C[i,j] <- total_cost_for_decision_i_to_j\r\n      edges$from <- append(edges$from, paste(\"Year\", i))\r\n      edges$to <- append(edges$to, paste(\"Year\", j))\r\n      edges$cost <- append(edges$cost, total_cost_for_decision_i_to_j)\r\n      edges$color <- append(edges$color, \"grey\")\r\n    }\r\n    else{\r\n      \r\n      # Big M otherwise to make edge infeasible\r\n      C[i,j] <- BIG_M\r\n    }\r\n  }\r\n}\r\n\r\n# Edges dataframe\r\nedges <- edges %>% as.data.frame\r\n\r\nnodes\r\n\r\n    Year Color\r\n1 Year 1 green\r\n2 Year 2  gold\r\n3 Year 3  gold\r\n4 Year 4  gold\r\n5 Year 5  gold\r\n6 Year 6   red\r\n\r\nedges\r\n\r\n     from     to  cost color\r\n1  Year 1 Year 2  7000  grey\r\n2  Year 1 Year 3 12000  grey\r\n3  Year 1 Year 4 21000  grey\r\n4  Year 1 Year 5 31000  grey\r\n5  Year 1 Year 6 44000  grey\r\n6  Year 2 Year 3  7000  grey\r\n7  Year 2 Year 4 12000  grey\r\n8  Year 2 Year 5 21000  grey\r\n9  Year 2 Year 6 31000  grey\r\n10 Year 3 Year 4  7000  grey\r\n11 Year 3 Year 5 12000  grey\r\n12 Year 3 Year 6 21000  grey\r\n13 Year 4 Year 5  7000  grey\r\n14 Year 4 Year 6 12000  grey\r\n15 Year 5 Year 6  7000  grey\r\n\r\nC\r\n\r\n      [,1]  [,2]    [,3]    [,4]    [,5]    [,6]\r\n[1,] 1e+06 7e+03   12000   21000   31000   44000\r\n[2,] 1e+06 1e+06    7000   12000   21000   31000\r\n[3,] 1e+06 1e+06 1000000    7000   12000   21000\r\n[4,] 1e+06 1e+06 1000000 1000000    7000   12000\r\n[5,] 1e+06 1e+06 1000000 1000000 1000000    7000\r\n[6,] 1e+06 1e+06 1000000 1000000 1000000 1000000\r\n\r\nSolving the Problem\r\nNow that we have our cost matrix, the last ingredient is to solve the\r\nproblem. That means to solve the s-t shortest path from\r\nYear 0 to Year 6, so we can determine what is\r\nthe cheapest investment strategy for us. For this, we will be using the\r\nigraph package.\r\nVisualizing the network\r\n\r\n\r\ng <- igraph::graph_from_data_frame(d = edges,\r\n                                   directed = TRUE,\r\n                                   vertices = nodes)\r\n# Make it a little cleaner\r\nplot(g,\r\n     main = \"Cost of Annual Vehicle Trade-in\",\r\n     edge.arrow.size=.5,\r\n     vertex.color=V(g)$Color,\r\n     edge.color=E(g)$color,\r\n     edge.label = E(g)$cost,\r\n     edge.width = E(g)$cost/10000,\r\n     vertex.size=20,\r\n     vertex.frame.color=\"gray\",\r\n     vertex.label.color=\"black\",\r\n     vertex.label.cex=0.8,\r\n     vertex.label.dist=2, layout = layout_as_star,\r\n     edge.curved=0.2)\r\n\r\n\r\n\r\nShortest Path Solution\r\nSo once we perform the Dijkstra's Shortest Path\r\nalgorithm on the network we obtain a solution in matrix form. This\r\nsolution tells us the best possible cost for our car decision from\r\nYear 1 to Year 6 will be at\r\n$31,000.\r\nWhat this does not show us is what path was chosen to obtain that\r\nvalue.\r\n\r\n\r\n# Get the shortest paht cost matrix\r\ns_paths_cost <- igraph::shortest.paths(graph = g, v = V(g), weights = E(g)$cost, algorithm = \"dijkstra\")\r\ns_paths_cost\r\n\r\n       Year 1 Year 2 Year 3 Year 4 Year 5 Year 6\r\nYear 1      0   7000  12000  19000  24000  31000\r\nYear 2   7000      0   7000  12000  19000  24000\r\nYear 3  12000   7000      0   7000  12000  19000\r\nYear 4  19000  12000   7000      0   7000  12000\r\nYear 5  24000  19000  12000   7000      0   7000\r\nYear 6  31000  24000  19000  12000   7000      0\r\n\r\nShortest Path Analysis\r\nThe optimal selection is the following sequence:\r\nBuy a new car in Year 1 keep the car for a year, then\r\nsell on Year 2.\r\nBuy another car in Year 2, but keep for two years and\r\nsell on Year 4.\r\nFinally, buy a car on Year 4, keep for two years and\r\nsell on Year 6.\r\nDo these numbers add up? Let’s check. 7000 + 12000 + 12000 == 31000\r\nis TRUE.\r\nSo our least cost strategy can be no less than $31000\r\nover the next 6 years. With the current cost structure, means to keep\r\nthe car for a year or two, then pitch it because the trade-off between\r\nmaintenance accumulation and depreciation start to mutually deter from a\r\nleast cost decision.\r\n\r\n\r\n# Get all path distances solution vertex path\r\ns.paths <- igraph::shortest_paths(graph = g,\r\n                                  from = \"Year 1\",\r\n                                  output = \"vpath\",\r\n                                  weights = E(g)$cost, \r\n                                  to = \"Year 6\")\r\n                                  # v = V(g), \r\n                                  # to = V(g), \r\n                                  # weights = E(g)$cost)\r\n\r\n# Update colors from vertex path found\r\ns.paths$vpath\r\n\r\n[[1]]\r\n+ 4/6 vertices, named, from 81b1a31:\r\n[1] Year 1 Year 2 Year 4 Year 6\r\n\r\nE(g, path = s.paths$vpath[[1]])$color <- \"red\"\r\n\r\n\r\n\r\n\r\nplot(g,\r\n     main = \"Least Cost Vehicle Trade-in Policy\",\r\n     edge.arrow.size=.5,\r\n     vertex.color=V(g)$Color,\r\n     edge.color=E(g)$color,\r\n     edge.label = E(g)$cost,\r\n     edge.width = E(g)$cost/10000,\r\n     vertex.size=20,\r\n     vertex.frame.color=\"gray\",\r\n     vertex.label.color=\"black\",\r\n     vertex.label.cex=0.8,\r\n     vertex.label.dist=2, layout = layout_as_star,\r\n     edge.curved=0.2)\r\n\r\n\r\n\r\nThis plot doesn’t look too great! Let’s try to spruce it up a bit\r\nusing visNetwork, a common package in R that leverages the\r\nvis.js framework, which can be found here.\r\n\r\n\r\nV(g)$label = nodes$Year\r\nV(g)$shape = \"circle\"\r\nE(g)$width = edges$cost/10000\r\nE(g)$weight = edges$cost/10000\r\nE(g)$label = edges$cost %>% as.character\r\n\r\nvisIgraph(igraph = g)\r\n\r\n\r\n\r\nReferences\r\nExample taken from the following sources:\r\nWinston., Wayne. Operations Research, Applications and\r\nAlgorithms 4th Edition.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-09-shortest-path-optimization-in-r/shortest-path-optimization-in-r_files/figure-html5/viz_network-1.png",
    "last_modified": "2022-07-13T06:06:55-04:00",
    "input_file": "shortest-path-optimization-in-r.knit.md"
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Optimization Daily",
    "description": "Welcome to Optimization Daily! Grab a coffee, take a read, and enjoy your stay.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-09",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-07-10T20:34:19-04:00",
    "input_file": {}
  }
]
