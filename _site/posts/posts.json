[
  {
    "path": "posts/2022-07-16-recommender-systems-in-r/",
    "title": "Recommender Systems in R",
    "description": "In this post we will describe the sub-field of recommender systems, simulate some data, and solve the problem analytically using iterative regression.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-16",
    "categories": [
      "machine learning",
      "recommender systems",
      "regression",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nRecommender Systems as a\r\nField\r\nContent-based\r\nRecommendation\r\nCollaborative Filtering\r\n\r\nProblem Setup\r\nModel Parameters\r\nLoss Function\r\nIterative Regression\r\nSimulating Data\r\n\r\n\r\n(User, Movie, Rating) Tuples\r\nOrdinary Least Squares\r\nCreate the U matrix\r\nCreate the V matrix\r\nLearn the U matrix\r\nLearn the V matrix\r\n\r\nLearning From\r\nUsers and Item Embeddings\r\nRecommendations\r\n\r\n\r\nRecommender Systems as a\r\nField\r\nRecommender Systems as such is often referred to as a\r\nsub-field of machine learning. This is a class of learning problems that\r\nis entirely based on recommendation. As such, this sub-field has it’s\r\nown unique flavor of data. In short, there is some form of\r\nrecommendation as the objective of the problem. Whether it be a movie,\r\nsong, or some other object of potential interest to someone who has yet\r\nto actually experience it. The most popular paradigms to approaching the\r\nproblem are:\r\nContent-based Recommendation\r\nCollaborative Filtering\r\nContent-based Recommendation\r\nIn Content-based Recommendation the goal is acquire\r\nhighly intelligent information about the domain. By obtaining a\r\nfeature vector on the items, as users experience those\r\nitems we can train based on their preferences, then predict out new\r\nobservations using the standard classification paradigm. This is very\r\nconvenient, and powerful, as classification is time tested\r\nand can be produce great results. Some challenges this faces are that of\r\nsmall datasets, as users will have to experience things first, which may\r\ntake hours, days, or weeks in some cases.\r\nFeature Vector \\[\r\n\\phi(x) \\rightarrow \\mathbb{R^d}\r\n\\] The feature vector will take all of the items and produce a\r\nhighly articulate representation of it. How would you describe features\r\nof music? What about a movie? How about a trip to a state park? Though\r\ncomplex, many companies have had success at this. Pandora pays experts\r\nto accurately tag down a high dimensional space for this. As users\r\nexperience a movie they populate one of these d-vectors and\r\ntheir positive or negative experience to it will drill into the feature\r\nspace of preferences, making recommendation possible.\r\nData Set \\[\r\nD_{items}=\\{(\\phi(x_i), y_i)\\}_{i=1..n}\r\n\\] The data set becomes what item a user experiences as x and\r\ntheir response as y. Apply a simple transformation to make x tabular and\r\nboom … you’ve got a classification problem.\r\nCollaborative Filtering\r\nA far more common approach to solving the problem is that of\r\nCollaborative Filtering. The data set for this is an\r\naccumulation of all user experience. The data set has one\r\nform with two common notations.\r\nUser Rating Matrix \\[\r\nR_{i,j}\\in \\mathbb{R^{n,m}} \\: \\forall i \\in 1..n, \\: \\forall j \\in 1..m\r\n\\] The user rating matrix contains every user i and\r\nevery item j. Populated typically with a\r\n1 to 5 type of ordinal ranking. This matrix becomes HUGE\r\nvery quickly. Seldom can you actually load this into memory. We will\r\ndiscuss the tricks required in order to extract meaningful information\r\nthis later.\r\nDatabase \\[\r\nD=\\{(i,j,r)\\}_{i=1..l}\r\n\\]\r\nSince the above matrix will become so sparse, the database is often a\r\nbook keeping data structure to contain all of the non-zero elements of\r\nthis matrix. For some user i with some rating\r\nr for item j. We assume there are\r\nl of these. Clearly, we expect l to be\r\nsignificantly smaller than n*m which is the total number of\r\nentries in this matrix.\r\nProblem Setup\r\nWe would like to construct a matrix that will essentially fill in the\r\ngaps for us with a highly suggestive amount of potential rating, then\r\nargmax over any particular user and provide the top k as a\r\nrecommendation. We could construct a loss function for this, but it\r\nmight not be too useful. Instead, we will stand on the shoulders of\r\ngiants and set up our problem as follows:\r\nModel Parameters\r\nUser Latent Representation \\[\r\nU \\in \\mathbb{R^{n,k+1}}\r\n\\] Item Latent Representation \\[\r\nV \\in \\mathbb{R^{k+1,n}}\r\n\\]\r\nThe plus one represents the bias for each user and item respectively\r\n(e.g., how pessimistic I am, and in general how well the movie is liked\r\nby folks).\r\nU will contain information about each user lodged up in\r\nk+1 mystical components. V will contain information about\r\neach item lodged up in k+1 mystical components. These\r\nbecome a feature vector of sorts for users and items respectively.\r\nLoss Function\r\nThe loss function is as follows:\r\n\\[\r\nJ(U,V)=[\\frac{1}{2}\\sum_{(i,j,r)\\in D}{(u_i^Tv_j + b_{u_{i}} + b_{v_{j}}\r\n- r)^2}] + \\frac{\\lambda}{2}\\sum_{i=1..n}{||U_i||^2}+\r\n\\frac{\\lambda}{2}\\sum_{j=1..m}{||V_j||^2}\r\n\\]\r\nAt a glance this looks like typical linear regression. Don’t be\r\nfooled. Try taking the derivative and setting to zero you end up with a\r\nnasty set of equations in both directions. One clever trick to solve\r\nthis problem is what is known as iterative regression.\r\nIterative Regression\r\nIn iterative regression we will leverage the fact that we know how to\r\nsolve a simple regression problem analytically. If we hold\r\nV constant and solve for U using the typical\r\nordinary least squares model, we will arrive at cofficients for some\r\nuser. We can loop this for all users to obtain their estimates. Then, we\r\ncan hold these estimates U constant and go solve for\r\nV for all items. We will leverage the fact that we know the\r\nfollowing as true for any data set X, response column\r\nY and coefficient estimate theta from least\r\nsquares.\r\nOrdinary Least Squares Solution \\[\r\n\\theta^*=(X^TX)^{-1}X^TY\r\n\\]\r\nSimulating Data\r\nLet’s see if we can tackle this monster with some fake data.\r\n\r\n\r\n# Items\r\nM = 3\r\n\r\n# Users\r\nN = 10\r\n\r\n# Populate ratings matrix\r\nR = matrix(0, nrow=N, ncol=M)\r\n\r\n# Populating ratings matrix\r\nR[1,] <- c(2,0,5)\r\nR[2,] <- c(1,5,3)\r\nR[3,] <- c(0,0,5)\r\nR[4,] <- c(4,1,5)\r\nR[5,] <- c(0,0,2)\r\nR[6,] <- c(2,4,0)\r\nR[7,] <- c(0,3,5)\r\nR[8,] <- c(2,0,1)\r\nR[9,] <- c(0,1,2)\r\nR[10,] <- c(2,0,0)\r\n\r\n# for(n in 1:N){\r\n#   user_n_ratings = rbinom(M, 5, 0.5)\r\n#   \r\n#   R[n, ] = user_n_ratings\r\n#   \r\n#   # Put some holes in the data\r\n#   R[n, floor(runif(1, min=0, max=4))] = 0\r\n#   # R[n, floor(runif(1, min=0, max=4))] = 0\r\n# }\r\n\r\nR\r\n\r\n      [,1] [,2] [,3]\r\n [1,]    2    0    5\r\n [2,]    1    5    3\r\n [3,]    0    0    5\r\n [4,]    4    1    5\r\n [5,]    0    0    2\r\n [6,]    2    4    0\r\n [7,]    0    3    5\r\n [8,]    2    0    1\r\n [9,]    0    1    2\r\n[10,]    2    0    0\r\n\r\n(User, Movie, Rating) Tuples\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\n\r\n\r\nWe are all fully aware this matrix will never fit into memory, so\r\nlet’s make it a structure we will probably actually be working with in\r\nthe real world.\r\n\r\n\r\nD <- R %>% as.data.frame %>%\r\n  setNames(nm = 1:M) %>% \r\n  dplyr::mutate(user_id=row_number()) %>% \r\n  tidyr::gather(., key = \"item_id\", value =\"rating\", -user_id) %>% \r\n  dplyr::mutate(user_name = paste(\"User_\", user_id, sep=\"\"),\r\n                item_name = paste(\"Item_\", item_id, sep=\"\")) %>% \r\n  dplyr::filter(rating != 0)\r\nD\r\n\r\n   user_id item_id rating user_name item_name\r\n1        1       1      2    User_1    Item_1\r\n2        2       1      1    User_2    Item_1\r\n3        4       1      4    User_4    Item_1\r\n4        6       1      2    User_6    Item_1\r\n5        8       1      2    User_8    Item_1\r\n6       10       1      2   User_10    Item_1\r\n7        2       2      5    User_2    Item_2\r\n8        4       2      1    User_4    Item_2\r\n9        6       2      4    User_6    Item_2\r\n10       7       2      3    User_7    Item_2\r\n11       9       2      1    User_9    Item_2\r\n12       1       3      5    User_1    Item_3\r\n13       2       3      3    User_2    Item_3\r\n14       3       3      5    User_3    Item_3\r\n15       4       3      5    User_4    Item_3\r\n16       5       3      2    User_5    Item_3\r\n17       7       3      5    User_7    Item_3\r\n18       8       3      1    User_8    Item_3\r\n19       9       3      2    User_9    Item_3\r\n\r\nOrdinary Least Squares\r\nThat looks better. Now let’s see if we can do some iterative least\r\nsquares to solve this beast.\r\n\r\n\r\nlibrary(MASS)\r\n\r\nsolve.ols <- function(X,y){\r\n  theta = (ginv(t(X) %*% X) %*% t(X)) %*% y\r\n}\r\n\r\nX = matrix(rep(rnorm(2), 10), nrow = 10)\r\ny = matrix(rnorm(10), ncol=1)\r\ntheta = solve.ols(X,y)\r\ntheta\r\n\r\n             [,1]\r\n[1,] 0.0007774986\r\n[2,] 0.0007774986\r\n\r\nCreate the U matrix\r\nNeat-o, we can solve a single OLS problem. Let’s define our\r\nparameters.\r\n\r\n\r\n# Latent dimensions\r\nK = 3\r\n\r\n# User matrix\r\nU = matrix(rnorm(N*K), nrow=N, ncol=K)\r\n\r\n# Don't forget to append the bias vector for the items\r\nU = cbind(U, 1)\r\n\r\n# # Don't forget to append the bias vector\r\nU = cbind(U, rnorm(N))\r\n\r\n\r\nU\r\n\r\n            [,1]        [,2]        [,3] [,4]       [,5]\r\n [1,] -1.3270366 -1.64229706 -0.74543293    1 -0.3785241\r\n [2,]  0.7363001  0.51817104  1.68335141    1 -0.7233356\r\n [3,]  0.8175293 -0.06825585 -0.25251234    1 -1.3509502\r\n [4,]  0.3253387  1.31086086  0.48109986    1 -0.8145052\r\n [5,]  0.3341991 -1.40574984 -0.97059620    1  0.4811544\r\n [6,]  0.4674589 -0.82871156  1.51013370    1 -0.4384462\r\n [7,]  0.4585841  0.90775656 -0.18085370    1  1.5732527\r\n [8,] -0.5184098 -0.87080546  0.00712267    1  0.1115782\r\n [9,] -0.0118098  1.33809955  2.14143620    1 -0.3801498\r\n[10,]  0.7271083  0.71103304  1.23705950    1  0.2636276\r\n\r\nCreate the V matrix\r\n\r\n\r\n# Item matrix\r\nV = matrix(rnorm(K*M), nrow=K, ncol=M)\r\n\r\n# Don't forget to append the bias vector\r\nV = rbind(V, rnorm(M))\r\n\r\n# Don't forget to append the bias vector for the users\r\nV = rbind(V, 1)\r\n\r\nV\r\n\r\n           [,1]       [,2]       [,3]\r\n[1,] 0.22583268 -0.7298176 -1.5101003\r\n[2,] 0.10789959 -1.0803082  0.6284866\r\n[3,] 0.04698169 -0.5567730 -0.4943936\r\n[4,] 1.48580304 -0.1077601 -0.7066269\r\n[5,] 1.00000000  1.0000000  1.0000000\r\n\r\nI believe by alternating those columns of ones like that we will\r\nbundle up the bias terms inside both matrices. So then when we get our\r\ndot product from the problem, it will hit a bias term for the user and a\r\nbias term for the item with ones of alternating locations. Just remember\r\nthey are at the end. :)\r\nExcellent so far, now we need to perform loops in succession to solve\r\nthe following sub-problem.\r\n\\[\r\nU_i = \\frac{1}{2}\\sum_{(j | (i,j,r) \\in D)}{(u_i^Tv_j - r)^2}\r\n\\] Once we solve the above problem for all users i,\r\nwe will alternative over and solve the inverse problem of all items.\r\nThis is as follows.\r\n\\[\r\nV_j = \\frac{1}{2}\\sum_{(i | (i,j,r) \\in D)}{(u_i^Tv_j - r)^2}\r\n\\] Same exact setup, this just alternates which role will be data\r\nand which will be parameters. So for the user problem, the items will\r\nserve as data X and u will be the theta. On\r\nthe item side the users will serve as data X and items will\r\nbe the theta v. Let’s give it a shot and cause some\r\ntrouble.\r\nLearn the U matrix\r\n\r\n\r\n# Solve for U\r\nn = unique(D$user_id)\r\nfor(uid in as.numeric(n)){\r\n  \r\n  # The user id\r\n  # cat(uid, \"\\n\")\r\n  \r\n  # Get all the items now\r\n  all_items <- D %>% dplyr::filter(user_id ==  uid) %>% dplyr::pull(item_id)\r\n  m <- length(all_items)\r\n  \r\n  tmp_X <- matrix(0, nrow=m, ncol=K+2)\r\n  tmp_y <- matrix(0, nrow=m, ncol=1)\r\n  \r\n  idx = 1\r\n  for(iid in as.numeric(all_items)){\r\n    \r\n    # The item id\r\n    # cat(\" \", iid, \" \")\r\n    \r\n    # Store away the item vector for this record\r\n    tmp_X[idx, ] = V[,iid]\r\n    \r\n    # Store away the response element for this record\r\n    r = D %>% dplyr::filter(user_id == uid, item_id == iid) %>% dplyr::pull(rating)\r\n    tmp_y[idx] = r\r\n    \r\n    # Iterate to the next item\r\n    idx = idx + 1\r\n    \r\n  }\r\n  # cat(\"\\n\")\r\n  # break\r\n  \r\n  U[uid,] = solve.ols(tmp_X, tmp_y)\r\n}\r\n\r\nU\r\n\r\n            [,1]        [,2]        [,3]       [,4]      [,5]\r\n [1,] -1.6293749  0.82692416 -0.55330880  0.2547490 1.9262298\r\n [2,] -1.2218737 -1.74068943 -0.91970519 -0.1309888 1.7015915\r\n [3,] -1.7085888  0.71109522 -0.55937700 -0.7995063 1.1314406\r\n [4,] -1.4344886  2.12956316 -0.23031705  1.2422230 2.2592973\r\n [5,] -0.6834355  0.28443809 -0.22375080 -0.3198025 0.4525762\r\n [6,] -0.8218100 -1.30872541 -0.67801104  0.4704885 1.6596034\r\n [7,] -1.7616312  0.13835765 -0.71333716 -0.7238713 1.3886281\r\n [8,] -0.2728864  0.24286636 -0.10660255  0.7561736 0.9169049\r\n [9,] -0.6939606  0.17079081 -0.25430084 -0.3047944 0.5036095\r\n[10,]  0.1380201  0.06594401  0.02871337  0.9080647 0.6111609\r\n\r\nLearn the V matrix\r\nWe got the U matrix after performing i=1..n\r\nregressions and building up our vectors from the item data. Now let’s do\r\nit for the ratings matrix.\r\n\r\n\r\n# U[,M+1] = 1\r\n\r\n# Solve for V\r\nm = unique(D$item_id)\r\nfor(iid in as.numeric(m)){\r\n  \r\n  # The item id\r\n  # cat(iid, \"\\n\")\r\n  \r\n  # Get all the users now\r\n  all_users <- D %>% dplyr::filter(item_id ==  iid) %>% dplyr::pull(user_id)\r\n  n <- length(all_users)\r\n  \r\n  tmp_X <- matrix(0, nrow=n, ncol=K+2)\r\n  tmp_y <- matrix(0, nrow=n, ncol=1)\r\n  \r\n  idx = 1\r\n  for(uid in as.numeric(all_users)){\r\n    \r\n    # The user id\r\n    # cat(\" \", uid, \" \")\r\n    \r\n    # Store away the user vector for this record\r\n    tmp_X[idx, ] = U[uid,]\r\n    \r\n    # Store away the response element for this record, same as last time\r\n    r = D %>% dplyr::filter(user_id == uid, item_id == iid) %>% dplyr::pull(rating)\r\n    tmp_y[idx] = r\r\n    \r\n    # Iterate to the next item\r\n    idx = idx + 1\r\n    \r\n  }\r\n\r\n  V[,iid] = solve.ols(tmp_X, tmp_y)\r\n}\r\n\r\nV\r\n\r\n           [,1]       [,2]       [,3]\r\n[1,] 0.22583268 -0.7298176 -1.5101003\r\n[2,] 0.10789959 -1.0803082  0.6284866\r\n[3,] 0.04698169 -0.5567730 -0.4943936\r\n[4,] 1.48580304 -0.1077601 -0.7066269\r\n[5,] 1.00000000  1.0000000  1.0000000\r\n\r\n\r\n\r\nt(V)\r\n\r\n           [,1]       [,2]        [,3]       [,4] [,5]\r\n[1,]  0.2258327  0.1078996  0.04698169  1.4858030    1\r\n[2,] -0.7298176 -1.0803082 -0.55677298 -0.1077601    1\r\n[3,] -1.5101003  0.6284866 -0.49439360 -0.7066269    1\r\n\r\nU\r\n\r\n            [,1]        [,2]        [,3]       [,4]      [,5]\r\n [1,] -1.6293749  0.82692416 -0.55330880  0.2547490 1.9262298\r\n [2,] -1.2218737 -1.74068943 -0.91970519 -0.1309888 1.7015915\r\n [3,] -1.7085888  0.71109522 -0.55937700 -0.7995063 1.1314406\r\n [4,] -1.4344886  2.12956316 -0.23031705  1.2422230 2.2592973\r\n [5,] -0.6834355  0.28443809 -0.22375080 -0.3198025 0.4525762\r\n [6,] -0.8218100 -1.30872541 -0.67801104  0.4704885 1.6596034\r\n [7,] -1.7616312  0.13835765 -0.71333716 -0.7238713 1.3886281\r\n [8,] -0.2728864  0.24286636 -0.10660255  0.7561736 0.9169049\r\n [9,] -0.6939606  0.17079081 -0.25430084 -0.3047944 0.5036095\r\n[10,]  0.1380201  0.06594401  0.02871337  0.9080647 0.6111609\r\n\r\nLearning From Users and\r\nItem Embeddings\r\nPretty sweet. Now we have two matrices U and\r\nV. U for our users. V for our\r\nitems. But they are SUBSTANTIALLY smaller than our initial matrix\r\n\r\n\r\nU %>% dim\r\n\r\n[1] 10  5\r\n\r\nV %>% dim\r\n\r\n[1] 5 3\r\n\r\nR %>% dim\r\n\r\n[1] 10  3\r\n\r\ncat(\"\\n\")\r\n\r\n\r\nnrow(U) * ncol(U)\r\n\r\n[1] 50\r\n\r\nnrow(V) * ncol(V)\r\n\r\n[1] 15\r\n\r\nnrow(R) * ncol(R)\r\n\r\n[1] 30\r\n\r\nWe didn’t really gas the problem, but we literally cut it in half.\r\nThat is 550 entries of data to learn instead of 1000. Imagine if we had\r\nTONS of items (which most do) and TONS of users (which most do). There\r\nit is folks. Now let’s see what we’ve learned\r\nFor users the bias column was in the last slot. Let’s see what each\r\nusers bias rating is.\r\n\r\n\r\nlibrary(ggplot2)\r\n\r\n\r\nWe want to be careful about interpreting that intercept necessarily\r\nas the basic, because it is ultimately created as a projection with many\r\nof things being factored in. Namely V1..V3. With each\r\nvariable it collectively minimizes things with that offset.\r\n\r\n\r\nU_df <- U %>% \r\n  as.data.frame %>%\r\n  dplyr::rename(user_bias = V4) %>%\r\n  dplyr::mutate(user = paste(\"User_\", row_number(), sep=\"\"),\r\n                user_actual_mean_rating = apply(R, 1, mean)) \r\n\r\nrow.names(U_df) <- U_df$user\r\nU_df\r\n\r\n                V1          V2          V3  user_bias        V5\r\nUser_1  -1.6293749  0.82692416 -0.55330880  0.2547490 1.9262298\r\nUser_2  -1.2218737 -1.74068943 -0.91970519 -0.1309888 1.7015915\r\nUser_3  -1.7085888  0.71109522 -0.55937700 -0.7995063 1.1314406\r\nUser_4  -1.4344886  2.12956316 -0.23031705  1.2422230 2.2592973\r\nUser_5  -0.6834355  0.28443809 -0.22375080 -0.3198025 0.4525762\r\nUser_6  -0.8218100 -1.30872541 -0.67801104  0.4704885 1.6596034\r\nUser_7  -1.7616312  0.13835765 -0.71333716 -0.7238713 1.3886281\r\nUser_8  -0.2728864  0.24286636 -0.10660255  0.7561736 0.9169049\r\nUser_9  -0.6939606  0.17079081 -0.25430084 -0.3047944 0.5036095\r\nUser_10  0.1380201  0.06594401  0.02871337  0.9080647 0.6111609\r\n           user user_actual_mean_rating\r\nUser_1   User_1               2.3333333\r\nUser_2   User_2               3.0000000\r\nUser_3   User_3               1.6666667\r\nUser_4   User_4               3.3333333\r\nUser_5   User_5               0.6666667\r\nUser_6   User_6               2.0000000\r\nUser_7   User_7               2.6666667\r\nUser_8   User_8               1.0000000\r\nUser_9   User_9               1.0000000\r\nUser_10 User_10               0.6666667\r\n\r\n\r\n\r\nggplot(data = U_df, aes(label = user_bias)) +\r\n  geom_col(aes(x=user, y=user_actual_mean_rating, fill = \"Average Rating\"), alpha = 0.5) +\r\n  geom_col(aes(x=user, y=user_bias, fill=\"User Bias\"), alpha = 0.5) +\r\n  ggtitle(\"Latent User Bias vs. Average Rating\") +\r\n  labs(fill = \"User Centrality\") +\r\n  theme_bw() +\r\n  theme(axis.text.x = element_text(angle = 90)) +\r\n  xlab(\"User\") +\r\n  ylab(\"Rating\")\r\n\r\n\r\n\r\nSince the iterative regression trains the users first, we will lose\r\nbias information for the\r\nRecommendations\r\nWe can now construct our rating matrix at once, or piece by piece\r\nsince we have U and V.\r\n\r\n\r\nR_reconstructed <- U %*% V\r\n\r\nR_reconstructed\r\n\r\n             [,1]      [,2]      [,3]\r\n [1,]  2.00000000 2.5026590  5.000000\r\n [2,]  1.00000000 5.0000000  3.000000\r\n [3,] -0.39187712 2.0077978  5.000000\r\n [4,]  4.00000000 1.0000000  5.000000\r\n [5,] -0.15675085 0.8031191  2.000000\r\n [6,]  2.00000000 4.0000000  2.080847\r\n [7,] -0.10332101 3.0000000  5.000000\r\n [8,]  2.00000000 0.8315597  1.000000\r\n [9,] -0.09949318 1.0000000  2.000000\r\n[10,]  2.00000000 0.3253515 -0.211677\r\n\r\nR\r\n\r\n      [,1] [,2] [,3]\r\n [1,]    2    0    5\r\n [2,]    1    5    3\r\n [3,]    0    0    5\r\n [4,]    4    1    5\r\n [5,]    0    0    2\r\n [6,]    2    4    0\r\n [7,]    0    3    5\r\n [8,]    2    0    1\r\n [9,]    0    1    2\r\n[10,]    2    0    0\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-16-recommender-systems-in-r/recommender-systems-in-r_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2022-07-16T16:54:12-04:00",
    "input_file": "recommender-systems-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-15-regression-with-keras-in-r/",
    "title": "Regression With Keras in R",
    "description": "In this post we will explore how to do deep regression in R using Keras.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-15",
    "categories": [
      "deep learning",
      "machine learning",
      "regression",
      "keras",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nBoston Housing Prices\r\nVisualize Correlation\r\nVisualize Serious\r\nCorrelation\r\nStandardize the data\r\nModel\r\nEvaluate\r\nVisualize\r\n\r\nDropping Correlated\r\nFeatures\r\nInferentials\r\n\r\nReferences\r\n\r\nBoston Housing Prices\r\n\r\n\r\nlibrary(tfdatasets)\r\nlibrary(keras)\r\nlibrary(tidyr)\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\n\r\n\r\nboston_housing <- dataset_boston_housing()\r\ntrain_data <- boston_housing$train$x\r\ntrain_labels <- boston_housing$train$y\r\n\r\ntest_data <- boston_housing$test$x\r\ntest_labels <- boston_housing$test$y\r\n\r\npaste0(\"Training entries: \", length(train_data), \", labels: \", length(train_labels))\r\n\r\n[1] \"Training entries: 5252, labels: 404\"\r\n\r\n\r\n\r\ncolumn_names <- c('CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \r\n                  'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT')\r\n\r\ntrain_df <- train_data %>% \r\n  dplyr::as_tibble(.name_repair = \"minimal\") %>% \r\n  stats::setNames(., column_names) %>% \r\n  dplyr::mutate(label = train_labels)\r\ntrain_df\r\n\r\n# A tibble: 404 x 14\r\n     CRIM    ZN INDUS  CHAS   NOX    RM   AGE   DIS   RAD   TAX\r\n    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\r\n 1 1.23     0    8.14     0 0.538  6.14  91.7  3.98     4   307\r\n 2 0.0218  82.5  2.03     0 0.415  7.61  15.7  6.27     2   348\r\n 3 4.90     0   18.1      0 0.631  4.97 100    1.33    24   666\r\n 4 0.0396   0    5.19     0 0.515  6.04  34.5  5.99     5   224\r\n 5 3.69     0   18.1      0 0.713  6.38  88.4  2.57    24   666\r\n 6 0.284    0    7.38     0 0.493  5.71  74.3  4.72     5   287\r\n 7 9.19     0   18.1      0 0.7    5.54 100    1.58    24   666\r\n 8 4.10     0   19.6      0 0.871  5.47 100    1.41     5   403\r\n 9 2.16     0   19.6      0 0.871  5.63 100    1.52     5   403\r\n10 1.63     0   21.9      0 0.624  5.02 100    1.44     4   437\r\n# ... with 394 more rows, and 4 more variables: PTRATIO <dbl>,\r\n#   B <dbl>, LSTAT <dbl>, label <dbl>\r\n\r\ntest_df <- test_data %>% \r\n  dplyr::as_tibble(.name_repair = \"minimal\") %>% \r\n  stats::setNames(., column_names) %>% \r\n  dplyr::mutate(label = test_labels)\r\ntest_df\r\n\r\n# A tibble: 102 x 14\r\n      CRIM    ZN INDUS  CHAS   NOX    RM   AGE   DIS   RAD   TAX\r\n     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\r\n 1 18.1        0 18.1      0 0.679  6.43 100    1.83    24   666\r\n 2  0.123      0 10.0      0 0.547  5.91  92.9  2.35     6   432\r\n 3  0.0550     0  5.19     0 0.515  5.98  45.4  4.81     5   224\r\n 4  1.27       0 19.6      1 0.605  6.25  92.6  1.80     5   403\r\n 5  0.0715     0  4.49     0 0.449  6.12  56.8  3.75     3   247\r\n 6  0.280      0  9.69     0 0.585  5.93  42.6  2.38     6   391\r\n 7  0.0305    55  3.78     0 0.484  6.87  28.1  6.47     5   370\r\n 8  0.0355    25  4.86     0 0.426  6.17  46.7  5.40     4   281\r\n 9  0.0930     0 25.6      0 0.581  5.96  92.9  2.09     2   188\r\n10  3.57       0 18.1      0 0.58   6.44  75    2.90    24   666\r\n# ... with 92 more rows, and 4 more variables: PTRATIO <dbl>,\r\n#   B <dbl>, LSTAT <dbl>, label <dbl>\r\n\r\n\r\n\r\ntrain_labels[1:10]\r\n\r\n [1] 15.2 42.3 50.0 21.1 17.7 18.5 11.3 15.6 15.6 14.4\r\n\r\nVisualize Correlation\r\n\r\n\r\n# See what features are correlated\r\n# Center and normalize\r\nXtrain <- train_df %>% select(-label)\r\nXs <- scale(Xtrain, center = TRUE, scale = TRUE)\r\n\r\n\r\n# Tidy the data\r\nC <- cor(Xs) %>% as.data.frame\r\nC$from <- row.names(C)\r\nC <- C %>% tidyr::gather(., key = \"to\", value = \"correlation\", -from)\r\n\r\n# Present a heatmap\r\nplt <- ggplot(data = C) + \r\n  geom_tile(aes(x=to, y=from, fill=correlation)) + \r\n  scale_fill_gradient(low = \"red\", high = \"green\") +\r\n  ggtitle(\"Correlation between Boston House features\") +\r\n  theme(axis.text.x = element_text(angle = 90)) \r\n\r\nplt\r\n\r\n\r\n\r\nA few areas of concern are those with greater than about 0.5\r\ncorrelation. This could mean they are excessive information and\r\nredundant. Let’s look at those who have a magnitude greater than between\r\n0.25 and 0.5.\r\nVisualize Serious\r\nCorrelation\r\nBelow is an analysis of some of the correlations. I performed a\r\ndetailed analysis on a pre-centered correlation matrix, which I will\r\nleave below for kicks. But ultimately, high correlated features\r\nshould be considered for removal from the model.\r\n\r\n\r\n# Present a heatmap\r\nplt <- ggplot(data = C %>% filter(abs(correlation) > 0.7)) + \r\n  geom_tile(aes(x=to, y=from, fill=correlation)) + \r\n  scale_fill_gradient(low = \"red\", high = \"green\") +\r\n  ggtitle(\"Correlations between Boston House features\") +\r\n  theme_bw() + \r\n  theme(axis.text.x = element_text(angle = 90))\r\n\r\nplt\r\n\r\n\r\n\r\nThere are several columns that could be dropped due to their high\r\ncorrelation with other variables. Let’s keep track of these for a hot\r\nminute. We will drop these and remodel later to see if this helps\r\nany.\r\n\r\n\r\nEXCESSIVE_FEATURES <- c(\"DIS\", \"NOX\", \"TAX\")\r\nEXCESSIVE_FEATURES\r\n\r\n[1] \"DIS\" \"NOX\" \"TAX\"\r\n\r\nStandardize the data\r\n\r\n\r\n# Center and normalize\r\nXtrain <- train_df %>% select(-label)\r\nXs_train <- scale(Xtrain, center = T, scale = T)\r\ntrain_df_scaled <- cbind(Xs_train, train_df %>% select(label))\r\n\r\nXtest <- test_df %>% select(-label)\r\nXs_test <- scale(Xtest, center = T, scale = T)\r\ntest_df_scaled <- cbind(Xs_test, test_df %>% select(label))\r\n\r\n\r\nModel\r\nSome cheat codes are required to make keras work with\r\ndataframes. First, you have to make a multi-head model. Define an\r\ninput with your dimensionality of the data frame. Second,\r\ncreate your other layers to pass it through. Merge them using the\r\nkeras_model function. R is great, but can be very painful\r\nat times. This was not pleasant. Further, only feed in\r\nmatrices. Don’t even fight with throwing in a data frame. This\r\nAPI isn’t smart like that.\r\n\r\n\r\n# Input head\r\ninput <- keras::layer_input(shape = c(13))\r\n\r\n# Output head\r\noutput <- input %>% \r\n  keras::layer_batch_normalization() %>%  \r\n  keras::layer_dense(units = 1)\r\n\r\n# Join input and output heads\r\nmodel <- keras::keras_model(input, output)\r\n\r\nmodel %>% keras::compile(\r\n  loss = \"mse\",\r\n  optimizer = \"adam\",\r\n  metrics = list(\"mean_absolute_error\")\r\n)\r\n\r\nhistory = model %>% keras::fit(\r\n  x = train_df %>% select(-label) %>% as.matrix,\r\n  y = train_df$label %>% as.matrix,\r\n  validation_split = 0.1,\r\n  epochs = 100,\r\n  verbose=0\r\n)\r\n\r\n\r\nplot(history)\r\n\r\n\r\n\r\nEvaluate\r\n\r\n\r\nevaluate(model,\r\n         x = test_df %>% select(-label) %>% as.matrix, \r\n         y = test_df$label %>% as.matrix)\r\n\r\n               loss mean_absolute_error \r\n          39.428188            4.456439 \r\n\r\nVisualize\r\n\r\n\r\nypred <- predict(model, \r\n                x = test_df %>% select(-label) %>% as.matrix)\r\n\r\ntmp <- data.frame(id = 1:length(ypred),\r\n                  prediction = ypred,\r\n                  target = test_df$label)\r\ntmp <- tmp %>% dplyr::mutate(mse = sqrt((tmp$prediction - tmp$target)**2),\r\n                             error = tmp$prediction - tmp$target)\r\n\r\nplt <- ggplot( data = tmp) + \r\n  geom_point(aes(x=id, y=prediction, color=\"prediction\")) +\r\n  geom_point(aes(x=id, y=target, color=\"target\")) +\r\n  theme_classic()\r\n\r\nggplot(tmp) + \r\n  geom_histogram(aes(x=mse, fill=\"mse\"), alpha = 0.4) +\r\n  geom_histogram(aes(x=error, fill=\"error\"), alpha=0.4) + \r\n  labs(fill = \"Error Distribution\") +\r\n  ggtitle(\"Test Error\") +\r\n  xlab(\"Error\") +\r\n  ylab(\"Occurances\") +\r\n  theme_light()\r\n\r\n\r\nshapiro.test(tmp$error) # This is probably normal. We can do inferential statistics on the population!\r\n\r\n\r\n    Shapiro-Wilk normality test\r\n\r\ndata:  tmp$error\r\nW = 0.91352, p-value = 5.358e-06\r\n\r\nDropping Correlated Features\r\nIn efforts to save time and effort. The experiment to drop\r\ncorrelation variables EXCESSIVE FEATURES turned out to\r\nactually make the model worse. This is probably due to the fact that\r\nthey store valuable information in this relatively small data set.\r\nInferentials\r\nSince we built our regression model with just one layer, the weights\r\nin that layer are in fact our parameters\r\n\r\n\r\noutput_layer <- model$layers[[3]]\r\n\r\nweights <- output_layer$get_weights()\r\ntheta <- weights[[1]]\r\nbias <- weights[[2]]\r\ntheta\r\n\r\n           [,1]\r\n [1,] -1.305097\r\n [2,]  1.302772\r\n [3,] -1.570979\r\n [4,]  1.210512\r\n [5,]  1.595219\r\n [6,] -1.211719\r\n [7,] -1.302150\r\n [8,]  1.229870\r\n [9,]  1.612442\r\n[10,] -1.396658\r\n[11,]  1.123266\r\n[12,] -1.451900\r\n[13,] -1.708958\r\n\r\nest_df <- data.frame(variable = c(names(train_df %>% select(-label)), \"intercept\"),\r\n                     estimate = c(theta, bias)\r\n          )\r\n\r\n\r\nggplot(data = est_df) + \r\n  geom_col(aes(x=variable, y=estimate, fill=variable)) + \r\n  coord_flip() +\r\n  labs(fill = \"Variable Estimate\") +\r\n  ggtitle(\"Boston Housing Factors\") +\r\n  ylab(\"Coefficient Estimate\") +\r\n  xlab(\"Variable\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nReferences\r\nhttps://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_regression/\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-15-regression-with-keras-in-r/regression-with-keras-in-r_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2022-07-15T19:47:02-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-15-text-classification-with-keras-in-r/",
    "title": "Text Classification With Keras in R",
    "description": "In this post we will walk through text classification using keras in R.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-15",
    "categories": [
      "deep learning",
      "machine learning",
      "classification",
      "natural language processing",
      "keras",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nMovie Reviews\r\nData Landscape\r\nSentence Length\r\nVisualization\r\nVectorizer\r\nAdapt\r\nSample Sentence\r\n\r\nBuild the Model\r\nLoss Function and\r\nOptimizer\r\nTrain the model\r\nEvaluate\r\nPlot Metrics\r\nWhat’s in that layer?\r\n\r\nReferences\r\n\r\nMovie Reviews\r\nThis problem is a binary classification problem. So we have a\r\ntext-based data set and a binary response variable.\r\n\r\n\r\nlibrary(keras)\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\nlibrary(purrr)\r\n\r\n\r\n\r\n\r\npath <- \"C:/Users/blake/Desktop/blog_data/movie_review.csv\"\r\n\r\ndf <- readr::read_csv(path)\r\ndf %>% dplyr::glimpse()\r\n\r\nRows: 64,720\r\nColumns: 6\r\n$ fold_id <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\r\n$ cv_tag  <chr> \"cv000\", \"cv000\", \"cv000\", \"cv000\", \"cv000\", \"cv000\"~\r\n$ html_id <chr> \"29590\", \"29590\", \"29590\", \"29590\", \"29590\", \"29590\"~\r\n$ sent_id <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15~\r\n$ text    <chr> \"films adapted from comic books have had plenty of s~\r\n$ tag     <chr> \"pos\", \"pos\", \"pos\", \"pos\", \"pos\", \"pos\", \"pos\", \"po~\r\n\r\nData Landscape\r\nThe data appears to have over 60,000 rows, each containing a movie\r\nreview in the text column with a response in the\r\ntag column for pos or neg (of the\r\nreview).\r\n\r\n\r\n# The spread on the response looks pretty even\r\ndf %>% dplyr::count(tag)\r\n\r\n# A tibble: 2 x 2\r\n  tag       n\r\n  <chr> <int>\r\n1 neg   31783\r\n2 pos   32937\r\n\r\n\r\n\r\ndf$text[1]\r\n\r\n[1] \"films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before .\"\r\n\r\n\r\n\r\ntraining_id <- sample(nrow(df), size = nrow(df)*.8)\r\ntraining <- df[training_id,]\r\ntesting <- df[-training_id,]\r\n\r\n\r\nIt might be useful to know the number of words in each review.\r\nSentence Length\r\nVisualization\r\n\r\n\r\ndf$text %>% \r\n  strsplit(., \" \") %>% \r\n  sapply(., length) %>% \r\n  hist(main = \"Distribution of Words Per Sentiment\", xlab = \"Number of Words\")\r\n\r\n\r\n\r\nWe will make the text a tensor. The most common 10,000\r\nwords will be specified by an integer. Every sequence will be\r\nrepresented by a sequence of integers.\r\nVectorizer\r\n\r\n\r\n# total words to account ids for, otherwise drop - this becomes the [UNK] token to represent unknown characters.\r\nnum_words <- 10000 \r\n\r\n# per sequence, how many words to keep, or pad to\r\nmax_length <- 50 \r\n\r\n# Fit the parameters into the vectorizer\r\ntext_vectorization <- keras::layer_text_vectorization(\r\n  max_tokens = num_words,\r\n  output_sequence_length = max_length\r\n)\r\n\r\n\r\nAdapt\r\nNext we will adapt the Text Vectorization. Once we\r\nadapt the layer will learn unique words in our dataset and\r\nassign integer values for each.\r\n\r\n\r\n# Now it knows the vocab of 10,000\r\n# Each will become a size 50 vector, of shape Nx50 now.\r\n\r\ntext_vectorization %>% \r\n  adapt(df$text)\r\n\r\n\r\n\r\n\r\n# Notice they are all tokeninzed and lower cased nice and neat.\r\nget_vocabulary(text_vectorization)[1:100]\r\n\r\n  [1] \"\"           \"[UNK]\"      \"the\"        \"a\"          \"and\"       \r\n  [6] \"of\"         \"to\"         \"is\"         \"in\"         \"that\"      \r\n [11] \"it\"         \"as\"         \"with\"       \"for\"        \"his\"       \r\n [16] \"this\"       \"film\"       \"but\"        \"he\"         \"i\"         \r\n [21] \"on\"         \"are\"        \"by\"         \"be\"         \"its\"       \r\n [26] \"an\"         \"not\"        \"one\"        \"movie\"      \"who\"       \r\n [31] \"from\"       \"at\"         \"was\"        \"have\"       \"has\"       \r\n [36] \"her\"        \"you\"        \"they\"       \"all\"        \"so\"        \r\n [41] \"like\"       \"about\"      \"out\"        \"more\"       \"when\"      \r\n [46] \"which\"      \"their\"      \"up\"         \"or\"         \"what\"      \r\n [51] \"some\"       \"just\"       \"if\"         \"there\"      \"she\"       \r\n [56] \"him\"        \"into\"       \"even\"       \"only\"       \"than\"      \r\n [61] \"no\"         \"we\"         \"good\"       \"most\"       \"time\"      \r\n [66] \"can\"        \"will\"       \"story\"      \"films\"      \"been\"      \r\n [71] \"would\"      \"much\"       \"also\"       \"characters\" \"other\"     \r\n [76] \"get\"        \"character\"  \"do\"         \"them\"       \"very\"      \r\n [81] \"two\"        \"first\"      \"after\"      \"see\"        \"well\"      \r\n [86] \"because\"    \"way\"        \"make\"       \"any\"        \"does\"      \r\n [91] \"really\"     \"had\"        \"too\"        \"while\"      \"how\"       \r\n [96] \"little\"     \"life\"       \"where\"      \"were\"       \"plot\"      \r\n\r\nSample Sentence\r\nLets convert one sample. Since tensorflow will only accept matrices,\r\nwe will transform that sample into a matrix and pass it in. Out comes\r\nthe 1x50 vector transformation.\r\n\r\n\r\n# Lets convert one sample\r\nx <- matrix(df$text[1], ncol=1)\r\n\r\n# Since tensorflow will ONLY accept matrices, we have to make it a fat 1x1 matrix and throw it in. Out comes a 1x50\r\ntext_vectorization(x)\r\n\r\ntf.Tensor(\r\n[[  68 2835   30  359 1662   33   91 1056    5  632  631  321   41 7803\r\n   709 4865 1767   48 7600 1337  398 5161   48    2    1 1808 1800  148\r\n    17  140  109   90   69    3  359  408   40   30  503  142    0    0\r\n     0    0    0    0    0    0    0    0]], shape=(1, 50), dtype=int64)\r\n\r\n# Out comes a 1x50. It looks to be rightside padded.\r\n\r\n\r\nBuild the Model\r\n\r\n\r\ninput <- layer_input(shape = c(1), dtype = \"string\")\r\n\r\noutput <- input %>% \r\n  #custom layer, input text; output tensor\r\n  text_vectorization() %>% \r\n  # produce 16 new dimensions for each sentiment \r\n  layer_embedding(input_dim = num_words, output_dim = 16) %>% \r\n  # average over the new 16 dimensions\r\n  layer_global_average_pooling_1d() %>% \r\n  layer_dense(units = 16, activation = \"relu\") %>% \r\n  layer_dropout(0.5) %>% \r\n  layer_dense(units = 1, activation = \"sigmoid\")\r\n\r\nmodel <- keras_model(input, output)\r\n\r\n\r\nI politely steal this from the documentation,\r\n“The first layer is an embedding layer. This layer takes the\r\ninteger-encoded vocabulary and looks up the embedding vector for each\r\nword-index. These vectors are learned as the model trains. The\r\nvectors add a dimension to the output array. The resulting dimensions\r\nare: (batch, sequence, embedding).”\r\nSo the word embedding layerdoes indeed\r\nexpand the dimensions for each word index. It\r\nlearns these as it trains. Once complete, we can go back and pluck this\r\nlayer out and go start to visualize words from our corpus against one\r\nanother. Maybe we could perform the famous\r\nman - woman = king - queen example? More to follow.\r\n“Next, a global_average_pooling_1d layer returns a\r\nfixed-length output vector for each example by averaging over the\r\nsequence dimension. This allows the model to handle input of\r\nvariable length, in the simplest way possible.”\r\nI gladly steal more excellent interpretation\r\n” HIDDEN UNITS The above model has two intermediate or “hidden”\r\nlayers, between the input and output. The number of outputs (units,\r\nnodes, or neurons) is the dimension of the representational space for\r\nthe layer. In other words, the amount of freedom the network is allowed\r\nwhen learning an internal representation.\r\nIf a model has more hidden units (a higher-dimensional representation\r\nspace), and/or more layers, then the network can learn more complex\r\nrepresentations. However, it makes the network more computationally\r\nexpensive and may lead to learning unwanted patterns — patterns that\r\nimprove performance on training data but not on the test data. This is\r\ncalled overfitting, and we’ll explore it later”\r\nLoss Function and Optimizer\r\n\r\n\r\nmodel %>% compile(\r\n  optimizer = 'adam',\r\n  loss = 'binary_crossentropy',\r\n  metrics = list(\"accuracy\")\r\n)\r\n\r\n\r\nTrain the model\r\n\r\n\r\nhistory <- model %>% fit(\r\n  x = training$text,\r\n  y = as.numeric(training$tag == \"pos\"),\r\n  epochs = 10,\r\n  batch_size = 512,\r\n  validation_split = 0.2,\r\n  verbose = 2\r\n)\r\n\r\n\r\nEvaluate\r\n\r\n\r\nevaluate(model, testing$text, \r\n         as.numeric(testing$tag == \"pos\"), \r\n         verbose = 0)\r\n\r\n     loss  accuracy \r\n0.5962703 0.6785383 \r\n\r\nThis fairly naive approach achieves an accuracy of about 68%. With\r\nmore advanced approaches, the model should get closer to 85%.\r\nPlot Metrics\r\n\r\n\r\nplot(history)\r\n\r\n\r\n\r\nWhat’s in that layer?\r\nThat’s the magic question. Time to take a peak.\r\n\r\n\r\nembeddings <- model$layers[[3]]\r\n\r\n# What a treasure trove. For each word, we have a 16 dimensional representation it learned.\r\nX <- embeddings$embeddings %>% as.matrix\r\ndim(X)\r\n\r\n[1] 10000    16\r\n\r\ncomponents <- prcomp(X)\r\nPC <- components$x %>% as.data.frame %>% select(PC1, PC2)\r\nPC <- PC %>% dplyr::mutate(word = get_vocabulary(text_vectorization))\r\n\r\nggplot(data = PC, aes(x=PC1, y=PC2, label = word)) + \r\n  geom_point() + \r\n  geom_text() + \r\n  theme_bw() +\r\n  ggtitle(\"Word Embeddings\", subtitle = \"PCA Dimensionality Reduction\")+\r\n  geom_vline(xintercept=min(PC$PC1), color=\"red\") +\r\n  geom_vline(xintercept=-0.1, color=\"red\", linetype=\"dotted\") +\r\n  geom_vline(xintercept=max(PC$PC1), color=\"green\") +\r\n  geom_vline(xintercept=0.1, color=\"green\", linetype=\"dotted\")\r\n\r\n\r\n# What about those just in a certain region?\r\n# ggplot(data = PC %>% filter(PC1 < 2, PC1 > 1.5,\r\n#                             PC2 < 0.1, PC2 > -0.1),\r\n#        aes(x=PC1, y=PC2, label = word)) + \r\n#   geom_point() + \r\n#   geom_text() + \r\n#   theme_bw()\r\n\r\n\r\nKind of amazing. Words such as …\r\nOutstanding\r\nBreathtaking\r\nFantastic\r\nGuido (which is usually a term for awesome)\r\nDamon (probably for Matt Damon, who is pretty awesome)\r\n… are all in just this little region. We learned a lot in this little\r\nmodel. Let’s look at one more visual with just our first two dimensions\r\nof word embeddings without pca.\r\n\r\n\r\nX <- embeddings$embeddings %>% \r\n  as.matrix %>% \r\n  as.data.frame %>% \r\n  select(V1, V2, V3) %>% \r\n  mutate(word = get_vocabulary(text_vectorization))\r\n\r\nggplot(data = X,\r\n       aes(x=V1, y=V2, color = V3, label = word)) + \r\n  geom_point() + \r\n  geom_text() + \r\n  theme_bw() +\r\n  xlab(\"Embedding 1\") +\r\n  ylab(\"Embedding 2\") +\r\n  labs(color = \"Embedding 3\") +\r\n  ggtitle(\"Word Embeddings\", subtitle = \"First Three Embeddings\") +\r\n  geom_vline(xintercept=min(X$V1), color=\"red\") +\r\n  geom_vline(xintercept=-0.01, color=\"red\", linetype=\"dotted\") +\r\n  geom_vline(xintercept=max(X$V1), color=\"green\") +\r\n  geom_vline(xintercept=0.01, color=\"green\", linetype=\"dotted\")\r\n\r\n\r\n# Maybe we can drill in\r\n# ggplot(data = X %>% filter(V1 < -0.4, V2 >-0.5),\r\n#        aes(x=V1, y=V2, color = V3, label = word)) + \r\n#   geom_point() + \r\n#   geom_text() + \r\n#   xlab(\"Embedding 1\") +\r\n#   ylab(\"Embedding 2\") +\r\n#   labs(color = \"Embedding 3\") +\r\n#   theme_bw() \r\n\r\n\r\nPretty amazing, even just the first few layers of word embeddings\r\nshow the same thing. Words like:\r\nawful\r\nwaste\r\njawbreaker\r\nrediculous\r\nstupid\r\nstupidity\r\nIncredible. This language model has learned words from our custom\r\ndomain.\r\nOne can only imagine that there are two populations of words because\r\nthis domain was specifically trained on them.\r\nAnother exercise is to flow these files from the directory\r\nstructure.\r\nReferences\r\nhttps://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification/\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-15-text-classification-with-keras-in-r/text-classification-with-keras-in-r_files/figure-html5/unnamed-chunk-17-1.png",
    "last_modified": "2022-07-16T16:48:10-04:00",
    "input_file": "text-classification-with-keras-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-14-q-learning-in-r/",
    "title": "Q-learning in R",
    "description": "In this post we will walk through the basics of Q-learning and source supporting functions.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-14",
    "categories": [
      "reinforcement learning",
      "markov decision process",
      "probability",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nQ-Learning House\r\nNavigation\r\nValue Iteration\r\n\r\nThe Problem\r\nReward Matrix\r\nEpisilon-greedy\r\n\r\nReferences\r\n\r\nQ-Learning House Navigation\r\nReinforcement learning is a relatively new field in machine learning,\r\nwith ties in control systems, decision theory, probability, and\r\nsimulation. A Markov Decision Process (MDP) is a process\r\nthat relies on previous information to make new decisions with maximum\r\nprobability of a meritorious outcome. One of the simplest algorithms to\r\ncompute this is called Value Iteration. We will be working\r\nthrough this with a simple housing example where we hope to\r\nlearn the optimal policy that will navigate an agent\r\nthrough the house. In general, these policies are often lodged in our\r\nown mental models, but once put on paper (or in a matrix), they are a\r\nlot more interesting, and sometimes calibrate even our own thinking.\r\nAfter all, the field of Deep Reinfrocement Learning has\r\nbeaten renouned championed in chess, alpha-go, and even star craft. So\r\nwhatever it is learning in those deep models can certainly augment our\r\nintelligence.\r\nThe simplicity of MCPs make them very interpretable, but at the cost\r\nof more power. So let’s take a look and see if we can cook up a function\r\nto solve the problem.\r\n\r\n\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\n\r\n\r\nValue Iteration\r\nThe idea of value iteration is that it is a recursive function with a\r\nfew well known things.\r\nS = State-space\r\nA = Action-space\r\nR = Reward function\r\nThe problem is set up as an objective. The real goal is to acquire a\r\npolicy that acts optimally. By acting optimally, this is\r\nscoped to maximizing a univariate reward function. How is this done?\r\nLet’s take a look at the math.\r\nValue Iteration \\[\r\nQ(s_1,a_1)=R(s_1,a_1) + \\gamma\\sum_{s_2 \\neq s_1 \\in S}{P(s_2 | s_1,\r\na_1) \\: max(Q(s_2, a_2) \\: \\forall a_2 \\in A)}\r\n\\]\r\nAt each iteration, a reward is known, and the future reward is also\r\ncalculated for every other state.\r\nBellman Equation \\[\r\nQ_{now}(s_1,a_1) = Q_{old}(s_1,a_1) +\\alpha \\: (r + \\gamma \\:\r\nmax_{a_2}(s_2, a_2)-Q_{old}(s_1, a_1))\r\n\\]\r\nThe Bellman Equation is also well known for solving\r\nproblems like this, and it doesn’t require the transition probability\r\nabove.\r\nThe Problem\r\nWe hope to mimic a house navigation agent. The goal is to be able to\r\nreview the optimal policy at the end that teaches the agent to travel\r\nfrom any room to any other room.\r\nHouse floorplanReward Matrix\r\nThere are three criteria,\r\n-1 if “you can’t get there from here”\r\n0 if the destination is not the target state\r\n100 if the destination is the target state\r\n\r\n\r\nR <- matrix(c(-1, -1, -1, -1, 0, 1,\r\n       -1, -1, -1, 0, -1, 0,\r\n       -1, -1, -1, 0, -1, -1, \r\n       -1, 0, 0, -1, 0, -1,\r\n        0, -1, -1, 0, -1, 0,\r\n       -1, 100, -1, -1, 100, 100), nrow=6, ncol=6, byrow=TRUE) %>% t\r\n\r\nR\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6]\r\n[1,]   -1   -1   -1   -1    0   -1\r\n[2,]   -1   -1   -1    0   -1  100\r\n[3,]   -1   -1   -1    0   -1   -1\r\n[4,]   -1    0    0   -1    0   -1\r\n[5,]    0   -1   -1    0   -1  100\r\n[6,]    1    0   -1   -1    0  100\r\n\r\n\r\n\r\ntmp <- R %>% as.data.frame()\r\ntmp <- tmp %>% dplyr::mutate(from = names(tmp))\r\ntmp <- tmp %>% tidyr::gather(., key = \"to\", \"reward\", -from)\r\ntmp <- tmp %>% dplyr::arrange(desc(reward))\r\n\r\nggplot(data = tmp) + \r\n  geom_tile(aes(x=from, y=to, fill=as.factor(reward))) + \r\n  labs(fill = \"Reward\") + theme_classic() + \r\n  ggtitle(\"Reward Matrix\")\r\n\r\n\r\n\r\n\r\n\r\nsource(\"https://raw.githubusercontent.com/NicoleRadziwill/R-Functions/master/qlearn.R\")\r\n\r\nresults <- q.learn(R,10000,alpha=0.1,gamma=0.8,tgt.state=6) \r\nround(results)\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6]\r\n[1,]    0    0    0    0   80    0\r\n[2,]    0    0    0   64    0  100\r\n[3,]    0    0    0   64    0    0\r\n[4,]    0   80   51    0   80    0\r\n[5,]   64    0    0   64    0  100\r\n[6,]   64   80    0    0   80  100\r\n\r\nThe table produced tells us the average value to obtain policies. A\r\npolicy is a path through the states. One can quickly observe going from\r\nroom 0 to room 5 which is the solution. You\r\ncan walk through these and choose the argmax which along\r\nthe columns that provides the decision for your next row of choice.\r\n\r\n\r\ntmp <- results %>% as.data.frame()\r\ntmp <- tmp %>% dplyr::mutate(from = names(tmp))\r\ntmp <- tmp %>% tidyr::gather(., key = \"to\", \"reward\", -from)\r\ntmp <- tmp %>% dplyr::arrange(desc(reward))\r\n\r\nggplot(data = tmp) + \r\n  geom_tile(aes(x=to, y=from, fill=reward)) + \r\n  labs(fill = \"Average Value (%)\") + theme_classic() + \r\n  ggtitle(\"Policy Decision Matrix\") +\r\n  scale_fill_gradient(low = \"red\", high = \"green\") +\r\n  ylab(\"State From\") + \r\n  xlab(\"Action To\")\r\n\r\n\r\n\r\nPretty cool!\r\nEpisilon-greedy\r\nOne common way of sampling an action is the\r\nepislon-greedy approach. This tells us to exploit with\r\nepsilon probability and randomly explore with 1-epsilon. Let’s take a\r\nlook.\r\n\r\n\r\nq.learn.epsilon.greedy <- function(R, epochs = 10000, alpha = 0.1,gamma = 0.8, epsilon = 0.75,tgt.state = 6, testme = T){\r\n  \r\n  Q = matrix(0, nrow = nrow(R), ncol=ncol(R))\r\n  \r\n  epsilon.greedy <- function(state, Q, epsilon = 0.75, test = F){\r\n    random_number <- runif(n = 1, min = 0, max = 1)\r\n    if(random_number < epsilon){\r\n      \r\n      if(test)\r\n        paste(\"exploit\")\r\n      else\r\n        which.max(Q[state,])\r\n    }\r\n    else{\r\n      \r\n      if(test)\r\n        paste(\"explore\")\r\n      else\r\n        sample(which(Q[state, ] != state), 1)\r\n      \r\n    }\r\n  }\r\n  \r\n  test.epsilon.greedy <- function(epsilon = 0.75){\r\n    memory <- c()\r\n    tmp <- c()\r\n    for(j in 1:100){\r\n      for(i in 1:100){\r\n      # Start at a random state\r\n      cs = sample(1:nrow(Q), 1)\r\n      ns = epsilon.greedy(cs, Q, epsilon = epsilon, test = T) \r\n      tmp <- c(tmp, ns) # we expect 75/100 to be exploits\r\n      } \r\n      memory <- c(memory, mean(tmp == \"exploit\"))\r\n    }\r\n    \r\n    # looks good.\r\n    memory %>% mean\r\n  }\r\n  \r\n  # Test it\r\n  if(testme){\r\n    \r\n    t1<-test.epsilon.greedy(epsilon=0.75)\r\n    t2<-test.epsilon.greedy(epsilon=0.5)\r\n    t3<-test.epsilon.greedy(epsilon=0.25)\r\n    print(paste(round(t1,2),\" == 0.75\"))\r\n    print(paste(round(t2,2),\" == 0.50\"))\r\n    print(paste(round(t3,2),\" == 0.25\"))\r\n  }\r\n  \r\n  \r\n  learning_rate = alpha\r\n  discount_rate = gamma\r\n  dest.state = tgt.state\r\n  \r\n  for(i in 1:epochs){\r\n    \r\n    # Start at a random state\r\n    cs = sample(1:nrow(Q), 1)\r\n    cs    \r\n  \r\n    while(TRUE){\r\n        \r\n        \r\n        # Take an action\r\n        a = epsilon.greedy(cs, Q, epsilon = 0.8, test = F)\r\n        \r\n        \r\n        # Assess the reward\r\n        r = R[cs, a]\r\n        \r\n        # Update policy matrix\r\n        Q[cs, a] = learning_rate * (Q[cs, a]) + (1 - learning_rate) * (r + discount_rate * max(Q[a, ]) )\r\n         # Q[cs,a] <- Q[cs,a] + learning_rate*(R[cs,a] + discount_rate*max(Q[a, ]) - Q[cs,a])\r\n        \r\n         # Update the current state\r\n        cs = a\r\n        \r\n        \r\n        if(a == dest.state){\r\n          break\r\n        }\r\n    }\r\n  }\r\n  Q*100/max(Q)\r\n}\r\n\r\nQ = q.learn.epsilon.greedy(R, epochs = 10000, alpha = 0.1,gamma = 0.8, epsilon = 0.75, tgt.state = 6, testme=T)\r\n\r\n[1] \"0.75  == 0.75\"\r\n[1] \"0.51  == 0.50\"\r\n[1] \"0.26  == 0.25\"\r\n\r\nidx = apply(Q, 1, which.max)\r\nI <- matrix(0, nrow=nrow(Q), ncol=ncol(Q))\r\nfor(i in 1:nrow(I)){\r\n  I[i,idx[i]] = 1\r\n}\r\nI\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6]\r\n[1,]    0    0    0    0    1    0\r\n[2,]    0    0    0    0    0    1\r\n[3,]    0    1    0    0    0    0\r\n[4,]    0    1    0    0    0    0\r\n[5,]    0    0    0    0    0    1\r\n[6,]    0    0    0    0    0    1\r\n\r\nOur resulting matrix from an epsilon greedy algorithm\r\ndoes something similar to the 100% exploit algorithm sourced in from the\r\ngithub above. The policy is as follows:\r\nIf in room 0 go to room 4, then to room 5 for the victory. (2\r\nsteps)\r\nIf in room 4 go to room 5 for the victory. (1 step)\r\nIf in room 1 go to room 5 for the victory. (1 step)\r\nIf in room 2 go to room 1, then go to room 5 for the victory. (2\r\nsteps)\r\nIf in room 3 go to room 1, then go to room 5 for the victory. (2\r\nsteps)\r\nIf in room 4, go to room 5 for the victory. (1 step)\r\nIf in room 5 .. victory! (0 steps)\r\nFor convenience, an indicator matrix is also built to illustrate the\r\nbest possibile choice to and from any state with zero ambiguity. At a\r\nglance, outside of room 5, room 1 is a favorable place to be. It seems\r\nlike it discovered a good outlet to arrive at the optimal, that is not\r\noptimal itself. Very interesting!\r\nReferences\r\nhttps://www.r-bloggers.com/2017/12/a-simple-intro-to-q-learning-in-r-floor-plan-navigation/#google_vignette\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-14-q-learning-in-r/q-learning-in-r_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-07-15T13:29:15-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-13-image-recognition-in-r/",
    "title": "Image Recognition in R",
    "description": "In this post we will explore image classification in keras for several datasets and how transfer learning can be readily applied to improve well known models.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-13",
    "categories": [
      "image recognition",
      "deep learning",
      "transfer learning",
      "keras",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nRecognize Handwritten\r\nDigits\r\nLibraries\r\nData Retrieval\r\nData Shapes\r\nVisualize Images\r\n\r\nModel\r\nSimple Dense Model\r\nSummary\r\nCompile\r\nFit\r\nVisualize\r\n\r\nAnother way\r\nSummary\r\nCompile\r\nFit\r\nVisualize\r\nPredictions\r\nEvaluate\r\nSave Model\r\nReload Model\r\n\r\n\r\nRecognize Fashion\r\nLoad the data\r\nPreprocess the data\r\nBuild the model\r\nEvaluate Accuracy\r\nMake predictions\r\nHow well did we do?\r\n\r\nRecognize Animals and\r\nObjects\r\nConvolutional Neural\r\nNetwork\r\nTransfer Learning\r\nVisualize performance\r\nSave the model\r\nEvaluate the model\r\n\r\nReferences\r\n\r\nRecognize Handwritten Digits\r\nComputer vision as a sub-field of deep learning has exploaded over\r\nthe last decade. The advent of better computers, readily available data\r\nsources, and explosively intelligent models with very little code has\r\nmade the unthinkable doable, and quickly.\r\nLibraries\r\n\r\n\r\nlibrary(keras)\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\nFirst we will grab the MNIST dataset. This consists of an array of\r\n28x28 images with 10 classification labels.\r\nData Retrieval\r\n\r\n\r\n\r\n\r\n\r\nmnist %>% names\r\n\r\n[1] \"train\" \"test\" \r\n\r\nWe can save the shapes and number of classes for later.\r\nData Shapes\r\n\r\n\r\n# Get the width and height\r\nWIDTH = dim(mnist$train$x)[[2]]\r\nHEIGHT = dim(mnist$train$x)[[3]]\r\n\r\n\r\n# Get unique number of classes\r\nCLASSES = length(unique(mnist$train$y))\r\n\r\nmnist$train$x %>% dim\r\n\r\n[1] 60000    28    28\r\n\r\nmnist$train$y %>% dim\r\n\r\n[1] 60000\r\n\r\nmnist$test$x %>% dim\r\n\r\n[1] 10000    28    28\r\n\r\nmnist$test$y %>% dim\r\n\r\n[1] 10000\r\n\r\nVisualize Images\r\nNext we can visualize a few images using the plot\r\nfunction in r. This was a little weird at first, because the images\r\nsometimes need standardized for rgb values depending on the function and\r\ndata shape.\r\n\r\n\r\nlibrary(raster)\r\nplot_a_few <- function(x,y, a_few = 3, rgb_dim=FALSE){\r\n    # Render a few images\r\n    rand_image_index = sample(1:dim(x)[[1]], size = a_few)\r\n    par(mar=c(0, 0, 0, 0))\r\n    for(i in rand_image_index){\r\n      if(rgb_dim){\r\n        img = x[i,,,]\r\n      }\r\n      else{\r\n        img = x[i,,]\r\n        # image(img, useRaster=TRUE, axes=FALSE)\r\n      }\r\n      \r\n      plot(as.raster(img))\r\n      label = y[i]\r\n      print(label)\r\n    }\r\n}\r\n\r\nplot_a_few(mnist$train$x, mnist$train$y, a_few=3)\r\n\r\n\r\n[1] 3\r\n\r\n[1] 2\r\n\r\n[1] 5\r\n\r\nModel\r\nSimple Dense Model\r\nThe simplest model will take the image tensor and flatten it into the\r\nstandard feed forward format. The prediction is over our\r\nCLASSES which is 10.\r\n\r\n\r\n# Simple model\r\nmodel <- keras::keras_model_sequential() %>% \r\n            keras::layer_flatten(input_shape = c(WIDTH, HEIGHT), \r\n                                 name = \"mnist_flatten_input\") %>% \r\n            keras::layer_dense(units = 128, activation = \"relu\", \r\n                               name = \"mnist_dense\") %>% \r\n            keras::layer_dropout(0.2, name = \"mnist_dropout\") %>% \r\n            keras::layer_dense(CLASSES, activation = \"softmax\", \r\n                               name = \"mnist_dense_output\")\r\nmodel\r\n\r\nModel: \"sequential\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nmnist_flatten_input (Flatten)  (None, 784)                 0          \r\n______________________________________________________________________\r\nmnist_dense (Dense)            (None, 128)                 100480     \r\n______________________________________________________________________\r\nmnist_dropout (Dropout)        (None, 128)                 0          \r\n______________________________________________________________________\r\nmnist_dense_output (Dense)     (None, 10)                  1290       \r\n======================================================================\r\nTotal params: 101,770\r\nTrainable params: 101,770\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nSummary\r\n\r\n\r\n# Some summary statistics\r\nbase::summary(model)\r\n\r\nModel: \"sequential\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nmnist_flatten_input (Flatten)  (None, 784)                 0          \r\n______________________________________________________________________\r\nmnist_dense (Dense)            (None, 128)                 100480     \r\n______________________________________________________________________\r\nmnist_dropout (Dropout)        (None, 128)                 0          \r\n______________________________________________________________________\r\nmnist_dense_output (Dense)     (None, 10)                  1290       \r\n======================================================================\r\nTotal params: 101,770\r\nTrainable params: 101,770\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nCompile\r\nReminder that sparse_categorical_crossentropy is for\r\nnon-matrix like y values. This will do it for you.\r\nOtherwise, you need to use the to_categorical function to\r\ntransform the y vector into a matrix.\r\n\r\n\r\n# Compile the model\r\nmodel %>% \r\n  keras::compile(\r\n    loss = \"sparse_categorical_crossentropy\",\r\n    optimizer = \"adam\",\r\n    metrics = \"accuracy\"\r\n  )\r\n\r\n\r\nFit\r\n\r\n\r\n# Fit the model\r\nhistory = model %>% \r\n            keras::fit(\r\n              x = mnist$train$x, y = mnist$train$y,\r\n              epochs = 5,\r\n              validation_split = 0.3,\r\n              verbose = 2\r\n            )\r\n\r\n\r\nVisualize\r\n\r\n\r\nplot_history_metrics = function(history){\r\n    # Plot fit results - loss and accuracy for this model\r\n    tmp = data.frame(history$metrics) %>% dplyr::mutate(epoch = row_number())\r\n    plt1 = ggplot(data=tmp) +\r\n            geom_line(aes(x=epoch, y = loss, color=\"training loss\")) +\r\n            geom_line(aes(x=epoch, y = val_loss, color=\"validation loss\")) +\r\n            theme_bw() +\r\n            labs(color=\"Legend\") + \r\n            ggtitle(\"Model Loss\")\r\n    plt1\r\n    \r\n    \r\n    plt2 = ggplot(data=tmp) +\r\n            geom_line(aes(x=epoch, y = accuracy, color=\"training accuracy\")) +\r\n            geom_line(aes(x=epoch, y = val_accuracy, color=\"validation accuracy\")) +\r\n            theme_bw() +\r\n            labs(color=\"Legend\") + \r\n            ggtitle(\"Model Accuracy\")\r\n    plt2\r\n    \r\n    list(loss_plot = plt1, acc_plot = plt2)\r\n}\r\n\r\nplot_history_metrics(history)\r\n\r\n$loss_plot\r\n\r\n\r\n$acc_plot\r\n\r\n\r\nAnother way\r\nAnother equally valid way as oppose to flattening the input as an\r\narray is to do it explicitely on the outside. This can be done use the\r\narray_reshape function. We can also make our y values into\r\na categorical matrix using the to_Categorical function.\r\nThis will change our sparse_categorical_crossentropy into\r\ncategorical_crossentropy. A tricky distinction, but one\r\ndoesn’t expect a matrix, one does.\r\n\r\n\r\nx_train <- keras::array_reshape(mnist$train$x, c(nrow(mnist$train$x), WIDTH*HEIGHT))\r\nx_test <- keras::array_reshape(mnist$test$x, c(nrow(mnist$test$x), WIDTH*HEIGHT))\r\ny_train <- keras::to_categorical(mnist$train$y, 10)\r\ny_test <- keras::to_categorical(mnist$test$y, 10)\r\n\r\nx_test %>% dim\r\n\r\n[1] 10000   784\r\n\r\ny_test %>% head\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\r\n[1,]    0    0    0    0    0    0    0    1    0     0\r\n[2,]    0    0    1    0    0    0    0    0    0     0\r\n[3,]    0    1    0    0    0    0    0    0    0     0\r\n[4,]    1    0    0    0    0    0    0    0    0     0\r\n[5,]    0    0    0    0    1    0    0    0    0     0\r\n[6,]    0    1    0    0    0    0    0    0    0     0\r\n\r\n\r\n\r\n# Model pre-flattened for shape and made categorically long in y\r\nmodel <- keras::keras_model_sequential() %>%  \r\n            keras::layer_dense(input_shape = c(WIDTH*HEIGHT), \r\n                               units = 128, \r\n                               activation = \"relu\", \r\n                               name = \"mnist_dense\") %>% \r\n            keras::layer_dropout(0.2, \r\n                                 name = \"mnist_dropout\") %>% \r\n            keras::layer_dense(CLASSES, \r\n                               activation = \"softmax\", \r\n                               name = \"mnist_dense_output\")\r\nmodel\r\n\r\nModel: \"sequential_1\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nmnist_dense (Dense)            (None, 128)                 100480     \r\n______________________________________________________________________\r\nmnist_dropout (Dropout)        (None, 128)                 0          \r\n______________________________________________________________________\r\nmnist_dense_output (Dense)     (None, 10)                  1290       \r\n======================================================================\r\nTotal params: 101,770\r\nTrainable params: 101,770\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nSummary\r\n\r\n\r\n# Model architectures\r\nbase::summary(model)\r\n\r\nModel: \"sequential_1\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nmnist_dense (Dense)            (None, 128)                 100480     \r\n______________________________________________________________________\r\nmnist_dropout (Dropout)        (None, 128)                 0          \r\n______________________________________________________________________\r\nmnist_dense_output (Dense)     (None, 10)                  1290       \r\n======================================================================\r\nTotal params: 101,770\r\nTrainable params: 101,770\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nCompile\r\n\r\n\r\n# Compile the model\r\nmodel %>% \r\n  keras::compile(\r\n    # loss = \"sparse_categorical_crossentropy\",\r\n    loss = \"categorical_crossentropy\",\r\n    optimizer = \"adam\",\r\n    metrics = \"accuracy\"\r\n  )\r\n\r\n\r\nFit\r\nOnce we configure our model, we can compile it,\r\nfit, then plot to see the performance. Turns\r\nout, you can just do plot(history) and the function to plot\r\nthese metrics is entirely superfluous.\r\n\r\n\r\nhistory = model %>% keras::fit(\r\n  x = x_train, y = y_train,\r\n  validation_split = 0.3,\r\n  epochs = 5,\r\n  verbose = 2\r\n)\r\n\r\n\r\nVisualize\r\n\r\n\r\nplot_history_metrics = function(history){\r\n    # Plot fit results - loss and accuracy for this model\r\n    tmp = data.frame(history$metrics) %>% dplyr::mutate(epoch = row_number())\r\n    plt1 = ggplot(data=tmp) +\r\n            geom_line(aes(x=epoch, y = loss, color=\"training loss\")) +\r\n            geom_line(aes(x=epoch, y = val_loss, color=\"validation loss\")) +\r\n            theme_bw() +\r\n            labs(color=\"Legend\") + \r\n            ggtitle(\"Model Loss\")\r\n    plt1\r\n    \r\n    \r\n    plt2 = ggplot(data=tmp) +\r\n            geom_line(aes(x=epoch, y = accuracy, color=\"training accuracy\")) +\r\n            geom_line(aes(x=epoch, y = val_accuracy, color=\"validation accuracy\")) +\r\n            theme_bw() +\r\n            labs(color=\"Legend\") + \r\n            ggtitle(\"Model Accuracy\")\r\n    plt2\r\n    \r\n    list(loss_plot = plt1, acc_plot = plt2)\r\n}\r\n\r\nplot_history_metrics(history)\r\n\r\n$loss_plot\r\n\r\n\r\n$acc_plot\r\n\r\n\r\nPredictions\r\n\r\n\r\n# Generate some predictions on the unseen data\r\npredictions = stats::predict(model, x_test)\r\npredictions %>% head()\r\n\r\n             [,1]         [,2]         [,3]         [,4]         [,5]\r\n[1,] 2.147511e-06 3.081970e-08 1.363529e-05 6.959682e-04 1.883129e-09\r\n[2,] 4.554735e-07 1.738241e-05 9.998441e-01 5.303881e-05 3.181212e-13\r\n[3,] 3.884773e-06 9.974592e-01 1.834323e-04 8.748658e-05 1.106339e-04\r\n[4,] 9.970744e-01 2.236041e-06 1.048536e-04 6.934661e-05 1.657578e-04\r\n[5,] 2.274211e-06 8.666921e-09 7.863012e-06 9.704200e-08 9.973132e-01\r\n[6,] 3.012697e-07 9.972008e-01 3.240131e-06 6.145242e-06 1.901888e-05\r\n             [,6]         [,7]         [,8]         [,9]        [,10]\r\n[1,] 1.727558e-06 8.484718e-11 9.990858e-01 1.901059e-06 1.988278e-04\r\n[2,] 5.569929e-05 2.090738e-07 1.970074e-10 2.905491e-05 5.533708e-11\r\n[3,] 6.059188e-06 1.315140e-04 1.861338e-03 1.532390e-04 3.115008e-06\r\n[4,] 6.942511e-06 1.050111e-03 8.822395e-04 5.042305e-06 6.392266e-04\r\n[5,] 1.149999e-05 3.039010e-06 5.597093e-05 1.561662e-06 2.604362e-03\r\n[6,] 3.678756e-08 1.334996e-06 2.748028e-03 1.928041e-05 1.795054e-06\r\n\r\nEvaluate\r\n\r\n\r\n# Evaluate performance\r\n# test_results = model %>% \r\n#                   evaluate(mnist$test$x, mnist$test$y, verbose = 0)\r\n# test_results\r\n\r\n\r\nSave Model\r\nOne thing keras makes incredibly easy is the ability to save your\r\nmodel. This will create a folder and allow for easy access to and from\r\nyour model if you need it for predictions in another environment or\r\nAPI.\r\n\r\n\r\n# Serialize the model (it becomes a folder)\r\n# keras::save_model_tf(object = model, filepath = \"mnist_model\")\r\n\r\n\r\nReload Model\r\n\r\n\r\n# Reload the model\r\n# reloaded_model = keras::load_model_tf(\"mnist_model\")\r\n# reloaded_model %>% summary\r\n# base::all.equal(stats::predict(model, x_test), \r\n#                 stats::predict(reloaded_model, x_test))\r\n\r\n\r\nRecognize Fashion\r\nRecognizing other types of objects is just as easy as before. Let’s\r\nrepeat our steps for a new dataset, because practice makes\r\nperfect!\r\nLoad the data\r\n\r\n\r\nfashion_mnist <- dataset_fashion_mnist()\r\n\r\nc(train_images, train_labels) %<-% fashion_mnist$train\r\nc(test_images, test_labels) %<-% fashion_mnist$test\r\n\r\n\r\n\r\n\r\nclass_names = c('T-shirt/top',\r\n                'Trouser',\r\n                'Pullover',\r\n                'Dress',\r\n                'Coat', \r\n                'Sandal',\r\n                'Shirt',\r\n                'Sneaker',\r\n                'Bag',\r\n                'Ankle boot')\r\n\r\n\r\n\r\n\r\ndim(train_images)\r\n\r\n[1] 60000    28    28\r\n\r\ndim(train_labels)\r\n\r\n[1] 60000\r\n\r\ntrain_labels[1:20]\r\n\r\n [1] 9 0 0 3 0 2 7 2 5 5 0 9 5 5 7 9 1 0 6 4\r\n\r\ndim(test_images)\r\n\r\n[1] 10000    28    28\r\n\r\ndim(test_labels)\r\n\r\n[1] 10000\r\n\r\nPreprocess the data\r\n\r\n\r\nlibrary(tidyr)\r\nlibrary(ggplot2)\r\n\r\n\r\nimage_1 <- as.data.frame(train_images[1,,])\r\ncolnames(image_1) <- seq_len(ncol(image_1))\r\nimage_1$y <- seq_len(nrow(image_1))\r\nimage_1 <- tidyr::gather(image_1, key = \"x\", value = \"value\", -y)\r\nimage_1$x <- as.integer(image_1$x)\r\n\r\nggplot(image_1, aes(x=x,y=y,fill=value)) +\r\n  geom_tile() +\r\n  scale_fill_gradient(low = \"white\", high = \"black\", na.value = NA) +\r\n  scale_y_reverse() +\r\n  theme_minimal() +\r\n  theme(panel.grid = element_blank()) +\r\n  theme(aspect.ratio = 1) +\r\n  xlab(\"\") +\r\n  ylab(\"\") +\r\n  ggtitle(paste(class_names[train_labels[1]+1]))\r\n\r\n\r\n\r\n\r\n\r\ntrain_images <- train_images / 255\r\ntest_images <- test_images / 255\r\n\r\npar(mfcol = c(5,5))\r\npar(mar=c(0,0,1.5,0), axs='i', yaxs='i')\r\nfor(i in 1:25){\r\n  img <- train_images[i,,]\r\n  # img <- t(apply(img, 2, rev))\r\n  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n', main = paste(class_names[train_labels[i]+1]))\r\n}\r\n\r\n\r\n\r\nBuild the model\r\n\r\n\r\nmodel <- keras_model_sequential() %>%\r\n  layer_flatten(input_shape = c(28, 28)) %>% \r\n  layer_dense(units = 128, activation = 'relu') %>% \r\n  layer_dense(units = 10, activation = 'softmax')\r\n\r\nmodel %>% compile(\r\n  optimizer = 'adam', \r\n  loss = 'sparse_categorical_crossentropy',\r\n  metrics = c('accuracy')\r\n)\r\n\r\nmodel %>% \r\n  fit(x=train_images, y=train_labels, \r\n      epochs = 5, verbose = 2, validation_split=0.3)\r\n\r\n\r\nEvaluate Accuracy\r\n\r\n\r\nscore <- model %>% evaluate(test_images, test_labels, verbose = 0)\r\n\r\ncat('Test loss:', score[1], \"\\n\")\r\n\r\nTest loss: 0.3637977 \r\n\r\ncat('Test accuracy:', score[2], \"\\n\")\r\n\r\nTest accuracy: 0.8708 \r\n\r\nMake predictions\r\n\r\n\r\npredictions <- model %>% predict(test_images)\r\npredictions %>% head\r\n\r\n             [,1]         [,2]         [,3]         [,4]         [,5]\r\n[1,] 5.944940e-06 3.568362e-09 9.062223e-07 3.813431e-07 7.632132e-07\r\n[2,] 6.902370e-05 6.028386e-09 9.972779e-01 5.225142e-08 3.154729e-04\r\n[3,] 5.386319e-06 9.999939e-01 1.031454e-08 3.366561e-07 2.402087e-07\r\n[4,] 9.895704e-06 9.999171e-01 3.511240e-07 6.675994e-05 4.046826e-06\r\n[5,] 1.955759e-01 2.231707e-04 2.183121e-01 8.242820e-03 1.920183e-02\r\n[6,] 1.318200e-03 9.986019e-01 8.571616e-06 8.032462e-06 3.114295e-05\r\n             [,6]         [,7]         [,8]         [,9]        [,10]\r\n[1,] 1.908284e-02 1.337585e-05 5.273721e-02 1.333138e-04 9.280253e-01\r\n[2,] 3.593275e-11 2.337541e-03 3.329948e-12 3.700688e-08 2.485134e-13\r\n[3,] 1.300379e-13 1.142511e-07 2.336990e-14 3.351031e-08 7.711556e-12\r\n[4,] 4.459056e-11 1.710582e-06 8.355450e-12 1.272454e-07 1.614447e-09\r\n[5,] 5.498093e-05 5.563514e-01 2.239543e-05 2.010975e-03 4.317736e-06\r\n[6,] 2.356733e-10 3.082445e-05 1.095561e-10 1.486991e-06 3.072765e-09\r\n\r\npreds = apply(predictions, 1, which.max)\r\npreds %>% head\r\n\r\n[1] 10  3  2  2  7  2\r\n\r\n#or\r\n\r\npreds = model %>% predict_classes(x = test_images)\r\npreds %>% unique\r\n\r\n [1] 9 2 1 6 4 5 7 3 8 0\r\n\r\nHow well did we do?\r\n\r\n\r\npar(mfcol=c(5,5))\r\npar(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')\r\nfor (i in 1:25) { \r\n  img <- test_images[i, , ]\r\n  img <- t(apply(img, 2, rev)) \r\n  # subtract 1 as labels go from 0 to 9\r\n  predicted_label <- which.max(predictions[i, ]) - 1\r\n  true_label <- test_labels[i]\r\n  if (predicted_label == true_label) {\r\n    color <- '#008800' \r\n  } else {\r\n    color <- '#bb0000'\r\n  }\r\n  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',\r\n        main = paste0(class_names[predicted_label + 1], \" (\",\r\n                      class_names[true_label + 1], \")\"),\r\n        col.main = color)\r\n}\r\n\r\n\r\n\r\n\r\n\r\ntrain_images %>% dim\r\n\r\n[1] 60000    28    28\r\n\r\nRecognize Animals and\r\nObjects\r\n\r\n\r\nlibrary(tensorflow)\r\nlibrary(keras)\r\n\r\ncifar <- dataset_cifar10()\r\n\r\nclass_names <- c('airplane', 'automobile', 'bird', 'cat', 'deer',\r\n               'dog', 'frog', 'horse', 'ship', 'truck')\r\n\r\nindex <- 1:30\r\n\r\npar(mfcol = c(5,6), mar = rep(1,4), oma=rep(0.2, 4))\r\ncifar$train$x[index,,,] %>% \r\n  purrr::array_tree(margin=1) %>% \r\n  purrr::set_names(class_names[cifar$train$y[index] + 1]) %>% \r\n  purrr::map(as.raster, max = 255) %>% \r\n  purrr::iwalk(~{plot(.x); title(.y)})\r\n\r\n\r\n\r\nConvolutional Neural Network\r\n\r\n\r\nmodel <- keras_model_sequential() %>% \r\n  layer_conv_2d(input_shape = c(32, 32, 3),  filters = 32, kernel_size = c(3,3), activation = \"relu\") %>% \r\n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \r\n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = \"relu\") %>% \r\n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \r\n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = \"relu\") %>% \r\n  layer_flatten() %>% \r\n  layer_dense(units = 64, activation = \"relu\") %>% \r\n  layer_dense(units = 10, activation = \"softmax\")\r\n\r\nsummary(model)\r\n\r\nModel: \"sequential_3\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nconv2d_2 (Conv2D)              (None, 30, 30, 32)          896        \r\n______________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2D) (None, 15, 15, 32)          0          \r\n______________________________________________________________________\r\nconv2d_1 (Conv2D)              (None, 13, 13, 64)          18496      \r\n______________________________________________________________________\r\nmax_pooling2d (MaxPooling2D)   (None, 6, 6, 64)            0          \r\n______________________________________________________________________\r\nconv2d (Conv2D)                (None, 4, 4, 64)            36928      \r\n______________________________________________________________________\r\nflatten_1 (Flatten)            (None, 1024)                0          \r\n______________________________________________________________________\r\ndense_3 (Dense)                (None, 64)                  65600      \r\n______________________________________________________________________\r\ndense_2 (Dense)                (None, 10)                  650        \r\n======================================================================\r\nTotal params: 122,570\r\nTrainable params: 122,570\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nmodel %>% compile(\r\n  optimizer = \"adam\",\r\n  loss = \"sparse_categorical_crossentropy\",\r\n  metrics = \"accuracy\"\r\n)\r\n\r\n\r\nhistory <- model %>% \r\n  fit(\r\n    x = cifar$train$x, y = cifar$train$y,\r\n    epochs = 10,\r\n    validation_data = unname(cifar$test),\r\n    verbose = 2\r\n  )\r\n\r\n\r\n\r\n\r\nevaluate(model, cifar$test$x, cifar$test$y, verbose = 0)\r\n\r\n    loss accuracy \r\n1.069247 0.656700 \r\n\r\nTransfer Learning\r\nWe will not actually train here, because it takes about 45 minutes.\r\nBut we will just unload the model previously trained. But the idea is to\r\ntake a layer or many layers from a previouos model and then stack our\r\nmodel on top of it. You don’t have to stack it “on top”, but I chose to\r\nhere for simplicity. By taking what the previous model learned, we then\r\nput our custom output layers there so it can learn to classify new\r\nthings, with old feature vectors it learned from the\r\nimagenet data set.\r\n\r\n\r\nlibrary(devtools)\r\nlibrary(tfhub)\r\nlibrary(keras)\r\nlibrary(reticulate)\r\n\r\nc(train_images, train_labels) %<-% cifar$train\r\nc(test_images, test_labels) %<-% cifar$test\r\n\r\ntrain_images %>% dim\r\n\r\n[1] 50000    32    32     3\r\n\r\ntrain_labels %>% dim\r\n\r\n[1] 50000     1\r\n\r\ntest_images %>% dim\r\n\r\n[1] 10000    32    32     3\r\n\r\ntest_labels %>% dim  \r\n\r\n[1] 10000     1\r\n\r\nimage_shape <- c(32,32,3)\r\n\r\nconv_base <- keras::application_resnet101(weights = \"imagenet\",\r\n                                          include_top = FALSE, \r\n                                          input_shape = c(32,32,3))\r\n\r\n\r\nfreeze_weights(conv_base)\r\n\r\nmodel <- keras_model_sequential() %>%\r\n  conv_base %>%\r\n  layer_flatten() %>%\r\n  # layer_reshape(c(1,2048)) %>% \r\n  layer_dense(units = 256, activation = \"relu\") %>%\r\n  layer_dense(units = 10, activation = \"softmax\")\r\n\r\n\r\nmodel %>% compile(\r\n  optimizer = \"adam\",\r\n  loss = \"sparse_categorical_crossentropy\",\r\n  metrics = \"accuracy\"\r\n)\r\n\r\n\r\n# unfreeze_weights(conv_base, from = \"block5_conv1\")\r\n\r\nhistory <- model %>% fit(\r\n  x=train_images, y=train_labels,\r\n  validation_split = 0.3,\r\n  epochs=3,\r\n  verbose = 2\r\n)\r\n\r\n# model = keras::load_model_tf(\"cifar10_tl_model\")\r\n# model %>% summary\r\n\r\n# summary(model)\r\n# train_images[1,,,] %>% dim\r\n# train_labels[1]\r\n\r\n\r\nVisualize performance\r\n\r\n\r\n# plot(history)\r\n\r\n\r\nSave the model\r\nThe following code is used to serialize the model, since this is\r\nalready done and the process is fairly intensive, we will not be\r\nrepeating it here.\r\n\r\n\r\n# # Serialize the model (it becomes a folder)\r\n# keras::save_model_tf(object = model, filepath = \"cifar10_tl_model\")\r\n# \r\n# # Reload the model\r\n# reloaded_model = keras::load_model_tf(\"cifar10_tl_model\")\r\n# reloaded_model %>% summary\r\n\r\n\r\nEvaluate the model\r\n\r\n\r\nevaluate(model, x = test_images, y = test_labels)\r\n\r\n    loss accuracy \r\n1.188451 0.592300 \r\n\r\nReferences\r\nhttps://tensorflow.rstudio.com/tutorials/beginners/\r\nhttps://tensorflow.rstudio.com/tutorials/advanced/images/cnn/\r\nhttps://tensorflow.rstudio.com/tutorials/advanced/images/transfer-learning-hub/\r\nhttps://keras.rstudio.com/reference/freeze_layers.html\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-13-image-recognition-in-r/image-recognition-in-r_files/figure-html5/visualize-1.png",
    "last_modified": "2022-07-16T16:41:41-04:00",
    "input_file": "image-recognition-in-r.knit.md"
  },
  {
    "path": "posts/2022-07-13-logistic-regression-and-gradient-descent-in-python/",
    "title": "Logistic Regression and Gradient Descent in Python",
    "description": "In this post we will walk through how to train a Logistic Regression model from scratch using Gradient Descent in Python.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-13",
    "categories": [
      "machine learning",
      "non-linear programming",
      "python"
    ],
    "contents": "\r\n\r\nContents\r\nOverview\r\nSigmoid\r\nNegative Log Likelihood\r\nGradient Log Likelihood\r\n\r\nGradient Descent\r\nValidation\r\nTest Data\r\nOptimal Decision\r\nBoundary\r\n\r\n\r\nOverview\r\nOne of the most simple machine learning problems is that of\r\nbinary classification. Further, one of the most simple\r\nnon-linear models is logistic regression. In short, this\r\nmodel takes a set of parameters and seeks a linear combination mapped to\r\na non-linear sigmoid function which maximizes the\r\nlikelihood of fitting the data. The math for this is\r\nbelow,\r\nSigmoid Function\r\n\\[\r\n\\sigma(z)=\\frac{1}{1+exp(-z)}\r\n\\] Maximum Likelihood Estimation\r\n\\[\r\nL(x,y; \\theta)=\\frac{1}{n} \\prod_{i=1}^n{\\sigma(\\theta^Tx)^{y_i} +\r\n(1-\\sigma(\\theta^Tx))^{1-y_i}}\r\n\\] Negative Log Likelihood or Cross Entropy\r\n\\[\r\nl(x,y; \\theta)=-\\frac{1}{n}\\sum_{i=1}^n{{y_i}log(\\sigma(\\theta^Tx)) +\r\n(1-y_i)log(1-\\sigma(\\theta^Tx))}\r\n\\] Gradient of Negative Log Likelihood\r\n\\[\r\n\\nabla_\\theta l(x,y) =\r\n\\frac{1}{n}\\sum_{i=1}^n{x_i(\\sigma(\\theta^Tx)-y_i)}\r\n\\]\r\nMinimizing Negative Log Likelihood\r\n\\[\r\n\\theta^*=argmin(l(x,y; \\theta) : \\: \\theta \\in \\Re^{d+1} )\r\n\\] ## The Data\r\nWe will start with some libraries and the simple data set we will be\r\nworking with for binary classification.\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nX,y = np.concatenate((np.random.randn(5000, 2), np.ones(5000).reshape(-1,1)), 1), np.random.binomial(1, 0.25, size=5000)\r\nN, D = X.shape\r\nX.shape, y.shape\r\n((5000, 3), (5000,))\r\n\r\nSigmoid\r\nNext, we will look at the sigmoid curve.\r\n\r\ndef sigmoid(z):\r\n    return 1/(1 + np.exp(-z))\r\ntheta_init = np.random.randn(3)\r\ntheta_init\r\narray([-0.43382384,  0.00477304, -0.42501194])\r\nlogits = sigmoid(X.dot(theta_init))\r\nlogits\r\narray([0.40473019, 0.51544891, 0.53547371, ..., 0.33712169, 0.40979105,\r\n       0.28786013])\r\ndef plot_logits(logits, title = \"logits=sigmoid(X.dot(theta))\"):\r\n    plt.scatter(range(len(logits)), sorted(logits), label = \"logits\")\r\n    plt.legend()\r\n    plt.title(title)\r\n    plt.show()\r\n    \r\nplt.clf()\r\nplot_logits(logits)\r\n\r\n\r\nNegative Log Likelihood\r\nNext, the log likelihood.\r\n\r\ndef negative_log_likelihood(logits, y):\r\n    errors = y * np.log(logits) + (1-y)*np.log(1-logits)\r\n    N = len(errors)\r\n    return -(1/N) * np.sum(errors) \r\n\r\nloss = negative_log_likelihood(logits, y)\r\nloss\r\n0.634010228209429\r\n\r\nFinally, the gradient of the log likelihood. Notice that the gradient\r\nis always a vector.\r\nGradient Log Likelihood\r\n\r\ndef gradient_log_likelihood(X, logits, y):\r\n    N, D = X.shape\r\n    grad_vec = (1/N)*X.T.dot(logits-y)\r\n    return grad_vec\r\n\r\ngradient_log_likelihood(X, logits, y)\r\narray([-0.09878357,  0.00720683,  0.14126207])\r\n\r\nGradient Descent\r\nNow we want to optimize. so we will build a gradient descent function\r\nto loop through our training set and converge on a solution. We will\r\nalso take a moment to visualize the logits.\r\n\r\n\r\ndef gradient_descent(f, grad_f, eps=0.01, eta=0.1, max_iter = 1000, **kwargs):\r\n  theta_init = np.random.randn(D)\r\n  thetas = [theta_init]\r\n  losses = []\r\n  for t in range(1,max_iter):\r\n      logits = kwargs[\"h\"](kwargs[\"X\"].dot(thetas[-1]))\r\n      \r\n      loss = f(logits, kwargs[\"y\"])\r\n      losses.append(loss)\r\n      \r\n      \r\n      #if t % 50 == 0:\r\n      #    print(\"Loss {}\".format(losses[-1]))\r\n          #plot_logits(logits)\r\n          #input(\"...\")\r\n      \r\n      grad_vec = grad_f(kwargs[\"X\"], logits, kwargs[\"y\"])\r\n      theta_t = thetas[t-1] - eta * grad_vec\r\n      thetas.append(theta_t)\r\n      \r\n      if np.sqrt(np.sum(np.square(thetas[-2] - thetas[-1]))) <= eps:\r\n        return thetas[-1], losses\r\n        break\r\n\r\n  return None\r\n  \r\n        \r\nfinal_theta, losses = gradient_descent(negative_log_likelihood, \r\n                                       gradient_log_likelihood, \r\n                                       eps = 0.001, \r\n                                       eta=0.01, \r\n                                       max_iter = 100000, \r\n                                       X=X, \r\n                                       h=sigmoid, \r\n                                       y=y)\r\nlogits = sigmoid(X.dot(final_theta))\r\nplt.clf()\r\nplot_logits(logits)\r\n\r\nplt.clf()\r\nplt.plot(losses)\r\nplt.title(\"Training losses\")\r\nplt.show()\r\n\r\n\r\nValidation\r\nThe initial training accuracy with null parameters is below. 49% with\r\na random guess on theta at the start.\r\n\r\ndef predict(logits, threshold = 0.5):\r\n    return (logits > threshold).astype(int)\r\n\r\nlogits = sigmoid(X.dot(theta_init))\r\ny_pred = predict(logits, threshold = 0.5)\r\n\r\nprint(\"Accuracy: {}\".format(np.mean(y_pred == y)))\r\nAccuracy: 0.666\r\n\r\nAfter some training, our final theta parameters now get 73%. Not too\r\nshabby!\r\n\r\ndef predict(logits, threshold = 0.5):\r\n    return (logits > threshold).astype(int)\r\n\r\nlogits = sigmoid(X.dot(final_theta))\r\ny_pred = predict(logits, threshold = 0.5)\r\n\r\nprint(\"Accuracy: {}\".format(np.mean(y_pred == y)))\r\nAccuracy: 0.742\r\n\r\nTest Data\r\nLet’s try this on some test data. We will just generate some more and\r\nsee how we do! It looks like we get about the same percentage on new\r\ndata, so that is a good indicator. We would expect this though, since\r\nthey are sampled directly from the signal population. One good study\r\nmight be to examine which data points are incorrect and where they fall\r\non the logistic curve. Were they really low values or really high?\r\nPerhaps they were right on the decision boundary and just couldn’t\r\ndecide. All these and others are good questions for a deep dive into\r\nmodel interpretation, which we will not get into now.\r\n\r\nXtest,ytest = np.concatenate((np.random.randn(5000, 2), np.ones(5000).reshape(-1,1)), 1), np.random.binomial(1, 0.25, size=5000)\r\n\r\nlogits = sigmoid(Xtest.dot(final_theta))\r\ny_pred = predict(logits, threshold = 0.5)\r\n\r\nprint(\"Test Accuracy: {}\".format(np.mean(y_pred == ytest)))\r\nTest Accuracy: 0.7614\r\n\r\nOptimal Decision Boundary\r\nRight now the decision boundary is 0.5 which is\r\ncustomary in the machine learning and statistical community. We will\r\nexplore what cutoff might be optimal by doing a\r\nrandom search through a bunch of possibilities from\r\n0 to 1. The decision boundary that eeked out some extra\r\nperformance for us is at around 0.68, or for the other\r\nengineers in the room, about 0.7. This tells us that there\r\nis about a 70% likelihood of predicting a 0\r\nclass label and about 30% likelihood of predicting a\r\n1. This is very interesting because our original data was\r\nsampled from a binomial distribution with a probability of success as\r\n25%. So the model, without being told, trained on a random\r\nparameter vector, after applying gradient descent, and through a bit of\r\nsearch optimization, was able to arrive at an estimate on the\r\npopulation’s yes prediction likelihood. Optimization is\r\npretty amazing!\r\n\r\ndef optimize_decision_boundary(logits, y, plot = False):\r\n    results = []\r\n    thresholds = []\r\n    for thresh in np.linspace(0, 1, 20):\r\n        y_pred = predict(logits, threshold = thresh)\r\n        results.append(np.mean(y_pred == y))\r\n        thresholds.append(thresh)\r\n    thresh_star = np.argmax(results)\r\n    \r\n    if plot:\r\n        plt.plot(results)\r\n        plt.suptitle(\"Decision Boundary Threshold Discovery\")\r\n        plt.title(\"Best Accuracy {} at index {} with value {}\".format(results[thresh_star], thresh_star, np.round(thresholds[thresh_star], 4)))\r\n        plt.axvline(x = thresh_star, c='r')\r\n        plt.axhline(y = results[thresh_star], c='r')\r\n        plt.show()\r\n        \r\n    return thresh_star, thresholds[thresh_star]\r\n\r\nplt.clf()\r\nthreshold = optimize_decision_boundary(logits, y, plot=True)\r\n\r\nthreshold\r\n(9, 0.47368421052631576)\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-13-logistic-regression-and-gradient-descent-in-python/logistic-regression-and-gradient-descent-in-python_files/figure-html5/unnamed-chunk-5-3.png",
    "last_modified": "2022-07-13T07:10:48-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-11-transportation-problem-in-r/",
    "title": "Transportation Problem in R",
    "description": "In this post we will learn how to solve supply and demand problems with the transportation problem and linear programming.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-12",
    "categories": [
      "network flows",
      "linear programming",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nMetropolitan Power Plant\r\nSupply\r\nThe Problem\r\nDescription\r\nMathematical\r\nFormulation\r\nData\r\nA Quick Visual\r\n\r\n\r\nLP Model Formulation\r\nObjective\r\nConstraints, Direction, and\r\nRHS\r\nSolution\r\nVisualization\r\n\r\n\r\nAn Easier Way\r\n\r\nMetropolitan Power Plant\r\nSupply\r\nThe Problem\r\nDescription\r\nOne of the most common daily tasks is that of transportation. From\r\nour bed to the dining room table, or from home to work and back, all of\r\nthese examples have limited supply some pre-determined\r\ndemand and an implicit cost in order\r\nto carry it out. The question is, what is the best way to do it? It\r\nwould be silly to travel to go home before you pick up dinner, then go\r\nback out again to do it; we seek to minimize our cost\r\nwhen transporting goods. This can be time, money, risk, or anything else\r\nthat matters.\r\nIn our example we will consider a power plant that needs to supply\r\nelectricity (or any resource for that matter) to a city. All of the\r\ncosts incurred to transport the electricity are known, these will be in\r\nUSD. Supply from each power plant is known. Demand\r\nto each city is also known. The problem is\r\nMathematical Formulation\r\nThe classical linear programming formulation goes as follows:\r\nObjective\r\n\\[\r\nMinimize. \\sum_{i,j}{x_{ij}c_{ij}}\r\n\\]\r\n\\[\r\ns.t.\r\n\\]\r\nSupply Constraints\r\n\\[\r\n\\sum_{j}{x_{ij}} \\le s_i \\: \\forall i \\in S\r\n\\]\r\nDemand Constraints\r\n\\[\r\n\\sum_{i}{x_{ij}} \\ge d_j \\: \\forall j \\in D\r\n\\]\r\nNon-negativity Constraints\r\n\\[\r\nx_{ij} \\ge 0 \\: \\forall (i,j)\\in A\r\n\\]\r\nData\r\nFirst we need to get our data. This will come in the form of an\r\nadjacency matrix which we will readily convert into an arc\r\nmatrix for reasons that will become clear soon (building constraints for\r\nthe LP).\r\n\r\n\r\n#Number of plants and cities\r\nNUM_POWER_PLANTS = 3\r\nNUM_CITIES = 4\r\n\r\n# Adjacency matrix\r\nadj.matrix = matrix(c(8, 6, 10, 9, 9, 12, 13, 7, 14, 9, 16, 5), \r\n                    nrow = NUM_POWER_PLANTS, \r\n                    ncol = NUM_CITIES)\r\nadj.matrix\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]    8    9   13    9\r\n[2,]    6    9    7   16\r\n[3,]   10   12   14    5\r\n\r\nNext we need to take a look at our supply and demand.\r\n\r\n\r\n# Supply and demand vectors\r\nsupply <- c(35, 50, 40)\r\ndemand <- c(45, 20, 30, 30)\r\nsupply\r\n\r\n[1] 35 50 40\r\n\r\ndemand\r\n\r\n[1] 45 20 30 30\r\n\r\nNext define the Power Plants and Cities.\r\n\r\n\r\n# Supply node id lookup\r\nPOWER_PLANTS = 1:3\r\nPOWER_PLANT_LABELS <- sprintf(\"Plant %s\", 1:3)\r\n\r\n# Demand node id lookup\r\nCITIES = 1:4\r\nCITIES_LABELS <- sprintf(\"City %s\", 1:4)\r\n\r\nPOWER_PLANT_LABELS\r\n\r\n[1] \"Plant 1\" \"Plant 2\" \"Plant 3\"\r\n\r\nCITIES_LABELS\r\n\r\n[1] \"City 1\" \"City 2\" \"City 3\" \"City 4\"\r\n\r\nFor the nodes we will supply them with some metatdata into the\r\nDiagrammeR package for slick plotting capability.\r\n\r\n\r\n# Create the powerplant nodes\r\npowerplant_nodes <- DiagrammeR::create_node_df(nodes = POWER_PLANTS,\r\n                                  label = POWER_PLANT_LABELS,\r\n                                  style = \"filled\",\r\n                                  type = \"upper\",\r\n                                  color = rep(\"green\", length(POWER_PLANTS)),\r\n                                  shape = rep(\"circle\", length(POWER_PLANTS)),\r\n                                  data = supply,\r\n                                  n = length(POWER_PLANTS)\r\n                                  )\r\n\r\n# Create the city nodes\r\ncity_nodes <- DiagrammeR::create_node_df(nodes = CITIES,\r\n                                  label = CITIES_LABELS,\r\n                                  style = \"filled\",\r\n                                  type = \"lower\",\r\n                                  color = rep(\"red\", length(CITIES)),\r\n                                  shape = rep(\"square\", length(CITIES)),\r\n                                  data = demand,\r\n                                  n = length(CITIES)\r\n                                  )\r\n\r\n# Create the DiagrammeR dataframe\r\nnodes <- DiagrammeR::combine_ndfs(powerplant_nodes, city_nodes)\r\nnodes\r\n\r\n  id  type   label nodes  style color  shape data\r\n1  1 upper Plant 1     1 filled green circle   35\r\n2  2 upper Plant 2     2 filled green circle   50\r\n3  3 upper Plant 3     3 filled green circle   40\r\n4  4 lower  City 1     1 filled   red square   45\r\n5  5 lower  City 2     2 filled   red square   20\r\n6  6 lower  City 3     3 filled   red square   30\r\n7  7 lower  City 4     4 filled   red square   30\r\n\r\nOne way to convert the adjacency matrix into a\r\nfriendlier edge format is to simply gather it with some\r\nbasic naming of the rows and columns. Once we have the\r\ntidy_edges we can acquire the node_id for each\r\nby doing a little joining with the nodes table formed\r\nabove. This will supply the id's required for the\r\nDiagrammeR package.\r\n\r\n\r\n# Create the \"from\" \"to\" representation, from the raw matrix form\r\ntmp <- adj.matrix %>% as.data.frame\r\nnames(tmp) <- CITIES_LABELS\r\ntmp$from <- POWER_PLANT_LABELS\r\ntidy_edges <- tmp %>% \r\n                tidyr::gather(., key = \"to\", value = \"data\", -c(from)) %>%\r\n                dplyr::mutate(color = \"black\", rel = \"requires\")\r\n\r\n# Go find the from_id\r\nfrom_id = tidy_edges %>% \r\n            dplyr::inner_join(nodes, by = c(\"from\"=\"label\")) %>% \r\n            dplyr::transmute(from_id = id) %>%\r\n            dplyr::pull(from_id)\r\n\r\n# Go find the to_id\r\nto_id = tidy_edges %>% \r\n            dplyr::inner_join(nodes, by = c(\"to\"=\"label\")) %>% \r\n            dplyr::transmute(to_id = id) %>% \r\n            dplyr::pull(to_id)\r\n\r\n\r\n# Get the from_id\r\ntidy_edges$from_id = from_id\r\n\r\n# Get the to_id\r\ntidy_edges$to_id = to_id\r\n\r\nedges <- DiagrammeR::create_edge_df(from = from_id,\r\n                                    to = to_id,\r\n                                    rel = tidy_edges$rel,\r\n                                    color = tidy_edges$color,\r\n                                    data = tidy_edges$data,\r\n                                    label = tidy_edges$data)\r\n\r\n\r\n# Always a good idea to short your edges logically\r\nedges <- edges %>% \r\n            dplyr::arrange(-desc(from), -desc(to)) %>% \r\n            dplyr::mutate(id = row_number())\r\nedges\r\n\r\n   id from to      rel color data label\r\n1   1    1  4 requires black    8     8\r\n2   2    1  5 requires black    9     9\r\n3   3    1  6 requires black   13    13\r\n4   4    1  7 requires black    9     9\r\n5   5    2  4 requires black    6     6\r\n6   6    2  5 requires black    9     9\r\n7   7    2  6 requires black    7     7\r\n8   8    2  7 requires black   16    16\r\n9   9    3  4 requires black   10    10\r\n10 10    3  5 requires black   12    12\r\n11 11    3  6 requires black   14    14\r\n12 12    3  7 requires black    5     5\r\n\r\nA Quick Visual\r\n\r\n\r\n# Create the graph\r\ng = DiagrammeR::create_graph(nodes_df = nodes,\r\n                             edges_df = edges, \r\n                             directed = T, \r\n                             graph_name = \"transportation\")\r\n\r\n# Render the graph\r\nDiagrammeR::render_graph(g, title = \"Power Plants Electricity Transportation to Cities\")\r\n\r\n\r\n\r\nAnother way to visualize this is by converting it to an igraph, which\r\nlooks pretty horrible.\r\n\r\n\r\nig <- DiagrammeR::to_igraph(g)\r\nplot(ig, vertex.size=30)\r\n\r\n\r\n\r\nLP Model Formulation\r\nObjective\r\nFirst things first, we know the costs on each arc already, because\r\nthey are supplied from our adjacency matrix initially. So\r\nwe can just go recover those.\r\n\r\n\r\n# Get objective value\r\nf.obj = edges %>% dplyr::pull(data)\r\nf.obj\r\n\r\n [1]  8  9 13  9  6  9  7 16 10 12 14  5\r\n\r\nConstraints, Direction, and\r\nRHS\r\nFirst some convenient numbering systems will be helpful to preserve\r\nknowledge of the nodes and which id is which.\r\n\r\n\r\n# Align ids up for matrix building\r\nPOWER_PLANT_NODE_IDS <- POWER_PLANTS\r\nCITY_NODE_IDS <- POWER_PLANTS[length(POWER_PLANTS)] + CITIES\r\n\r\nPOWER_PLANT_NODE_IDS\r\n\r\n[1] 1 2 3\r\n\r\nCITY_NODE_IDS\r\n\r\n[1] 4 5 6 7\r\n\r\nNext we need to build the supply constraints. So for each unique\r\npowerplant arc, then sum of it’s outbound arcs must be less than the\r\nsupply constraint, for all supply.\r\n\r\n\r\n# One constraint row for each supply node\r\nSmat = matrix(0, nrow = length(POWER_PLANTS), ncol = nrow(edges))\r\nSmat_rhs = supply\r\nSmat_dir = rep(\"<=\", length(supply))\r\n\r\nfor(i in 1:nrow(Smat)){\r\n  for(j in 1:ncol(Smat)){\r\n    from = edges$from[j]\r\n    to = edges$to[j]\r\n    \r\n    if(from == POWER_PLANT_NODE_IDS[i]){\r\n      Smat[i, j] = 1\r\n    }\r\n  }\r\n}\r\n\r\nSmat\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\r\n[1,]    1    1    1    1    0    0    0    0    0     0     0     0\r\n[2,]    0    0    0    0    1    1    1    1    0     0     0     0\r\n[3,]    0    0    0    0    0    0    0    0    1     1     1     1\r\n\r\nSmat_rhs\r\n\r\n[1] 35 50 40\r\n\r\nSmat_dir\r\n\r\n[1] \"<=\" \"<=\" \"<=\"\r\n\r\nWith the same logic we need to construct the demand matrix.\r\n\r\n\r\n# One constraint node for each demand node\r\nDmat = matrix(0, nrow=length(CITIES), ncol = nrow(edges))\r\nDmat_rhs = demand\r\nDmat_dir = rep(\">=\", length(demand))\r\n\r\nfor(i in 1:nrow(Dmat)){\r\n  for(j in 1:ncol(Dmat)){\r\n    from = edges$from[j]\r\n    to = edges$to[j]\r\n    \r\n    if(to == CITY_NODE_IDS[i]){\r\n      Dmat[i, j] = 1\r\n    }\r\n  }\r\n}\r\n\r\nDmat\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\r\n[1,]    1    0    0    0    1    0    0    0    1     0     0     0\r\n[2,]    0    1    0    0    0    1    0    0    0     1     0     0\r\n[3,]    0    0    1    0    0    0    1    0    0     0     1     0\r\n[4,]    0    0    0    1    0    0    0    1    0     0     0     1\r\n\r\nDmat_rhs\r\n\r\n[1] 45 20 30 30\r\n\r\nDmat_dir\r\n\r\n[1] \">=\" \">=\" \">=\" \">=\"\r\n\r\nNow that we have our supply and demand data matrices put together,\r\nlet’s unify them and take a look at all of our constraints.\r\n\r\n\r\n# All `lpSolve` data\r\nf.obj <- f.obj\r\nf.cons <- rbind(Smat, Dmat)\r\nf.rhs <- c(Smat_rhs, Dmat_rhs)\r\nf.dir <- c(Smat_dir, Dmat_dir)\r\n\r\nf.obj\r\n\r\n [1]  8  9 13  9  6  9  7 16 10 12 14  5\r\n\r\nf.cons\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\r\n[1,]    1    1    1    1    0    0    0    0    0     0     0     0\r\n[2,]    0    0    0    0    1    1    1    1    0     0     0     0\r\n[3,]    0    0    0    0    0    0    0    0    1     1     1     1\r\n[4,]    1    0    0    0    1    0    0    0    1     0     0     0\r\n[5,]    0    1    0    0    0    1    0    0    0     1     0     0\r\n[6,]    0    0    1    0    0    0    1    0    0     0     1     0\r\n[7,]    0    0    0    1    0    0    0    1    0     0     0     1\r\n\r\nf.rhs\r\n\r\n[1] 35 50 40 45 20 30 30\r\n\r\nf.dir\r\n\r\n[1] \"<=\" \"<=\" \"<=\" \">=\" \">=\" \">=\" \">=\"\r\n\r\nSolution\r\nNow we unite everything together and drop it into the solver.\r\nRemember, we are trying to minimize cost in our objective, the\r\nconstraints will maximize the flow. So specify min.\r\n\r\n\r\n# Get the results\r\nresults <- lp(direction = \"min\",  \r\n              objective.in = f.obj, \r\n              const.mat = f.cons, \r\n              const.dir = f.dir, \r\n              const.rhs = f.rhs)\r\n\r\nresults$objval\r\n\r\n[1] 880\r\n\r\nmatrix(results$solution, nrow = length(supply), ncol = length(demand))\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]   15    0   30    0\r\n[2,]   20   20    0    0\r\n[3,]    0    0   10   30\r\n\r\nVisualization\r\nCool! We have our solution. It looks like:\r\nPlant 1 should supply City 1 and\r\nCity 2.\r\nPlant 2 should supply City 1 and\r\nCity 3, and\r\nPlant 3 should supply City 1 and\r\nCity 4.\r\nOne interesting insight is the shared responsibility all plants have\r\nin getting resources to City 1. Another insight is the\r\npartnerships to make City 1 supplied as cheaply as\r\npossible, may require some additional coordination costs to\r\ncarefully handle any error in costs between ill-shipments. The last\r\ninsights are the siloe’s, City 2 is entirely handled by\r\nPlant 1, City 3 by Plant 2, and\r\nCity 4 by Plant 3. This shows us the reliance\r\nwe have on those energy transfers going through, otherwise it’s not only\r\nlights out, it is also a more expensive transfer to get them back\r\non!\r\n\r\n\r\n# Add in the flow\r\nedges$flow <- results$solution\r\n\r\n# Color things if there exists flow\r\nedges <- edges %>% dplyr::mutate(weight = flow,\r\n                                 label = flow,\r\n                                 color = if_else(condition = flow > 0, true = \"black\",false = \"grey\"))\r\n\r\n# Create the graph\r\ng = DiagrammeR::create_graph(nodes_df = nodes,\r\n                             edges_df = edges, \r\n                             directed = T, \r\n                             graph_name = \"transportation\")\r\n\r\n# Draw the graph\r\nDiagrammeR::render_graph(g, title = \"Power Plants Electricity Transportation to Cities\")\r\n\r\n\r\n\r\n\r\n\r\n# ig <- DiagrammeR::to_igraph(g)\r\n\r\nnodes\r\n\r\n  id  type   label nodes  style color  shape data\r\n1  1 upper Plant 1     1 filled green circle   35\r\n2  2 upper Plant 2     2 filled green circle   50\r\n3  3 upper Plant 3     3 filled green circle   40\r\n4  4 lower  City 1     1 filled   red square   45\r\n5  5 lower  City 2     2 filled   red square   20\r\n6  6 lower  City 3     3 filled   red square   30\r\n7  7 lower  City 4     4 filled   red square   30\r\n\r\nvisNet_nodes <- nodes %>% \r\n                dplyr::mutate(shape = \"circle\",\r\n                              font = 12,\r\n                              size = data,)\r\nedges\r\n\r\n   id from to      rel color data label flow weight\r\n1   1    1  4 requires black    8    15   15     15\r\n2   2    1  5 requires black    9    20   20     20\r\n3   3    1  6 requires  grey   13     0    0      0\r\n4   4    1  7 requires  grey    9     0    0      0\r\n5   5    2  4 requires black    6    20   20     20\r\n6   6    2  5 requires  grey    9     0    0      0\r\n7   7    2  6 requires black    7    30   30     30\r\n8   8    2  7 requires  grey   16     0    0      0\r\n9   9    3  4 requires black   10    10   10     10\r\n10 10    3  5 requires  grey   12     0    0      0\r\n11 11    3  6 requires  grey   14     0    0      0\r\n12 12    3  7 requires black    5    30   30     30\r\n\r\nvisNet_edges <- edges %>% \r\n                dplyr::mutate(label = paste(\"(\",flow,\",\",data,\")\",sep=\"\"),\r\n                              length = 250,\r\n                              width = flow/5,\r\n                              arrows = \"to\") \r\n  \r\nvisNetwork(visNet_nodes, \r\n           visNet_edges,\r\n           width = \"100%\",\r\n           main = \"Power Distribution | (flow, cost)\")\r\n\r\n\r\n\r\nAn Easier Way\r\nIt’s almost spooky how easy it is with the lp.transport\r\nAPI. Verify that we got the same solution, 880 and the\r\ndecision vector is also the same. Since the objective value is the same,\r\nthe solutions might differ, because they are equally optimal. Pretty\r\ninteresting to see how easy it is with the convenience of\r\nlpSolve though!\r\n\r\n\r\n# Get the results\r\nresults <- lpSolve::lp.transport(cost.mat = adj.matrix, \r\n                                  direction = \"min\", \r\n                                  row.signs = rep(\"<=\", nrow(adj.matrix)), \r\n                                  col.signs = rep(\">=\", ncol(adj.matrix)),\r\n                                  row.rhs = supply, \r\n                                  col.rhs = demand)\r\n\r\n# Verify `objval` is the same as above\r\nresults$objval\r\n\r\n[1] 880\r\n\r\nresults$solution\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]   15   20    0    0\r\n[2,]   20    0   30    0\r\n[3,]   10    0    0   30\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-11-transportation-problem-in-r/transportation-problem-in-r_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-07-13T06:07:51-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-10-minimum-cost-network-flow-problem-in-r/",
    "title": "Minimum Cost Network Flow Problem (MCNFP) in R",
    "description": "In this post we will walk through how to make least cost maximum flow decisions using linear programming.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-10",
    "categories": [
      "network flows",
      "linear programming",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nTraffic Minimum Cost\r\nNetwork Flow\r\nThe Problem\r\nThe Data\r\nNetwork Visualization\r\n\r\nModel Data\r\nAverage In-Flow\r\nDemand\r\n\r\n\r\nLinear Programming (LP)\r\nLP Structure\r\nArc Constraints\r\nPutting It All Together\r\n\r\nSolve the LP\r\nVisualize Solution\r\nCleaning Up The Visual\r\nFlow vs. Capacity\r\nvs. Time\r\nShortest Path\r\nShortest Path\r\nVisualization\r\n\r\n\r\nReusable Functionality\r\nSourcing\r\n\r\nTraffic Minimum Cost Network\r\nFlow\r\nOne common extension to Maximum Flow problems is to do\r\nit as cheaply as possible. In this article we will extend the maximum\r\nflow example we wrote on in the last\r\npost and include a minimum cost component. The generic problem\r\nformation is below:\r\nObjective \\[\r\nMinimize. \\sum_{(i,j)\\in A}{c_{ij}x_{ij}}   \r\n\\] \\[\r\ns.t.\r\n\\] Node Flow Constraints \\[\r\n\\sum_{j}{x_{ij}} - \\sum_{i}{x_{ji}} = b_i \\: \\forall i \\in N\r\n\\] Arc Flow Constraints \\[\r\nl_{ij} \\le x_{ij} \\le u_{ij} \\: \\forall (i,j) \\in A\r\n\\]\r\nThe Problem\r\nRoad networks are everywhere in our society. In any given\r\nintersection there is a flow of cars, intersections with stop lights,\r\nand connections between each. Every road has a feasible limit it can\r\nsupport. In fact, this is often the cause of most congestion. Our goal\r\nis to minimize the total time required for all cars to travel from node\r\n1 to node 6 in a fictitious road network.\r\nThe Data\r\n\r\n\r\nnodes <- data.frame(id = c(1:6), color = c(\"green\", rep(\"grey\", 4), \"red\"))\r\nedges <- data.frame(from = c(1, 1, 2, 2, 5, 4, 4, 3, 3), \r\n                    to = c(2, 3, 5, 4, 6, 5, 6, 5, 4),\r\n                    lower_bound = 0,\r\n                    upper_bound = c(800, 600, 100, 600, 600, 600, 400, 400, 300),\r\n                    time = c(10, 50, 70, 30, 30, 30, 60, 60, 10), # in minutes\r\n                    flow = 0, #TBD\r\n                    color = \"grey\") %>% dplyr::arrange(from, to)\r\nnodes\r\n\r\n  id color\r\n1  1 green\r\n2  2  grey\r\n3  3  grey\r\n4  4  grey\r\n5  5  grey\r\n6  6   red\r\n\r\nedges\r\n\r\n  from to lower_bound upper_bound time flow color\r\n1    1  2           0         800   10    0  grey\r\n2    1  3           0         600   50    0  grey\r\n3    2  4           0         600   30    0  grey\r\n4    2  5           0         100   70    0  grey\r\n5    3  4           0         300   10    0  grey\r\n6    3  5           0         400   60    0  grey\r\n7    4  5           0         600   30    0  grey\r\n8    4  6           0         400   60    0  grey\r\n9    5  6           0         600   30    0  grey\r\n\r\nNetwork Visualization\r\nWe can see the upper bounds plotted on the edges of this\r\ntransportation network below. The width indicates more capacity for\r\nflow. Examine the trade-off between time and space for travel between\r\narc (1,2).\r\n\r\n\r\ng <- igraph::graph_from_data_frame(d = edges,\r\n                                   directed = TRUE,\r\n                                   vertices = nodes)\r\n\r\n# Capacity plot\r\nplot(g, \r\n     main = \"Vehicle Transportion Network Capacities\", \r\n     edge.label = E(g)$upper_bound, \r\n     edge.width = E(g)$upper_bound/150,\r\n     vertex.color = V(g)$color,\r\n     edge.color = E(g)$color)\r\n\r\n\r\n# Time plot\r\nplot(g, \r\n     main = \"Vehicle Transportion Network Travel Time\", \r\n     edge.label = E(g)$time, \r\n     edge.width = E(g)$time/10,\r\n     vertex.color = V(g)$color,\r\n     edge.color = E(g)$color)\r\n\r\n\r\n\r\nModel Data\r\nAverage In-Flow\r\nFirst we assume the average number of cars that flow through this\r\nnetwork. This is often recovered from past databases that record the\r\nflows through the network.\r\n\r\n\r\nAVERAGE_FLOW <- 900 # per hour\r\nAVERAGE_FLOW\r\n\r\n[1] 900\r\n\r\nDemand\r\nNext we set up the demand that will be flowing through the network.\r\nThis is indicated as the vector b in our model formation\r\nabove. This means the initial node has a supply of 900 vehicles, while\r\nthe final node has a demand of 900 nodes. The objective is to flow as\r\nmany vehicles through the network, in the shortest amount of time.\r\n\r\n\r\ndemand <- c(AVERAGE_FLOW, rep(0, nrow(nodes)-2), -AVERAGE_FLOW)\r\ndemand\r\n\r\n[1]  900    0    0    0    0 -900\r\n\r\nLinear Programming (LP)\r\nLP Structure\r\nThe next step is to set up the optimization. Let’s do that now. There\r\nare 3 key ingredients.\r\nObjective\r\nConstraints\r\nDirections\r\nArc Constraints\r\nWe want to build the constraint matrix, which has 2 parts.\r\nArc Capacity Constraints\r\nNode Flow Constraints\r\nThe Arc Capacity Constraints are the first we\r\naddress.\r\nThe Amat, or A matrix, is the arc matrix which contains\r\nthe upper bounds. Since linear programming relies on the resource\r\nmatrix, we need one row for each arc, our dimensions for variable flow\r\nselection are the number of arcs also. So this means we need an\r\nidentity matrix for the rows of arcs and columns of\r\narcs.\r\n\r\n\r\ncreate_upper_arc_constraints <- function(edges){\r\n  Amat <- matrix(0, nrow = nrow(edges), ncol = nrow(edges))\r\n  Amat_dir <- rep(\"<=\", nrow(Amat))\r\n  Amat_rhs <- c()\r\n\r\n  for(i in 1:ncol(Amat)){\r\n    Amat[i,i] <- 1\r\n    Amat_rhs <- c(Amat_rhs, edges$upper_bound[i])\r\n  }\r\n  \r\n  list(Amat_upper = Amat,\r\n       Amat_upper_dir = Amat_dir,\r\n       Amat_upper_rhs = Amat_rhs)\r\n}\r\n\r\n# This could be higher than zero, but for standard LP this is the default configuration, so not needed.\r\n# create_lower_arc_constraints <- function(edges){\r\n#   Amat <- matrix(0, nrow = nrow(edges), ncol = nrow(edges))\r\n#   Amat_dir <- rep(\">=\", nrow(Amat))\r\n#   Amat_rhs <- c()\r\n# \r\n#   for(i in 1:ncol(Amat)){\r\n#     Amat[i,i] <- 1\r\n#     Amat_rhs <- c(Amat_rhs, edges$lower_bound[i])\r\n#   }\r\n#   \r\n#   list(Amat_lower = Amat,\r\n#        Amat_lower_dir = Amat_dir,\r\n#        Amat_lower_rhs = Amat_rhs)\r\n# }\r\n\r\nupper_results <- create_upper_arc_constraints(edges)\r\n# lower_results <- create_lower_arc_constraints(edges)\r\n# \r\n# Amat <- rbind(upper_results$Amat_upper, lower_results$Amat_lower)\r\n# Amat_dir <- c(upper_results$Amat_upper_dir, lower_results$Amat_lower_dir)\r\n# Amat_rhs <- c(upper_results$Amat_upper_rhs, lower_results$Amat_lower_rhs)\r\n\r\nAmat <- upper_results$Amat_upper\r\nAmat_dir <- upper_results$Amat_upper_dir\r\nAmat_rhs <- upper_results$Amat_upper_rhs\r\n\r\nAmat\r\n\r\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\r\n [1,]    1    0    0    0    0    0    0    0    0\r\n [2,]    0    1    0    0    0    0    0    0    0\r\n [3,]    0    0    1    0    0    0    0    0    0\r\n [4,]    0    0    0    1    0    0    0    0    0\r\n [5,]    0    0    0    0    1    0    0    0    0\r\n [6,]    0    0    0    0    0    1    0    0    0\r\n [7,]    0    0    0    0    0    0    1    0    0\r\n [8,]    0    0    0    0    0    0    0    1    0\r\n [9,]    0    0    0    0    0    0    0    0    1\r\n\r\nAmat_dir\r\n\r\n[1] \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\"\r\n\r\nAmat_rhs\r\n\r\n[1] 800 600 600 100 300 400 600 400 600\r\n\r\nThe Node Flow Constraints are the next to take care\r\nof.\r\nThe Bmat, or B matrix, is a matrix which contains the\r\nnode balance equations codified by flows.\r\nFor each node, we need to match it’s from node and\r\nto node with the appropriate inflow and outflow. If it\r\nmatches an inflow arc, this is increase, so 1\r\nis in the arc column. If it matches an outflow arch, this\r\nis decrease, so -1 is in the arc column. Otherwise\r\n0 remains as the placeholder. The sign here is\r\n== because they must match (i.e., supply = demand). If we\r\nrequire excess at certain points we can set this demand to be higher\r\nthan zero, but we will not do that here.\r\n\r\n\r\ncreate_node_constraints <- function(nodes, edges){\r\n    Bmat <- matrix(0, nrow = nrow(nodes), ncol = nrow(edges))\r\n    Bmat_dir <- rep(\"==\", nrow(Bmat))\r\n    Bmat_rhs <- rep(0, nrow(Bmat))\r\n    \r\n    for(i in 1:nrow(Bmat)){\r\n      node_id <- nodes[i, \"id\"]\r\n      for(j in 1:ncol(Bmat)){\r\n        edge_from <- edges[j,\"from\"]\r\n        edge_to <- edges[j, \"to\"]\r\n        \r\n        if(node_id == edge_from){\r\n          # print(paste(\"Outbound for \", node_id, \"From: \",edge_from, \"to: \", edge_to))\r\n          Bmat[i,j] = 1\r\n        }\r\n        else if(node_id == edge_to){\r\n          # print(paste(\"Inbound for \", node_id, \"From: \",edge_from, \"to: \", edge_to))\r\n          Bmat[i,j] = -1\r\n        }\r\n        else{\r\n          Bmat[i,j] = 0\r\n        }\r\n      }\r\n      Bmat_rhs[i] <- demand[i]\r\n    }\r\n    \r\n    list(Bmat = Bmat,\r\n         Bmat_dir = Bmat_dir,\r\n         Bmat_rhs = Bmat_rhs\r\n    )\r\n}\r\n\r\nresults <- create_node_constraints(nodes, edges)\r\nBmat <- results$Bmat\r\nBmat_dir <- results$Bmat_dir\r\nBmat_rhs <- results$Bmat_rhs\r\n\r\nBmat\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\r\n[1,]    1    1    0    0    0    0    0    0    0\r\n[2,]   -1    0    1    1    0    0    0    0    0\r\n[3,]    0   -1    0    0    1    1    0    0    0\r\n[4,]    0    0   -1    0   -1    0    1    1    0\r\n[5,]    0    0    0   -1    0   -1   -1    0    1\r\n[6,]    0    0    0    0    0    0    0   -1   -1\r\n\r\nBmat_dir\r\n\r\n[1] \"==\" \"==\" \"==\" \"==\" \"==\" \"==\"\r\n\r\nBmat_rhs\r\n\r\n[1]  900    0    0    0    0 -900\r\n\r\nPutting It All Together\r\nNext, the objective is going to be the time. This is the cost we have\r\nto pay for assigning flow to an arc. Let’s take a look at everything all\r\ntogether.\r\n\r\n\r\nf.obj <- edges %>% dplyr::pull(time)\r\nf.cons <- rbind(Amat, Bmat)\r\nf.rhs <- c(Amat_rhs, Bmat_rhs)\r\nf.dir <- c(Amat_dir, Bmat_dir)\r\n\r\nf.obj\r\n\r\n[1] 10 50 30 70 10 60 30 60 30\r\n\r\nf.cons\r\n\r\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\r\n [1,]    1    0    0    0    0    0    0    0    0\r\n [2,]    0    1    0    0    0    0    0    0    0\r\n [3,]    0    0    1    0    0    0    0    0    0\r\n [4,]    0    0    0    1    0    0    0    0    0\r\n [5,]    0    0    0    0    1    0    0    0    0\r\n [6,]    0    0    0    0    0    1    0    0    0\r\n [7,]    0    0    0    0    0    0    1    0    0\r\n [8,]    0    0    0    0    0    0    0    1    0\r\n [9,]    0    0    0    0    0    0    0    0    1\r\n[10,]    1    1    0    0    0    0    0    0    0\r\n[11,]   -1    0    1    1    0    0    0    0    0\r\n[12,]    0   -1    0    0    1    1    0    0    0\r\n[13,]    0    0   -1    0   -1    0    1    1    0\r\n[14,]    0    0    0   -1    0   -1   -1    0    1\r\n[15,]    0    0    0    0    0    0    0   -1   -1\r\n\r\nf.rhs\r\n\r\n [1]  800  600  600  100  300  400  600  400  600  900    0    0    0\r\n[14]    0 -900\r\n\r\nf.dir\r\n\r\n [1] \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"==\" \"==\" \"==\" \"==\"\r\n[14] \"==\" \"==\"\r\n\r\nSolve the LP\r\nNow we unite everything together and drop it into the solver.\r\nRemember, we are trying to minimize cost in our objective, the\r\nconstraints will maximize the flow. So specify min.\r\n\r\n\r\nresults <- lp(direction = \"min\",  \r\n              objective.in = f.obj, \r\n              const.mat = f.cons, \r\n              const.dir = f.dir, \r\n              const.rhs = f.rhs)\r\n\r\nedges$flow <- results$solution\r\nedges\r\n\r\n  from to lower_bound upper_bound time flow color\r\n1    1  2           0         800   10  700  grey\r\n2    1  3           0         600   50  200  grey\r\n3    2  4           0         600   30  600  grey\r\n4    2  5           0         100   70  100  grey\r\n5    3  4           0         300   10  200  grey\r\n6    3  5           0         400   60    0  grey\r\n7    4  5           0         600   30  500  grey\r\n8    4  6           0         400   60  300  grey\r\n9    5  6           0         600   30  600  grey\r\n\r\nVisualize Solution\r\nNow that we have our flow we can do some visualizing and analysis.\r\nThere are two key graphics to examine; the\r\nflow vs. capacity and the flow vs. time.\r\nFirst, the flow vs. capacity will give us insights into\r\nstress on the network. This is because of their implicit advantage they\r\nsupply to the optimizer, maximum flows, so naturally, these get flooded\r\nwith traffic. Second, the flow vs. time will give us\r\ninsights into shortest distance paths (i.e., assuming time is\r\nproportional to distance, which is not always the case). This is because\r\npaths with shorter times will enable more to flow through in any given\r\ntime delta. Between these two visuals, a good assessment of the model\r\noutput is feasible.\r\n\r\n\r\n# Set the flow\r\nedges$flow <- results$solution\r\n\r\n# If the arc is flowing oil, change to black\r\nedges$color[which(edges$flow > 0)] <- \"black\"\r\n\r\n# If the arc is at capacity change it to red\r\nedges$color[which(edges$flow == edges$upper_bound)] <- \"red\"\r\n\r\ng <- igraph::graph_from_data_frame(d = edges,\r\n                                   directed = TRUE,\r\n                                   vertices = nodes)\r\n\r\n# Plot the flow and capacity\r\nplot(g,\r\n     edge.label = paste(\"(\",E(g)$flow, \r\n                        \",\",\r\n                         E(g)$upper_bound, \r\n                         \")\", \r\n                        sep=\"\"), \r\n     main = \"Traffic Flow | (flow, capacity)\", \r\n     vertex.color = V(g)$color,\r\n     vertex.size = 25,\r\n     vertex.label.cex = 1.6,\r\n     edge.color = E(g)$color,\r\n     edge.width = E(g)$flow/200)\r\n\r\n\r\n# Plot the time\r\nplot(g,\r\n     edge.label = paste(\"(\",E(g)$flow, \r\n                        \",\",\r\n                         E(g)$time, \r\n                         \")\", \r\n                        sep=\"\"), \r\n     main = \"Traffic Flow | (flow, time)\", \r\n     vertex.color = V(g)$color,\r\n     vertex.size = 25,\r\n     vertex.label.cex = 1.6,\r\n     edge.color = E(g)$color,\r\n     edge.width = E(g)$flow/200)\r\n\r\n\r\n\r\nCleaning Up The Visual\r\n\r\n\r\nnodes\r\n\r\n  id color\r\n1  1 green\r\n2  2  grey\r\n3  3  grey\r\n4  4  grey\r\n5  5  grey\r\n6  6   red\r\n\r\nvisNet_nodes <- nodes %>% \r\n                dplyr::mutate(label = paste(\"Location \", id),\r\n                              font.size =10,\r\n                              type = \"black\",\r\n                              shape = \"circle\",\r\n                              font = 12)\r\n\r\nvisNet_edges <- edges %>% \r\n                dplyr::mutate(label = paste(\"(\",flow,\",\",upper_bound,\")\",sep=\"\"),\r\n                              length = 250,\r\n                              width = flow/100,\r\n                              arrows = \"to\") \r\n  \r\nvisNetwork(visNet_nodes, \r\n           visNet_edges,\r\n           width = \"100%\",\r\n           main = \"Least Time Maximum Vehicle Flow | (flow, capacity)\")\r\n\r\n\r\n\r\nFlow vs. Capacity vs. Time\r\nTwo of the most popular roadways are\r\n2 -> 4 and 5 -> 6. These supply the\r\nleast amount of time and the most amount of capacity. Without a doubt,\r\nthe model has exploited these as much as possible. The only trick is,\r\njust how the flow gets there. We see up front that 1 ships\r\noff a 200 to 700 flow split from the 900\r\nsupply with the heavier allocation toward 2. Why?\r\n2 is connected to a popular\r\nroadway, meaning much more potential to flow (and\r\nquickly).\r\nIn order to get to 4, going through 3\r\nwill cost much more time (60 oppose to 40).\r\n3 has two outlets, but one is one of the worst\r\nroutes on the network due to it’s 60 minute trek, so it doesn’t even get\r\nany flow allocated.\r\nShortest Path\r\nThe shortest path shows one very interesting insight to this model;\r\nsending a maximum flow through the network is not all about time. The\r\nshortest path (least time) is the sequence\r\n1 -> 2 -> 4 -> 6. However, from the above, we see\r\nthat this isn’t the most stressed path. Why? We aren’t only\r\ninterested in short times for the vehicles flowing through the network.\r\nWe are also interested in getting them all through it! We assumed there\r\nwas a 900 average vehicle flow, and having a macro-level view of this\r\nsystem, sending them all down the shortest path would not solve it (that\r\nis, we would not send as much as possible, only as cheaply as possible;\r\nwe could have pushed more). In order to get the most cars sent through\r\nthe network, in the shortest amount of time we also must take advantage\r\nof the popular roadways that the model is straining (or\r\nadd incentive to the not so popular roadways with\r\ngreater capacity or shorter commute times).\r\n\r\n\r\nshortest.paths <- igraph::shortest_paths(graph = g, from = 1, to = 6)\r\ns_path <- shortest.paths$vpath[[1]]\r\ns_path\r\n\r\n+ 4/6 vertices, named, from ac6f2e8:\r\n[1] 1 2 4 6\r\n\r\nshortest_commute_time <- E(g, path = s_path)$time %>% sum\r\nshortest_commute_time\r\n\r\n[1] 100\r\n\r\nShortest Path Visualization\r\n\r\n\r\nE(g)$color <- \"black\"\r\nE(g, path = s_path)$color <- \"blue\"\r\n\r\nplot(g,\r\n     edge.label = paste(\"(\",E(g)$flow, \r\n                        \",\",\r\n                         E(g)$time, \r\n                         \")\", \r\n                        sep=\"\"), \r\n     main = \"Shortest Path | (flow, capacity)\", \r\n     vertex.color = V(g)$color,\r\n     vertex.size = 25,\r\n     vertex.label.cex = 1.6,\r\n     edge.color = E(g)$color,\r\n     edge.width = E(g)$flow/200)\r\n\r\n\r\n\r\nReusable Functionality\r\nWe can also build a function to reuse for next time.\r\n\r\n\r\n#\r\n# @lp.mincost.maxflow: data.frame, integer, integer, integer or list -> list\r\n#   function outputs an edge list with flows.\r\n#\r\n# @edges: data.frame, should have from, to, and capacity columns for each edge in the network. from and to columns should contain unique node ids as integers from 0 to N.\r\n# @source.id: integer, unique node id\r\n# @dest.id: integer, unique node id\r\n# @flow.demand: integer or list, if integer create a demand signal to push that much from source.id to dest.id, if a list expectation is that it sums to 0 and will contain each nodes supply (if positive) and demand (if negative).\r\n\r\n\r\nlp.mincost.maxflow <- function(edges, source.id, dest.id, flow.demand){\r\n  if(!(\"from\" %in% names(edges)) || \r\n     !(\"to\" %in% names(edges)) ||\r\n     !(\"upper_bound\" %in% names(edges)) ||\r\n     !(\"lower_bound\" %in% names(edges))){\r\n    print(\"Need from, to, and capacity columns.\")\r\n  }\r\n  else{\r\n    \r\n    # Infer the nodes\r\n    nodes <- data.frame(id = sort(unique(c(unique(edges$from), unique(edges$to)))))\r\n    \r\n    # Infer the demand to flow through the network\r\n    if(length(flow.demand)>1){\r\n      if(sum(flow.demand) == 0){\r\n        demand <- flow.demand\r\n      }else{\r\n        print(\"Flow demand doesn't add up to 0.\")\r\n      }\r\n    }\r\n    else{\r\n      demand <- c(flow.demand, rep(0, nrow(nodes)-2), -flow.demand)\r\n    }\r\n\r\n    # Get arc capacity constraints\r\n    create_arc_capacity_constraints <- function(edges){\r\n        \r\n      # For upper\r\n      Amat <- matrix(0, nrow = nrow(edges), ncol = nrow(edges))\r\n      Amat_dir <- rep(\"<=\", nrow(Amat))\r\n      Amat_rhs <- c()\r\n    \r\n      for(i in 1:ncol(Amat)){\r\n        Amat[i,i] <- 1\r\n        Amat_rhs <- c(Amat_rhs, edges$upper_bound[i])\r\n      }\r\n      \r\n      # For lower\r\n      Bmat <- matrix(0, nrow = nrow(edges), ncol = nrow(edges))\r\n      Bmat_dir <- rep(\">=\", nrow(Bmat))\r\n      Bmat_rhs <- c()\r\n    \r\n      for(i in 1:ncol(Bmat)){\r\n        Bmat[i,i] <- 1\r\n        Bmat_rhs <- c(Bmat_rhs, edges$lower_bound[i])\r\n      }\r\n      \r\n      list(Amat = rbind(Amat, Bmat),\r\n           Amat_dir = c(Amat_dir, Bmat_dir),\r\n           Amat_rhs = c(Amat_rhs, Bmat_rhs))\r\n    }\r\n    \r\n   results <- create_arc_capacity_constraints(edges)\r\n    Amat <- results$Amat\r\n    Amat_dir <- results$Amat_dir\r\n    Amat_rhs <- results$Amat_rhs\r\n    \r\n    # Create node flow constraints (in = out)\r\n    create_node_constraints <- function(nodes, edges){\r\n        Bmat <- matrix(0, nrow = nrow(nodes), ncol = nrow(edges))\r\n        Bmat_dir <- rep(\"==\", nrow(Bmat))\r\n        Bmat_rhs <- rep(0, nrow(Bmat))\r\n        \r\n        for(i in 1:nrow(Bmat)){\r\n          node_id <- nodes[i, \"id\"]\r\n          for(j in 1:ncol(Bmat)){\r\n            edge_from <- edges[j,\"from\"]\r\n            edge_to <- edges[j, \"to\"]\r\n            \r\n            if(node_id == edge_from){\r\n              # print(paste(\"Outbound for \", node_id, \"From: \",edge_from, \"to: \", edge_to))\r\n              Bmat[i,j] = 1\r\n            }\r\n            else if(node_id == edge_to){\r\n              # print(paste(\"Inbound for \", node_id, \"From: \",edge_from, \"to: \", edge_to))\r\n              Bmat[i,j] = -1\r\n            }\r\n            else{\r\n              Bmat[i,j] = 0\r\n            }\r\n          }\r\n          Bmat_rhs[i] <- demand[i]\r\n        }\r\n        \r\n        list(Bmat = Bmat,\r\n             Bmat_dir = Bmat_dir,\r\n             Bmat_rhs = Bmat_rhs\r\n        )\r\n    }\r\n    \r\n    results <- create_node_constraints(nodes, edges)\r\n    Bmat <- results$Bmat\r\n    Bmat_dir <- results$Bmat_dir\r\n    Bmat_rhs <- results$Bmat_rhs\r\n    \r\n    # Bring together\r\n    f.obj <- edges$cost\r\n    f.cons <- rbind(Amat, Bmat)\r\n    f.rhs <- c(Amat_rhs, Bmat_rhs)\r\n    f.dir <- c(Amat_dir, Bmat_dir)\r\n    results <- lp(direction = \"min\",  \r\n                  objective.in = f.obj, \r\n                  const.mat = f.cons, \r\n                  const.dir = f.dir, \r\n                  const.rhs = f.rhs)\r\n    \r\n    edges$flow <- results$solution\r\n  }\r\n  \r\n  list(edges = edges, results = results)\r\n}\r\n\r\nedges <- data.frame(from = c(1, 1, 2, 2, 5, 4, 4, 3, 3), \r\n                    to = c(2, 3, 5, 4, 6, 5, 6, 5, 4),\r\n                    lower_bound = 0,\r\n                    upper_bound = c(800, 600, 100, 600, 600, 600, 400, 400, 300),\r\n                    cost = c(10, 50, 70, 30, 30, 30, 60, 60, 10))\r\n\r\nlp.mincost.maxflow(edges = edges, source.id = 1, dest.id = 6, flow.demand = 900)\r\n\r\n$edges\r\n  from to lower_bound upper_bound cost flow\r\n1    1  2           0         800   10  700\r\n2    1  3           0         600   50  200\r\n3    2  5           0         100   70  100\r\n4    2  4           0         600   30  600\r\n5    5  6           0         600   30  600\r\n6    4  5           0         600   30  500\r\n7    4  6           0         400   60  300\r\n8    3  5           0         400   60    0\r\n9    3  4           0         300   10  200\r\n\r\n$results\r\nSuccess: the objective function is 95000 \r\n\r\nSourcing\r\nYou can also source the file to make things easier for next time. The\r\ncode can be found here.\r\n\r\n\r\nsource(\"lp.mincost.maxflow.r\")\r\n\r\nedges <- data.frame(from = c(1, 1, 2, 2, 5, 4, 4, 3, 3), \r\n                    to = c(2, 3, 5, 4, 6, 5, 6, 5, 4),\r\n                    lower_bound = 0,\r\n                    upper_bound = c(800, 600, 100, 600, 600, 600, 400, 400, 300),\r\n                    cost = c(10, 50, 70, 30, 30, 30, 60, 60, 10))\r\n\r\nlp.mincost.maxflow(edges = edges, source.id = 1, dest.id = 6, flow.demand = 900)\r\n\r\n$edges\r\n  from to lower_bound upper_bound cost flow\r\n1    1  2           0         800   10  700\r\n2    1  3           0         600   50  200\r\n3    2  5           0         100   70  100\r\n4    2  4           0         600   30  600\r\n5    5  6           0         600   30  600\r\n6    4  5           0         600   30  500\r\n7    4  6           0         400   60  300\r\n8    3  5           0         400   60    0\r\n9    3  4           0         300   10  200\r\n\r\n$results\r\nSuccess: the objective function is 95000 \r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-10-minimum-cost-network-flow-problem-in-r/minimum-cost-network-flow-problem-in-r_files/figure-html5/visual_1-1.png",
    "last_modified": "2022-07-15T04:32:36-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-09-maximum-network-flows-in-r/",
    "title": "Maximum Network Flows in R",
    "description": "In this post we will walk through how to make a maximum flow decision using network flows and linear programming.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-09",
    "categories": [
      "network flows",
      "linear programming",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nOil Flow Maximization\r\nProblem Formation\r\nNetwork Structure\r\nNetwork Visual\r\nBig M\r\n\r\nLinear Programming (LP)\r\nLP Structure\r\nConstraints\r\nObjective\r\n\r\nSolve the LP\r\nVisualize Maximum Flow\r\nSolution\r\nCleaning Up The Visual\r\n\r\n\r\nReusable Functionality\r\nSourcing\r\nReferences\r\n\r\nOil Flow Maximization\r\nOne classic problem in Network Flows and\r\nOptimization is called the Max-Flow Problem.\r\nThis takes any two nodes in a network, s and t\r\nand attempts to send as much of a resource (or multiple) from\r\ns to t. This is called a flow,\r\nand the flow which maximizes the total bandwidth of the network is\r\ncalled the maximum flow.\r\nFirst, the problem starts with an objective: to\r\nmaximize flow. These are denoted as,\r\n\\[x_{ij} = flow \\: on \\: node_i \\: to \\:\r\nnode_j \\:(or \\: on \\: arc \\: (i,j))\\]\r\nSecond, the problem has a set of constraints, these are called the\r\narc capacities. These are denoted as,\r\n\\[u_{ij} = maximum \\:amount \\:of \\:\r\nfeasible \\: flow \\: on \\: node_i \\: to \\: node_j \\:(or \\: on \\: arc \\:\r\n(i,j))\\] Last, the network graph is supplied as a set of\r\nconnections under the traditional network structure:\r\n\\[ G = (N,E) \\]\r\nProblem Formation\r\nFor our problem, the feasible flow is going to be in units of\r\nmillions of barrels of oil per hour that will pass through an arc of\r\npipeline.\r\nNetwork Structure\r\nThe source for our network is indicated in green and sink in red.\r\n\r\n\r\nnodes <- data.frame(id = c(0:4), color = c(\"green\", rep(\"grey\", 3), \"red\"))\r\nedges <- data.frame(from = c(0,0,1,1,3,2), to = c(1,2,2,3,4,4), capacity = c(2,3,3,4,1,2), color = \"grey\")\r\n\r\nedges\r\n\r\n  from to capacity color\r\n1    0  1        2  grey\r\n2    0  2        3  grey\r\n3    1  2        3  grey\r\n4    1  3        4  grey\r\n5    3  4        1  grey\r\n6    2  4        2  grey\r\n\r\nNetwork Visual\r\n\r\n\r\ng <- igraph::graph_from_data_frame(d = edges,\r\n                                   directed = TRUE,\r\n                                   vertices = nodes)\r\nplot(g, edge.label = E(g)$capacity, \r\n     main = \"Oil Pipeline Capacities\", \r\n     vertex.color = V(g)$color,\r\n     edge.color = E(g)$color)\r\n\r\n\r\n\r\nBig M\r\nIn order to model this problem, we need to put a very large capacity\r\nfrom the source node to the sink node, these\r\nare s and t mentioned above. We have given\r\nthem node names 0 and 4 respectively in the\r\ndataframe for nodes.\r\n\r\n\r\nBIG_M <- 1000000\r\n\r\nedges <- rbind(edges, data.frame(from = c(4),\r\n                                 to = c(0), \r\n                                 capacity = c(BIG_M), \r\n                                 color = \"purple\"))\r\n\r\n# Never hurts to add an id\r\nedges$id <- 0:(nrow(edges)-1)\r\n\r\ng <- igraph::graph_from_data_frame(d = edges,\r\n                                   directed = TRUE,\r\n                                   vertices = nodes)\r\nplot(g,\r\n     edge.label = E(g)$capacity, \r\n     main = \"Oil Pipeline Capacities\", \r\n     vertex.color = V(g)$color,\r\n     edge.color = E(g)$color)\r\n\r\n\r\n\r\nLinear Programming (LP)\r\nLP Structure\r\nThe next step is to set up the optimization. Let’s do that now. There\r\nare 3 key ingredients.\r\nObjective\r\nConstraints\r\nDirections\r\nConstraints\r\nWe want to build the constraint matrix, which has 2 parts.\r\nArc Capacity Constraints\r\nNode Flow Constraints\r\nThe Arc Capacity Constraints are the first we\r\naddress.\r\nThe Amat, or A matrix, is the arc matrix which contains\r\nthe upper bounds. Since linear programming relies on the resource\r\nmatrix, we need one row for each arc, our dimensions for variable flow\r\nselection are the number of arcs also. So this means we need an\r\nidentity matrix for the rows of arcs and columns of\r\narcs.\r\n\r\n\r\nAmat <- matrix(0, nrow = nrow(edges), ncol = nrow(edges))\r\nAmat_dir <- rep(\"<=\", nrow(Amat))\r\nAmat_rhs <- c()\r\n\r\nfor(i in 1:ncol(Amat)){\r\n  Amat[i,i] <- 1\r\n  Amat_rhs <- c(Amat_rhs, edges$capacity[i])\r\n}\r\n\r\nAmat\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\r\n[1,]    1    0    0    0    0    0    0\r\n[2,]    0    1    0    0    0    0    0\r\n[3,]    0    0    1    0    0    0    0\r\n[4,]    0    0    0    1    0    0    0\r\n[5,]    0    0    0    0    1    0    0\r\n[6,]    0    0    0    0    0    1    0\r\n[7,]    0    0    0    0    0    0    1\r\n\r\nAmat_rhs\r\n\r\n[1] 2e+00 3e+00 3e+00 4e+00 1e+00 2e+00 1e+06\r\n\r\nAmat_dir\r\n\r\n[1] \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\" \"<=\"\r\n\r\nThe Node Flow Constraints are the next to take care\r\nof.\r\nFor each node, we need to match it’s from node and\r\nto node with the appropriate inflow and outflow. If it\r\nmatches an inflow arc, this is increase, so 1\r\nis in the arc column. If it matches an outflow arch, this\r\nis decrease, so -1 is in the arc column. Otherwise\r\n0 remains as the placeholder. The sign here is\r\n== because they must match (i.e., supply = demand). If we\r\nrequire excess at certain points we can set this demand to be higher\r\nthan zero, but we will not do that here.\r\n\r\n\r\nBmat <- matrix(0, nrow = nrow(nodes), ncol = nrow(edges))\r\nBmat_dir <- rep(\"==\", nrow(Bmat))\r\nBmat_rhs <- rep(0, nrow(Bmat))\r\n\r\nfor(i in 1:nrow(Bmat)){\r\n  node_id <- nodes[i, \"id\"]\r\n  for(j in 1:ncol(Bmat)){\r\n    edge_from <- edges[j,\"from\"]\r\n    edge_to <- edges[j, \"to\"]\r\n    edge_id <- edges[j, \"id\"]\r\n    \r\n    if(node_id == edge_from){\r\n      Bmat[i,j] = -1\r\n    }\r\n    else if(node_id == edge_to){\r\n      Bmat[i,j] = 1\r\n    }\r\n    else{\r\n      Bmat[i,j] = 0\r\n    }\r\n  }\r\n}\r\n\r\nBmat\r\n\r\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\r\n[1,]   -1   -1    0    0    0    0    1\r\n[2,]    1    0   -1   -1    0    0    0\r\n[3,]    0    1    1    0    0   -1    0\r\n[4,]    0    0    0    1   -1    0    0\r\n[5,]    0    0    0    0    1    1   -1\r\n\r\nBmat_dir\r\n\r\n[1] \"==\" \"==\" \"==\" \"==\" \"==\"\r\n\r\nBmat_rhs\r\n\r\n[1] 0 0 0 0 0\r\n\r\nObjective\r\nNext, the objective is going to be 0 for all values,\r\nexcept our final flow. This we want to maximize.\r\n\r\n\r\nf.obj <- c(rep(0, nrow(edges)-1), 1)\r\nf.obj\r\n\r\n[1] 0 0 0 0 0 0 1\r\n\r\nSolve the LP\r\nNow we unite everything together and drop it into the solver.\r\nRemember, we are trying to maximize flow. So specify\r\nmax.\r\n\r\n\r\nf.cons <- rbind(Amat, Bmat)\r\nf.rhs <- c(Amat_rhs, Bmat_rhs)\r\nf.dir <- c(Amat_dir, Bmat_dir)\r\n\r\nresults <- lp(direction = \"max\",  \r\n              objective.in = f.obj, \r\n              const.mat = f.cons, \r\n              const.dir = f.dir, \r\n              const.rhs = f.rhs)\r\nresults$solution\r\n\r\n[1] 1 2 0 1 1 2 3\r\n\r\nVisualize Maximum Flow\r\nSolution\r\nSince the results$solution contain the maximum flow we\r\ncan push through the pipes as a system, we can add this into our flow\r\ncomponent of the edges dataframe.\r\nThe visual indicates that 2/3 of the capacity was\r\nshipped to node 2, and 1/2 to\r\nnode 1. After this the next best transfer was from\r\nnode 2 to node 4 maxed out at\r\n2/2. This is indicated in red. The arcs that have flow are\r\nindicated in black, and no flow is indicated by grey. The maximum flow\r\narc is just an artificial arc that indicates the maximum\r\nflow, this is indicated in purple.\r\nLastly from the node 1 to node 3\r\n1/4 of the capacity was sent. Then from node 3\r\nto node 4 was maxed out at 1/1.\r\nPosterior analysis to this model output tells us that sending\r\n2 million tons of oil from the source to\r\ndestination 2 and 1 million tons of oil from\r\nthe source to destination 1 will push as much\r\nflow to destination 4 as we possibly can.\r\n\r\n\r\n# Set the flow\r\nedges$flow <- results$solution\r\n\r\n# If the arc is flowing oil, change to black\r\nedges$color[which(edges$flow > 0)] <- \"black\"\r\n\r\n# If the arc is at capacity change it to red\r\nedges$color[which(edges$flow == edges$capacity)] <- \"red\"\r\n\r\n# Last flow is purple\r\nedges$color[nrow(edges)] <- \"purple\"\r\n\r\n\r\ng <- igraph::graph_from_data_frame(d = edges,\r\n                                   directed = TRUE,\r\n                                   vertices = nodes)\r\n\r\n# Make it look a little more appealing\r\nL = cbind(1:5, 1:5)\r\nCURVE = c(0,0.15, 0.3, 0.45, 0, -0.15, -0.3, 0, 0.15, 0) * 5\r\n\r\n# Plot\r\nplot(g,\r\n     layout = L,\r\n     edge.curved = CURVE,\r\n     edge.label = paste(\"(\",E(g)$flow, \r\n                        \",\", \r\n                        E(g)$capacity, \")\", \r\n                        sep=\"\"), \r\n     main = \"Oil Pipeline Flow | (flow, capacity)\", \r\n     vertex.color = V(g)$color,\r\n     vertex.size = 25,\r\n     vertex.label.cex = 1.6,\r\n     edge.color = E(g)$color,\r\n     edge.width = E(g)$flow*2)\r\n\r\n\r\n\r\nCleaning Up The Visual\r\n\r\n\r\nvisNet_nodes <- nodes %>% \r\n                dplyr::mutate(label = paste(\"Location \", id),\r\n                              font.size =10,\r\n                              type = \"black\",\r\n                              shape = \"circle\",\r\n                              font = 12)\r\n\r\nvisNet_edges <- edges %>% \r\n                dplyr::mutate(label = paste(\"(\",flow,\",\",capacity,\")\",sep=\"\"),\r\n                              length = 250,\r\n                              width = flow*2,\r\n                              arrows = \"to\") \r\n  \r\nvisNetwork(visNet_nodes, \r\n           visNet_edges,\r\n           width = \"100%\",\r\n           main = \"Maximum Oil Flow | (flow, capacity)\")\r\n\r\n\r\n\r\nReusable Functionality\r\nIf we would like, we can take the code and example above and make a\r\nreusable functional API so we don’t need to do this time and again. The\r\ncode is as follows.\r\n\r\n\r\n#\r\n# @lp.maxflow: data.frame, integer, integer -> list\r\n#   function outputs an edge list with max flows.\r\n#\r\n# @edges: data.frame, should have from, to, and capacity columns for each edge in the network. from and to columns should contain unique node ids as integers from 0 to N.\r\n# @source.id: integer, unique node id\r\n# @dest.id: integer, unique node id\r\n#\r\nlp.maxflow <- function(edges, source.id, dest.id){\r\n  if(!(\"from\" %in% names(edges)) || !(\"to\" %in% names(edges)) || !(\"capacity\" %in% names(edges))){\r\n    print(\"Need from, to, and capacity columns.\")\r\n  }\r\n  else{\r\n    \r\n    # Infer the nodes\r\n    nodes <- data.frame(id = sort(unique(c(unique(edges$from), unique(edges$to)))))\r\n    \r\n    # Connect source.id to dest.id\r\n    BIG_M = max(edges$capacity)*10\r\n    edges <- rbind(edges, data.frame(from = c(dest.id),\r\n                                 to = c(source.id), \r\n                                 capacity = c(BIG_M)))\r\n    \r\n    \r\n    \r\n    # Build up edge constraints \r\n    Amat <- matrix(0, nrow = nrow(edges), ncol = nrow(edges))\r\n    Amat_dir <- rep(\"<=\", nrow(Amat))\r\n    Amat_rhs <- c()\r\n    \r\n    for(i in 1:ncol(Amat)){\r\n      Amat[i,i] <- 1\r\n      Amat_rhs <- c(Amat_rhs, edges$capacity[i])\r\n    }\r\n    \r\n    # Build up node constraints\r\n    Bmat <- matrix(0, nrow = nrow(nodes), ncol = nrow(edges))\r\n    Bmat_dir <- rep(\"==\", nrow(Bmat))\r\n    Bmat_rhs <- rep(0, nrow(Bmat))\r\n    \r\n    for(i in 1:nrow(Bmat)){\r\n      node_id <- nodes[i, \"id\"]\r\n      for(j in 1:ncol(Bmat)){\r\n        edge_from <- edges[j,\"from\"]\r\n        edge_to <- edges[j, \"to\"]\r\n        edge_id <- edges[j, \"id\"]\r\n        \r\n        if(node_id == edge_from){\r\n          Bmat[i,j] = -1\r\n        }\r\n        else if(node_id == edge_to){\r\n          Bmat[i,j] = 1\r\n        }\r\n        else{\r\n          Bmat[i,j] = 0\r\n        }\r\n      }\r\n    }\r\n\r\n    # Join all model parameters\r\n    f.obj <- c(rep(0, nrow(edges)-1), 1)\r\n    f.cons <- rbind(Amat, Bmat)\r\n    f.rhs <- c(Amat_rhs, Bmat_rhs)\r\n    f.dir <- c(Amat_dir, Bmat_dir)\r\n    \r\n    results <- lp(direction = \"max\",  \r\n                  objective.in = f.obj, \r\n                  const.mat = f.cons, \r\n                  const.dir = f.dir, \r\n                  const.rhs = f.rhs)\r\n    edges$flow <- results$solution\r\n  }\r\n  \r\n  list(edges=edges, flow = results$solution, maxflow = results$objval, results = results)\r\n}\r\n\r\nedges <- data.frame(from = c(0,0,1,1,3,2), to = c(1,2,2,3,4,4), capacity = c(2,3,3,4,1,2))\r\nlp.maxflow(edges, source.id = 0, dest.id = 4)\r\n\r\n$edges\r\n  from to capacity flow\r\n1    0  1        2    1\r\n2    0  2        3    2\r\n3    1  2        3    0\r\n4    1  3        4    1\r\n5    3  4        1    1\r\n6    2  4        2    2\r\n7    4  0       40    3\r\n\r\n$flow\r\n[1] 1 2 0 1 1 2 3\r\n\r\n$maxflow\r\n[1] 3\r\n\r\n$results\r\nSuccess: the objective function is 3 \r\n\r\nSourcing\r\nWe can also save that function and source it for later use. The code\r\nis available on github here.\r\n\r\n\r\nsource(\"lp.maxflow.r\")\r\nedges <- data.frame(from = c(0,0,1,1,3,2), to = c(1,2,2,3,4,4), capacity = c(2,3,3,4,1,2))\r\nlp.maxflow(edges, source.id = 0, dest.id = 4)\r\n\r\n$edges\r\n  from to capacity flow\r\n1    0  1        2    1\r\n2    0  2        3    2\r\n3    1  2        3    0\r\n4    1  3        4    1\r\n5    3  4        1    1\r\n6    2  4        2    2\r\n7    4  0       40    3\r\n\r\n$flow\r\n[1] 1 2 0 1 1 2 3\r\n\r\n$maxflow\r\n[1] 3\r\n\r\n$results\r\nSuccess: the objective function is 3 \r\n\r\nReferences\r\nExample taken from the following sources:\r\nWinston., Wayne. Operations Research, Applications and\r\nAlgorithms 4th Edition.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-09-maximum-network-flows-in-r/maximum-network-flows-in-r_files/figure-html5/initial_graph-1.png",
    "last_modified": "2022-07-15T04:01:51-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-09-shortest-path-optimization-in-r/",
    "title": "Shortest Path Optimization in R",
    "description": "In this post we will walk through how to make least cost decisions using network flows shortest path algorithm.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-09",
    "categories": [
      "network flows",
      "linear programming",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nVehicle\r\nReplacement Using Shortest Path\r\nProblem Setup\r\nProblem Data\r\nNew Car Cost\r\nMaintenance Costs\r\nTrade-in Revenues\r\nCost Matrix\r\n\r\nSolving the Problem\r\nVisualizing the network\r\nShortest Path Solution\r\nShortest Path Analysis\r\n\r\n\r\nReferences\r\n\r\nVehicle Replacement\r\nUsing Shortest Path\r\nA common problem we often face in the world is to find the shortest\r\npath between two points. Whether it is on the road, or how to obtain an\r\nobject in in our intellectual trajectory, we are always seeking to\r\noptimize.\r\nIn network flows one common problem is to find the\r\ns-t shortest path. The problem formulation is as\r\nfollows.\r\nGiven some net of nodes, and two particular nodes s and\r\nt that are not the same, find the shortest distance through\r\nthe edges connecting them. The mathematical formulation is below. \\[ N \\in \\{n_1, n_2, .., n_D\\} \\] \\[ E = \\{N \\times N\\} = \\{e_{ij} \\in \\Re^+ |\r\n\\:\\:\\forall i,j \\in N \\} \\] \\[ s,t \\in\r\nN : s\\neq t\\]\r\nProblem Setup\r\nWe all have to drive cars at some point in our adult lives (well,\r\nmost of us). With this comes the question of investment\r\ndecisions if I am allowed to call it that. Which car should we buy?\r\nHow long should we keep it? Is it more prudent to keep paying\r\nmaintenance, repairs, and damages, or just get a new ride?\r\nWe would like to know what is the best decision to make over the next\r\n5 years for our vehicle needs. So we take this problem and model it as\r\nan optimization problem using the famous\r\nshortest paths algorithm.\r\nSo with our problem the decision space is pretty straight\r\nforward:\r\nEach year we can choose to keep our vehicle, or\r\nTrade it in\r\nHowever, every year we choose to keep our vehicle, we must pay\r\nmaintenance costs for it. So every year we keep it, there is a\r\ncumulative maintenance cost. Once we trade-in we offset the cost of the\r\nnew car with the trade in value, and pay much less maintenance on the\r\nnew ride. Let’s take a look at the problem data.\r\nProblem Data\r\nWe know the cost every year of a new vehicle is assumed as\r\n$12,000 for simplicity. Further, we have some records of\r\nwhat maintenance costs and trade-in values will be.\r\nNew Car Cost\r\nThe new car cost is assumed constant every year. An interesting\r\nhomework assignment would be to make this stochastic and\r\nchange over time. This is much more suitable to the real world, but for\r\nthis example will remain constant.\r\n\r\n\r\nNEW_CAR_COST <- 12000\r\nNEW_CAR_COST\r\n\r\n[1] 12000\r\n\r\nMaintenance Costs\r\nThe maintenance costs are for 4 years. Each year you keep the car,\r\nyou will pay more on maintenance.\r\n\r\n\r\nM <- data.frame(Year = c(0:4), Maintenance_cost = c(2000, 4000, 5000, 9000, 12000))\r\nM_vec <- M %>% dplyr::pull(Maintenance_cost)\r\nM\r\n\r\n  Year Maintenance_cost\r\n1    0             2000\r\n2    1             4000\r\n3    2             5000\r\n4    3             9000\r\n5    4            12000\r\n\r\nTrade-in Revenues\r\nThe trade-in price is similar to maintenance. Each year you keep the\r\nvehicle, it will depreciate. So to account for this we have a decreasing\r\ntrade-in value.\r\n\r\n\r\nT <- data.frame(Year = c(1:5), Trade_in_price = c(7000, 6000, 2000, 1000, 0))\r\nT_vec <- T %>% dplyr::pull(Trade_in_price)\r\nT\r\n\r\n  Year Trade_in_price\r\n1    1           7000\r\n2    2           6000\r\n3    3           2000\r\n4    4           1000\r\n5    5              0\r\n\r\nCost Matrix\r\nSince we know the costs will be cumulative, so we know what each\r\nyears will be. The cost matrix will be for the number of years the car\r\nis kept to accumulate costs from maintenance. In mathematical language,\r\nthis is represented below:\r\n\\[ c_{ij} = \\sum_{t=1}^{j-1}{M_{t-1}}\\:\\:\r\nif\\: j > i \\:\\: otherwise \\: \\infty \\]\r\nWhere M is the maintenance matrix defined above.\r\nWe also know our objective is to minimize the total cost, which\r\nequates to maintanence cost +\r\ncost to purchase a new car -\r\ntrade in value.\r\n\r\n\r\n# Nodes dataframe\r\nnodes = data.frame(Year = c(sprintf(\"Year %s\", seq(1:6))),\r\n                   Color = c(\"green\", \"gold\", \"gold\", \"gold\", \"gold\", \"red\"))\r\nn = nrow(nodes)\r\n\r\n# Edges list\r\nedges = list(from=c(), to=c(), cost=c(), color=c())\r\n\r\n# Cost matrix\r\nC <- matrix(0, n, n)\r\nBIG_M <- 1000000\r\nfor(i in 1:n){\r\n  for (j in 1:n){\r\n    if(j > i){\r\n      \r\n      # Cost of maintenance\r\n      maintenance_cost <- M_vec[1:(j-i)]\r\n      maintenance_cost_total <- sum(maintenance_cost)\r\n\r\n      # Cost of new car\r\n      new_car_cost = NEW_CAR_COST\r\n      \r\n      # Trade-in value\r\n      trade_in_revenue <- T_vec[j-i]\r\n\r\n      # Total cost for decision to buy car on year i and sell it on year j\r\n      total_cost_for_decision_i_to_j <- new_car_cost + maintenance_cost_total - trade_in_revenue\r\n\r\n      # Save the value into cost matrix\r\n      C[i,j] <- total_cost_for_decision_i_to_j\r\n      edges$from <- append(edges$from, paste(\"Year\", i))\r\n      edges$to <- append(edges$to, paste(\"Year\", j))\r\n      edges$cost <- append(edges$cost, total_cost_for_decision_i_to_j)\r\n      edges$color <- append(edges$color, \"grey\")\r\n    }\r\n    else{\r\n      \r\n      # Big M otherwise to make edge infeasible\r\n      C[i,j] <- BIG_M\r\n    }\r\n  }\r\n}\r\n\r\n# Edges dataframe\r\nedges <- edges %>% as.data.frame\r\n\r\nnodes\r\n\r\n    Year Color\r\n1 Year 1 green\r\n2 Year 2  gold\r\n3 Year 3  gold\r\n4 Year 4  gold\r\n5 Year 5  gold\r\n6 Year 6   red\r\n\r\nedges\r\n\r\n     from     to  cost color\r\n1  Year 1 Year 2  7000  grey\r\n2  Year 1 Year 3 12000  grey\r\n3  Year 1 Year 4 21000  grey\r\n4  Year 1 Year 5 31000  grey\r\n5  Year 1 Year 6 44000  grey\r\n6  Year 2 Year 3  7000  grey\r\n7  Year 2 Year 4 12000  grey\r\n8  Year 2 Year 5 21000  grey\r\n9  Year 2 Year 6 31000  grey\r\n10 Year 3 Year 4  7000  grey\r\n11 Year 3 Year 5 12000  grey\r\n12 Year 3 Year 6 21000  grey\r\n13 Year 4 Year 5  7000  grey\r\n14 Year 4 Year 6 12000  grey\r\n15 Year 5 Year 6  7000  grey\r\n\r\nC\r\n\r\n      [,1]  [,2]    [,3]    [,4]    [,5]    [,6]\r\n[1,] 1e+06 7e+03   12000   21000   31000   44000\r\n[2,] 1e+06 1e+06    7000   12000   21000   31000\r\n[3,] 1e+06 1e+06 1000000    7000   12000   21000\r\n[4,] 1e+06 1e+06 1000000 1000000    7000   12000\r\n[5,] 1e+06 1e+06 1000000 1000000 1000000    7000\r\n[6,] 1e+06 1e+06 1000000 1000000 1000000 1000000\r\n\r\nSolving the Problem\r\nNow that we have our cost matrix, the last ingredient is to solve the\r\nproblem. That means to solve the s-t shortest path from\r\nYear 0 to Year 6, so we can determine what is\r\nthe cheapest investment strategy for us. For this, we will be using the\r\nigraph package.\r\nVisualizing the network\r\n\r\n\r\ng <- igraph::graph_from_data_frame(d = edges,\r\n                                   directed = TRUE,\r\n                                   vertices = nodes)\r\n# Make it a little cleaner\r\nplot(g,\r\n     main = \"Cost of Annual Vehicle Trade-in\",\r\n     edge.arrow.size=.5,\r\n     vertex.color=V(g)$Color,\r\n     edge.color=E(g)$color,\r\n     edge.label = E(g)$cost,\r\n     edge.width = E(g)$cost/10000,\r\n     vertex.size=20,\r\n     vertex.frame.color=\"gray\",\r\n     vertex.label.color=\"black\",\r\n     vertex.label.cex=0.8,\r\n     vertex.label.dist=2, layout = layout_as_star,\r\n     edge.curved=0.2)\r\n\r\n\r\n\r\nShortest Path Solution\r\nSo once we perform the Dijkstra's Shortest Path\r\nalgorithm on the network we obtain a solution in matrix form. This\r\nsolution tells us the best possible cost for our car decision from\r\nYear 1 to Year 6 will be at\r\n$31,000.\r\nWhat this does not show us is what path was chosen to obtain that\r\nvalue.\r\n\r\n\r\n# Get the shortest paht cost matrix\r\ns_paths_cost <- igraph::shortest.paths(graph = g, v = V(g), weights = E(g)$cost, algorithm = \"dijkstra\")\r\ns_paths_cost\r\n\r\n       Year 1 Year 2 Year 3 Year 4 Year 5 Year 6\r\nYear 1      0   7000  12000  19000  24000  31000\r\nYear 2   7000      0   7000  12000  19000  24000\r\nYear 3  12000   7000      0   7000  12000  19000\r\nYear 4  19000  12000   7000      0   7000  12000\r\nYear 5  24000  19000  12000   7000      0   7000\r\nYear 6  31000  24000  19000  12000   7000      0\r\n\r\nShortest Path Analysis\r\nThe optimal selection is the following sequence:\r\nBuy a new car in Year 1 keep the car for a year, then\r\nsell on Year 2.\r\nBuy another car in Year 2, but keep for two years and\r\nsell on Year 4.\r\nFinally, buy a car on Year 4, keep for two years and\r\nsell on Year 6.\r\nDo these numbers add up? Let’s check. 7000 + 12000 + 12000 == 31000\r\nis TRUE.\r\nSo our least cost strategy can be no less than $31000\r\nover the next 6 years. With the current cost structure, means to keep\r\nthe car for a year or two, then pitch it because the trade-off between\r\nmaintenance accumulation and depreciation start to mutually deter from a\r\nleast cost decision.\r\n\r\n\r\n# Get all path distances solution vertex path\r\ns.paths <- igraph::shortest_paths(graph = g,\r\n                                  from = \"Year 1\",\r\n                                  output = \"vpath\",\r\n                                  weights = E(g)$cost, \r\n                                  to = \"Year 6\")\r\n                                  # v = V(g), \r\n                                  # to = V(g), \r\n                                  # weights = E(g)$cost)\r\n\r\n# Update colors from vertex path found\r\ns.paths$vpath\r\n\r\n[[1]]\r\n+ 4/6 vertices, named, from 81b1a31:\r\n[1] Year 1 Year 2 Year 4 Year 6\r\n\r\nE(g, path = s.paths$vpath[[1]])$color <- \"red\"\r\n\r\n\r\n\r\n\r\nplot(g,\r\n     main = \"Least Cost Vehicle Trade-in Policy\",\r\n     edge.arrow.size=.5,\r\n     vertex.color=V(g)$Color,\r\n     edge.color=E(g)$color,\r\n     edge.label = E(g)$cost,\r\n     edge.width = E(g)$cost/10000,\r\n     vertex.size=20,\r\n     vertex.frame.color=\"gray\",\r\n     vertex.label.color=\"black\",\r\n     vertex.label.cex=0.8,\r\n     vertex.label.dist=2, layout = layout_as_star,\r\n     edge.curved=0.2)\r\n\r\n\r\n\r\nThis plot doesn’t look too great! Let’s try to spruce it up a bit\r\nusing visNetwork, a common package in R that leverages the\r\nvis.js framework, which can be found here.\r\n\r\n\r\nV(g)$label = nodes$Year\r\nV(g)$shape = \"circle\"\r\nE(g)$width = edges$cost/10000\r\nE(g)$weight = edges$cost/10000\r\nE(g)$label = edges$cost %>% as.character\r\n\r\nvisIgraph(igraph = g)\r\n\r\n\r\n\r\nReferences\r\nExample taken from the following sources:\r\nWinston., Wayne. Operations Research, Applications and\r\nAlgorithms 4th Edition.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-09-shortest-path-optimization-in-r/shortest-path-optimization-in-r_files/figure-html5/viz_network-1.png",
    "last_modified": "2022-07-13T06:06:56-04:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Optimization Daily",
    "description": "Welcome to Optimization Daily! Grab a coffee, take a read, and enjoy your stay.",
    "author": [
      {
        "name": "Blake Conrad",
        "url": "https://github.com/conradbm"
      }
    ],
    "date": "2022-07-09",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-07-10T20:34:19-04:00",
    "input_file": {}
  }
]
