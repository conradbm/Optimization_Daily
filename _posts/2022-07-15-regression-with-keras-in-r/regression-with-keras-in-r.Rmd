---
title: "Regression With Keras in R"
description: |
  In this post we will explore how to do deep regression in R using Keras.
categories:
  - deep learning
  - machine learning
  - regression
  - keras
  - r
author:
  - name: Blake Conrad
    url: https://github.com/conradbm
date: 2022-07-15
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
---

# Boston Housing Prices

```{r}
library(tfdatasets)
library(keras)
library(tidyr)
library(dplyr)
library(ggplot2)
```

```{r}
boston_housing <- dataset_boston_housing()
train_data <- boston_housing$train$x
train_labels <- boston_housing$train$y

test_data <- boston_housing$test$x
test_labels <- boston_housing$test$y

paste0("Training entries: ", length(train_data), ", labels: ", length(train_labels))
```

```{r}
column_names <- c('CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 
                  'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT')

train_df <- train_data %>% 
  dplyr::as_tibble(.name_repair = "minimal") %>% 
  stats::setNames(., column_names) %>% 
  dplyr::mutate(label = train_labels)
train_df

test_df <- test_data %>% 
  dplyr::as_tibble(.name_repair = "minimal") %>% 
  stats::setNames(., column_names) %>% 
  dplyr::mutate(label = test_labels)
test_df

```

```{r}
train_labels[1:10]
```


## Visualize Correlation

```{r}
# See what features are correlated
# Center and normalize
Xtrain <- train_df %>% select(-label)
Xs <- scale(Xtrain, center = TRUE, scale = TRUE)


# Tidy the data
C <- cor(Xs) %>% as.data.frame
C$from <- row.names(C)
C <- C %>% tidyr::gather(., key = "to", value = "correlation", -from)

# Present a heatmap
plt <- ggplot(data = C) + 
  geom_tile(aes(x=to, y=from, fill=correlation)) + 
  scale_fill_gradient(low = "red", high = "green") +
  ggtitle("Correlation between Boston House features") +
  theme(axis.text.x = element_text(angle = 90)) 

plt
```


A few areas of concern are those with greater than about 0.5 correlation. This could mean they are excessive information and redundant. Let's look at those who have a magnitude greater than between 0.25 and 0.5.

## Visualize Serious Correlation

Below is an analysis of some of the correlations. I performed a detailed analysis on a pre-centered correlation matrix, which I will leave below for kicks. But ultimately, **high correlated features should be considered for removal from the model**.

```{r, preview=TRUE}
# Present a heatmap
plt <- ggplot(data = C %>% filter(abs(correlation) > 0.7)) + 
  geom_tile(aes(x=to, y=from, fill=correlation)) + 
  scale_fill_gradient(low = "red", high = "green") +
  ggtitle("Correlations between Boston House features") +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))

plt
```

There are several columns that could be dropped due to their high correlation with other variables. Let's keep track of these for a hot minute. We will drop these and remodel later to see if this helps any.

```{r}
EXCESSIVE_FEATURES <- c("DIS", "NOX", "TAX")
EXCESSIVE_FEATURES
```

## Standardize the data

```{r}

# Center and normalize
Xtrain <- train_df %>% select(-label)
Xs_train <- scale(Xtrain, center = T, scale = T)
train_df_scaled <- cbind(Xs_train, train_df %>% select(label))

Xtest <- test_df %>% select(-label)
Xs_test <- scale(Xtest, center = T, scale = T)
test_df_scaled <- cbind(Xs_test, test_df %>% select(label))


```

## Model

Some cheat codes are required to make `keras` work with dataframes. First, you have to make a multi-head model. Define an `input` with your dimensionality of the data frame. Second, create your other layers to pass it through. Merge them using the `keras_model` function. R is great, but can be very painful at times. This was not pleasant. Further, **only feed in matrices**. Don't even fight with throwing in a data frame. This API isn't smart like that.

```{r}

# Input head
input <- keras::layer_input(shape = c(13))

# Output head
output <- input %>% 
  keras::layer_batch_normalization() %>%  
  keras::layer_dense(units = 1)

# Join input and output heads
model <- keras::keras_model(input, output)

model %>% keras::compile(
  loss = "mse",
  optimizer = "adam",
  metrics = list("mean_absolute_error")
)

history = model %>% keras::fit(
  x = train_df %>% select(-label) %>% as.matrix,
  y = train_df$label %>% as.matrix,
  validation_split = 0.1,
  epochs = 100,
  verbose=0
)


plot(history)

```
### Evaluate

```{r}
evaluate(model,
         x = test_df %>% select(-label) %>% as.matrix, 
         y = test_df$label %>% as.matrix)
```

### Visualize

```{r}
ypred <- predict(model, 
                x = test_df %>% select(-label) %>% as.matrix)

tmp <- data.frame(id = 1:length(ypred),
                  prediction = ypred,
                  target = test_df$label)
tmp <- tmp %>% dplyr::mutate(mse = sqrt((tmp$prediction - tmp$target)**2),
                             error = tmp$prediction - tmp$target)

plt <- ggplot( data = tmp) + 
  geom_point(aes(x=id, y=prediction, color="prediction")) +
  geom_point(aes(x=id, y=target, color="target")) +
  theme_classic()

ggplot(tmp) + 
  geom_histogram(aes(x=mse, fill="mse"), alpha = 0.4) +
  geom_histogram(aes(x=error, fill="error"), alpha=0.4) + 
  labs(fill = "Error Distribution") +
  ggtitle("Test Error") +
  xlab("Error") +
  ylab("Occurances") +
  theme_light()


shapiro.test(tmp$error) # This is probably normal. We can do inferential statistics on the population!
```

## Dropping Correlated Features

In efforts to save time and effort. The experiment to drop correlation variables `EXCESSIVE FEATURES` turned out to actually make the model worse. This is probably due to the fact that they store valuable information in this relatively small data set.

## Inferentials

Since we built our regression model with just one layer, the weights in that layer are in fact our parameters

```{r}

output_layer <- model$layers[[3]]

weights <- output_layer$get_weights()
theta <- weights[[1]]
bias <- weights[[2]]
theta

est_df <- data.frame(variable = c(names(train_df %>% select(-label)), "intercept"),
                     estimate = c(theta, bias)
          )


ggplot(data = est_df) + 
  geom_col(aes(x=variable, y=estimate, fill=variable)) + 
  coord_flip() +
  labs(fill = "Variable Estimate") +
  ggtitle("Boston Housing Factors") +
  ylab("Coefficient Estimate") +
  xlab("Variable") +
  theme_bw()
```


# References

<cite>https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_regression/</cite>


