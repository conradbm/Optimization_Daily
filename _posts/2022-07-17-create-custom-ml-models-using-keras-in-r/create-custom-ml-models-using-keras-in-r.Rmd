---
title: "Create Custom ML Models using Keras in R"
description: |
  In this post we will explore automatic differentiation as applied to custom machine learning model generation using keras in R.
categories:
  - machine learning
  - deep learning
  - non-linear programming
  - automatic differentiation
  - keras
  - r
author:
  - name: Blake Conrad
    url: https://github.com/conradbm
date: 2022-07-18
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
---


# Building Custom Models

In the post on `Automatic Differentiation with Keras in R` there was a walk through some of the documentation and an example of using the `GradientTape` API. There is no creativity in this post, just a repeat from the `keras` documentation. The goal here is to take an example, play around with it, and explore the API a bit.

## Fitting a linear model

```{r}
library(tensorflow)
Model <- R6::R6Class(
  classname = "Model",
  public = list(
    W = NULL,
    b = NULL,
    
    initialize = function(){
      self$W <- tf$Variable(5) # Initialize weight to 5
      self$b <- tf$Variable(0) # Initialize bias to 0 
      
    },
    
    call = function(x){
      self$W*x + self$b
    }
  )
)
model <- Model$new()
model$call(3)

```
## Define a loss

```{r}
loss <- function(y_pred, y_true) {
  tf$reduce_mean(tf$square(y_pred - y_true))
}
```


## Obtain training data

```{r}
TRUE_W = 3.0
TRUE_b = 2.0

NUM_EXAMPLES = 1000

# Random 1-d vector of inputs
inputs = tf$random$normal(shape = shape(NUM_EXAMPLES))
# Random 1-d noise
noise = tf$random$normal(shape = shape(NUM_EXAMPLES))

# True 1-d output target population
outputs = inputs * TRUE_W + TRUE_b + noise

```


```{r}
library(dplyr)
library(ggplot2)
df = tibble(
  inputs = as.numeric(inputs),
  outputs = as.numeric(outputs),
  predicted = as.numeric(model$call(inputs))
) 

df %>% ggplot(., aes(x=inputs)) +
  geom_point(aes(y = outputs)) +
  geom_line(aes(y = predicted), color = "blue")
```

## Define a training loop

```{r}
train <- function(model, inputs, outputs, learning_rate) {
  with (tf$GradientTape() %as% t, {
    current_loss = loss(model$call(inputs), outputs)
  })
  
  d <- t$gradient(current_loss, list(model$W, model$b))
  
  model$W$assign_sub(learning_rate * d[[1]])
  model$b$assign_sub(learning_rate * d[[2]])
  current_loss
}

```

```{r}
library(glue)

model <- Model$new()

Ws <- bs <- c()

for(epoch in seq_len(20)){
  
  Ws[epoch] <- as.numeric(model$W)
  bs[epoch] <- as.numeric(model$b)
  
  current_loss <- train(model, inputs, outputs, learning_rate = 0.1)
  cat(glue::glue("Epoch: {epoch}, Loss: {as.numeric(current_loss)}"), "\n")
}

```
```{r}
library(tidyr)

# Variables per column
df = tibble(
  epoch = 1:20,
  Ws = Ws,
  bs = bs
)

# Melt on multiple columns! Name their similarity..
df = df %>% tidyr::pivot_longer(c(Ws, bs), 
                           names_to = "parameter", 
                           values_to = "estimate")

ggplot(df, aes(x=epoch, y=estimate)) +
  geom_line() +
  facet_wrap(~parameter, scales = "free")
```




# References

<cite>https://tensorflow.rstudio.com/tutorials/advanced/customization/custom-training/</cite>


